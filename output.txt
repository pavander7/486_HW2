[main][L1] bm25 document weighting scheme selected.
[main][L1] bm25 query weighting scheme selected.
[main][L1] Cosine normalization disabled.
[main][L1] Document directory found successfully.
[main][L1] Test query file found successfully.
[main][L1] Found 1400 files in document directory.
[indexDocument][L2] Indexing document '1223'.
[indexDocument][L3] Opened file '1223'.
[preprocess][L2] Starting preprocessing for '1223'.
[preprocess][L3] 1223: Tokenized into 98 tokens.
[preprocess][L3] 1223: Removed stop words; 63 tokens remain.
[preprocess][L3] 1223: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1223'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1223'.
[indexDocument][L3] Recorded doc length 63 for '1223'.
[indexDocument][L3] Tokens added to the inverted index for '1223'.
[indexDocument][L2] Indexing document '1011'.
[indexDocument][L3] Opened file '1011'.
[preprocess][L2] Starting preprocessing for '1011'.
[preprocess][L3] 1011: Tokenized into 66 tokens.
[preprocess][L3] 1011: Removed stop words; 39 tokens remain.
[preprocess][L3] 1011: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1011'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1011'.
[indexDocument][L3] Recorded doc length 39 for '1011'.
[indexDocument][L3] Tokens added to the inverted index for '1011'.
[indexDocument][L2] Indexing document '795'.
[indexDocument][L3] Opened file '795'.
[preprocess][L2] Starting preprocessing for '795'.
[preprocess][L3] 795: Tokenized into 61 tokens.
[preprocess][L3] 795: Removed stop words; 39 tokens remain.
[preprocess][L3] 795: Stemming complete.
[indexDocument][L3] Preprocessing complete for '795'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '795'.
[indexDocument][L3] Recorded doc length 39 for '795'.
[indexDocument][L3] Tokens added to the inverted index for '795'.
[indexDocument][L2] Indexing document '553'.
[indexDocument][L3] Opened file '553'.
[preprocess][L2] Starting preprocessing for '553'.
[preprocess][L3] 553: Tokenized into 184 tokens.
[preprocess][L3] 553: Removed stop words; 103 tokens remain.
[preprocess][L3] 553: Stemming complete.
[indexDocument][L3] Preprocessing complete for '553'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '553'.
[indexDocument][L3] Recorded doc length 103 for '553'.
[indexDocument][L3] Tokens added to the inverted index for '553'.
[indexDocument][L2] Indexing document '761'.
[indexDocument][L3] Opened file '761'.
[preprocess][L2] Starting preprocessing for '761'.
[preprocess][L3] 761: Tokenized into 85 tokens.
[preprocess][L3] 761: Removed stop words; 56 tokens remain.
[preprocess][L3] 761: Stemming complete.
[indexDocument][L3] Preprocessing complete for '761'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '761'.
[indexDocument][L3] Recorded doc length 56 for '761'.
[indexDocument][L3] Tokens added to the inverted index for '761'.
[indexDocument][L2] Indexing document '305'.
[indexDocument][L3] Opened file '305'.
[preprocess][L2] Starting preprocessing for '305'.
[preprocess][L3] 305: Tokenized into 158 tokens.
[preprocess][L3] 305: Removed stop words; 103 tokens remain.
[preprocess][L3] 305: Stemming complete.
[indexDocument][L3] Preprocessing complete for '305'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '305'.
[indexDocument][L3] Recorded doc length 103 for '305'.
[indexDocument][L3] Tokens added to the inverted index for '305'.
[indexDocument][L2] Indexing document '137'.
[indexDocument][L3] Opened file '137'.
[preprocess][L2] Starting preprocessing for '137'.
[preprocess][L3] 137: Tokenized into 52 tokens.
[preprocess][L3] 137: Removed stop words; 31 tokens remain.
[preprocess][L3] 137: Stemming complete.
[indexDocument][L3] Preprocessing complete for '137'; 31 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '137'.
[indexDocument][L3] Recorded doc length 31 for '137'.
[indexDocument][L3] Tokens added to the inverted index for '137'.
[indexDocument][L2] Indexing document '598'.
[indexDocument][L3] Opened file '598'.
[preprocess][L2] Starting preprocessing for '598'.
[preprocess][L3] 598: Tokenized into 69 tokens.
[preprocess][L3] 598: Removed stop words; 46 tokens remain.
[preprocess][L3] 598: Stemming complete.
[indexDocument][L3] Preprocessing complete for '598'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '598'.
[indexDocument][L3] Recorded doc length 46 for '598'.
[indexDocument][L3] Tokens added to the inverted index for '598'.
[indexDocument][L2] Indexing document '934'.
[indexDocument][L3] Opened file '934'.
[preprocess][L2] Starting preprocessing for '934'.
[preprocess][L3] 934: Tokenized into 224 tokens.
[preprocess][L3] 934: Removed stop words; 131 tokens remain.
[preprocess][L3] 934: Stemming complete.
[indexDocument][L3] Preprocessing complete for '934'; 131 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '934'.
[indexDocument][L3] Recorded doc length 131 for '934'.
[indexDocument][L3] Tokens added to the inverted index for '934'.
[indexDocument][L2] Indexing document '108'.
[indexDocument][L3] Opened file '108'.
[preprocess][L2] Starting preprocessing for '108'.
[preprocess][L3] 108: Tokenized into 111 tokens.
[preprocess][L3] 108: Removed stop words; 65 tokens remain.
[preprocess][L3] 108: Stemming complete.
[indexDocument][L3] Preprocessing complete for '108'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '108'.
[indexDocument][L3] Recorded doc length 65 for '108'.
[indexDocument][L3] Tokens added to the inverted index for '108'.
[indexDocument][L2] Indexing document '130'.
[indexDocument][L3] Opened file '130'.
[preprocess][L2] Starting preprocessing for '130'.
[preprocess][L3] 130: Tokenized into 96 tokens.
[preprocess][L3] 130: Removed stop words; 52 tokens remain.
[preprocess][L3] 130: Stemming complete.
[indexDocument][L3] Preprocessing complete for '130'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '130'.
[indexDocument][L3] Recorded doc length 52 for '130'.
[indexDocument][L3] Tokens added to the inverted index for '130'.
[indexDocument][L2] Indexing document '302'.
[indexDocument][L3] Opened file '302'.
[preprocess][L2] Starting preprocessing for '302'.
[preprocess][L3] 302: Tokenized into 147 tokens.
[preprocess][L3] 302: Removed stop words; 89 tokens remain.
[preprocess][L3] 302: Stemming complete.
[indexDocument][L3] Preprocessing complete for '302'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '302'.
[indexDocument][L3] Recorded doc length 89 for '302'.
[indexDocument][L3] Tokens added to the inverted index for '302'.
[indexDocument][L2] Indexing document '766'.
[indexDocument][L3] Opened file '766'.
[preprocess][L2] Starting preprocessing for '766'.
[preprocess][L3] 766: Tokenized into 250 tokens.
[preprocess][L3] 766: Removed stop words; 157 tokens remain.
[preprocess][L3] 766: Stemming complete.
[indexDocument][L3] Preprocessing complete for '766'; 157 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '766'.
[indexDocument][L3] Recorded doc length 157 for '766'.
[indexDocument][L3] Tokens added to the inverted index for '766'.
[indexDocument][L2] Indexing document '554'.
[indexDocument][L3] Opened file '554'.
[preprocess][L2] Starting preprocessing for '554'.
[preprocess][L3] 554: Tokenized into 152 tokens.
[preprocess][L3] 554: Removed stop words; 105 tokens remain.
[preprocess][L3] 554: Stemming complete.
[indexDocument][L3] Preprocessing complete for '554'; 105 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '554'.
[indexDocument][L3] Recorded doc length 105 for '554'.
[indexDocument][L3] Tokens added to the inverted index for '554'.
[indexDocument][L2] Indexing document '792'.
[indexDocument][L3] Opened file '792'.
[preprocess][L2] Starting preprocessing for '792'.
[preprocess][L3] 792: Tokenized into 439 tokens.
[preprocess][L3] 792: Removed stop words; 266 tokens remain.
[preprocess][L3] 792: Stemming complete.
[indexDocument][L3] Preprocessing complete for '792'; 266 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '792'.
[indexDocument][L3] Recorded doc length 266 for '792'.
[indexDocument][L3] Tokens added to the inverted index for '792'.
[indexDocument][L2] Indexing document '1016'.
[indexDocument][L3] Opened file '1016'.
[preprocess][L2] Starting preprocessing for '1016'.
[preprocess][L3] 1016: Tokenized into 76 tokens.
[preprocess][L3] 1016: Removed stop words; 47 tokens remain.
[preprocess][L3] 1016: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1016'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1016'.
[indexDocument][L3] Recorded doc length 47 for '1016'.
[indexDocument][L3] Tokens added to the inverted index for '1016'.
[indexDocument][L2] Indexing document '1224'.
[indexDocument][L3] Opened file '1224'.
[preprocess][L2] Starting preprocessing for '1224'.
[preprocess][L3] 1224: Tokenized into 251 tokens.
[preprocess][L3] 1224: Removed stop words; 147 tokens remain.
[preprocess][L3] 1224: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1224'; 147 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1224'.
[indexDocument][L3] Recorded doc length 147 for '1224'.
[indexDocument][L3] Tokens added to the inverted index for '1224'.
[indexDocument][L2] Indexing document '759'.
[indexDocument][L3] Opened file '759'.
[preprocess][L2] Starting preprocessing for '759'.
[preprocess][L3] 759: Tokenized into 211 tokens.
[preprocess][L3] 759: Removed stop words; 135 tokens remain.
[preprocess][L3] 759: Stemming complete.
[indexDocument][L3] Preprocessing complete for '759'; 135 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '759'.
[indexDocument][L3] Recorded doc length 135 for '759'.
[indexDocument][L3] Tokens added to the inverted index for '759'.
[indexDocument][L2] Indexing document '933'.
[indexDocument][L3] Opened file '933'.
[preprocess][L2] Starting preprocessing for '933'.
[preprocess][L3] 933: Tokenized into 281 tokens.
[preprocess][L3] 933: Removed stop words; 159 tokens remain.
[preprocess][L3] 933: Stemming complete.
[indexDocument][L3] Preprocessing complete for '933'; 159 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '933'.
[indexDocument][L3] Recorded doc length 159 for '933'.
[indexDocument][L3] Tokens added to the inverted index for '933'.
[indexDocument][L2] Indexing document '1029'.
[indexDocument][L3] Opened file '1029'.
[preprocess][L2] Starting preprocessing for '1029'.
[preprocess][L3] 1029: Tokenized into 68 tokens.
[preprocess][L3] 1029: Removed stop words; 38 tokens remain.
[preprocess][L3] 1029: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1029'; 38 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1029'.
[indexDocument][L3] Recorded doc length 38 for '1029'.
[indexDocument][L3] Tokens added to the inverted index for '1029'.
[indexDocument][L2] Indexing document '750'.
[indexDocument][L3] Opened file '750'.
[preprocess][L2] Starting preprocessing for '750'.
[preprocess][L3] 750: Tokenized into 91 tokens.
[preprocess][L3] 750: Removed stop words; 55 tokens remain.
[preprocess][L3] 750: Stemming complete.
[indexDocument][L3] Preprocessing complete for '750'; 55 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '750'.
[indexDocument][L3] Recorded doc length 55 for '750'.
[indexDocument][L3] Tokens added to the inverted index for '750'.
[indexDocument][L2] Indexing document '562'.
[indexDocument][L3] Opened file '562'.
[preprocess][L2] Starting preprocessing for '562'.
[preprocess][L3] 562: Tokenized into 229 tokens.
[preprocess][L3] 562: Removed stop words; 128 tokens remain.
[preprocess][L3] 562: Stemming complete.
[indexDocument][L3] Preprocessing complete for '562'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '562'.
[indexDocument][L3] Recorded doc length 128 for '562'.
[indexDocument][L3] Tokens added to the inverted index for '562'.
[indexDocument][L2] Indexing document '106'.
[indexDocument][L3] Opened file '106'.
[preprocess][L2] Starting preprocessing for '106'.
[preprocess][L3] 106: Tokenized into 74 tokens.
[preprocess][L3] 106: Removed stop words; 44 tokens remain.
[preprocess][L3] 106: Stemming complete.
[indexDocument][L3] Preprocessing complete for '106'; 44 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '106'.
[indexDocument][L3] Recorded doc length 44 for '106'.
[indexDocument][L3] Tokens added to the inverted index for '106'.
[indexDocument][L2] Indexing document '334'.
[indexDocument][L3] Opened file '334'.
[preprocess][L2] Starting preprocessing for '334'.
[preprocess][L3] 334: Tokenized into 237 tokens.
[preprocess][L3] 334: Removed stop words; 139 tokens remain.
[preprocess][L3] 334: Stemming complete.
[indexDocument][L3] Preprocessing complete for '334'; 139 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '334'.
[indexDocument][L3] Recorded doc length 139 for '334'.
[indexDocument][L3] Tokens added to the inverted index for '334'.
[indexDocument][L2] Indexing document '1020'.
[indexDocument][L3] Opened file '1020'.
[preprocess][L2] Starting preprocessing for '1020'.
[preprocess][L3] 1020: Tokenized into 123 tokens.
[preprocess][L3] 1020: Removed stop words; 73 tokens remain.
[preprocess][L3] 1020: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1020'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1020'.
[indexDocument][L3] Recorded doc length 73 for '1020'.
[indexDocument][L3] Tokens added to the inverted index for '1020'.
[indexDocument][L2] Indexing document '1212'.
[indexDocument][L3] Opened file '1212'.
[preprocess][L2] Starting preprocessing for '1212'.
[preprocess][L3] 1212: Tokenized into 258 tokens.
[preprocess][L3] 1212: Removed stop words; 142 tokens remain.
[preprocess][L3] 1212: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1212'; 142 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1212'.
[indexDocument][L3] Recorded doc length 142 for '1212'.
[indexDocument][L3] Tokens added to the inverted index for '1212'.
[indexDocument][L2] Indexing document '596'.
[indexDocument][L3] Opened file '596'.
[preprocess][L2] Starting preprocessing for '596'.
[preprocess][L3] 596: Tokenized into 116 tokens.
[preprocess][L3] 596: Removed stop words; 63 tokens remain.
[preprocess][L3] 596: Stemming complete.
[indexDocument][L3] Preprocessing complete for '596'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '596'.
[indexDocument][L3] Recorded doc length 63 for '596'.
[indexDocument][L3] Tokens added to the inverted index for '596'.
[indexDocument][L2] Indexing document '905'.
[indexDocument][L3] Opened file '905'.
[preprocess][L2] Starting preprocessing for '905'.
[preprocess][L3] 905: Tokenized into 87 tokens.
[preprocess][L3] 905: Removed stop words; 61 tokens remain.
[preprocess][L3] 905: Stemming complete.
[indexDocument][L3] Preprocessing complete for '905'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '905'.
[indexDocument][L3] Recorded doc length 61 for '905'.
[indexDocument][L3] Tokens added to the inverted index for '905'.
[indexDocument][L2] Indexing document '139'.
[indexDocument][L3] Opened file '139'.
[preprocess][L2] Starting preprocessing for '139'.
[preprocess][L3] 139: Tokenized into 150 tokens.
[preprocess][L3] 139: Removed stop words; 84 tokens remain.
[preprocess][L3] 139: Stemming complete.
[indexDocument][L3] Preprocessing complete for '139'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '139'.
[indexDocument][L3] Recorded doc length 84 for '139'.
[indexDocument][L3] Tokens added to the inverted index for '139'.
[indexDocument][L2] Indexing document '591'.
[indexDocument][L3] Opened file '591'.
[preprocess][L2] Starting preprocessing for '591'.
[preprocess][L3] 591: Tokenized into 69 tokens.
[preprocess][L3] 591: Removed stop words; 40 tokens remain.
[preprocess][L3] 591: Stemming complete.
[indexDocument][L3] Preprocessing complete for '591'; 40 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '591'.
[indexDocument][L3] Recorded doc length 40 for '591'.
[indexDocument][L3] Tokens added to the inverted index for '591'.
[indexDocument][L2] Indexing document '1215'.
[indexDocument][L3] Opened file '1215'.
[preprocess][L2] Starting preprocessing for '1215'.
[preprocess][L3] 1215: Tokenized into 135 tokens.
[preprocess][L3] 1215: Removed stop words; 71 tokens remain.
[preprocess][L3] 1215: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1215'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1215'.
[indexDocument][L3] Recorded doc length 71 for '1215'.
[indexDocument][L3] Tokens added to the inverted index for '1215'.
[indexDocument][L2] Indexing document '1027'.
[indexDocument][L3] Opened file '1027'.
[preprocess][L2] Starting preprocessing for '1027'.
[preprocess][L3] 1027: Tokenized into 93 tokens.
[preprocess][L3] 1027: Removed stop words; 58 tokens remain.
[preprocess][L3] 1027: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1027'; 58 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1027'.
[indexDocument][L3] Recorded doc length 58 for '1027'.
[indexDocument][L3] Tokens added to the inverted index for '1027'.
[indexDocument][L2] Indexing document '333'.
[indexDocument][L3] Opened file '333'.
[preprocess][L2] Starting preprocessing for '333'.
[preprocess][L3] 333: Tokenized into 72 tokens.
[preprocess][L3] 333: Removed stop words; 50 tokens remain.
[preprocess][L3] 333: Stemming complete.
[indexDocument][L3] Preprocessing complete for '333'; 50 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '333'.
[indexDocument][L3] Recorded doc length 50 for '333'.
[indexDocument][L3] Tokens added to the inverted index for '333'.
[indexDocument][L2] Indexing document '101'.
[indexDocument][L3] Opened file '101'.
[preprocess][L2] Starting preprocessing for '101'.
[preprocess][L3] 101: Tokenized into 332 tokens.
[preprocess][L3] 101: Removed stop words; 199 tokens remain.
[preprocess][L3] 101: Stemming complete.
[indexDocument][L3] Preprocessing complete for '101'; 199 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '101'.
[indexDocument][L3] Recorded doc length 199 for '101'.
[indexDocument][L3] Tokens added to the inverted index for '101'.
[indexDocument][L2] Indexing document '565'.
[indexDocument][L3] Opened file '565'.
[preprocess][L2] Starting preprocessing for '565'.
[preprocess][L3] 565: Tokenized into 222 tokens.
[preprocess][L3] 565: Removed stop words; 133 tokens remain.
[preprocess][L3] 565: Stemming complete.
[indexDocument][L3] Preprocessing complete for '565'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '565'.
[indexDocument][L3] Recorded doc length 133 for '565'.
[indexDocument][L3] Tokens added to the inverted index for '565'.
[indexDocument][L2] Indexing document '757'.
[indexDocument][L3] Opened file '757'.
[preprocess][L2] Starting preprocessing for '757'.
[preprocess][L3] 757: Tokenized into 338 tokens.
[preprocess][L3] 757: Removed stop words; 185 tokens remain.
[preprocess][L3] 757: Stemming complete.
[indexDocument][L3] Preprocessing complete for '757'; 185 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '757'.
[indexDocument][L3] Recorded doc length 185 for '757'.
[indexDocument][L3] Tokens added to the inverted index for '757'.
[indexDocument][L2] Indexing document '1018'.
[indexDocument][L3] Opened file '1018'.
[preprocess][L2] Starting preprocessing for '1018'.
[preprocess][L3] 1018: Tokenized into 124 tokens.
[preprocess][L3] 1018: Removed stop words; 77 tokens remain.
[preprocess][L3] 1018: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1018'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1018'.
[indexDocument][L3] Recorded doc length 77 for '1018'.
[indexDocument][L3] Tokens added to the inverted index for '1018'.
[indexDocument][L2] Indexing document '768'.
[indexDocument][L3] Opened file '768'.
[preprocess][L2] Starting preprocessing for '768'.
[preprocess][L3] 768: Tokenized into 90 tokens.
[preprocess][L3] 768: Removed stop words; 51 tokens remain.
[preprocess][L3] 768: Stemming complete.
[indexDocument][L3] Preprocessing complete for '768'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '768'.
[indexDocument][L3] Recorded doc length 51 for '768'.
[indexDocument][L3] Tokens added to the inverted index for '768'.
[indexDocument][L2] Indexing document '902'.
[indexDocument][L3] Opened file '902'.
[preprocess][L2] Starting preprocessing for '902'.
[preprocess][L3] 902: Tokenized into 231 tokens.
[preprocess][L3] 902: Removed stop words; 119 tokens remain.
[preprocess][L3] 902: Stemming complete.
[indexDocument][L3] Preprocessing complete for '902'; 119 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '902'.
[indexDocument][L3] Recorded doc length 119 for '902'.
[indexDocument][L3] Tokens added to the inverted index for '902'.
[indexDocument][L2] Indexing document '358'.
[indexDocument][L3] Opened file '358'.
[preprocess][L2] Starting preprocessing for '358'.
[preprocess][L3] 358: Tokenized into 92 tokens.
[preprocess][L3] 358: Removed stop words; 55 tokens remain.
[preprocess][L3] 358: Stemming complete.
[indexDocument][L3] Preprocessing complete for '358'; 55 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '358'.
[indexDocument][L3] Recorded doc length 55 for '358'.
[indexDocument][L3] Tokens added to the inverted index for '358'.
[indexDocument][L2] Indexing document '956'.
[indexDocument][L3] Opened file '956'.
[preprocess][L2] Starting preprocessing for '956'.
[preprocess][L3] 956: Tokenized into 126 tokens.
[preprocess][L3] 956: Removed stop words; 86 tokens remain.
[preprocess][L3] 956: Stemming complete.
[indexDocument][L3] Preprocessing complete for '956'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '956'.
[indexDocument][L3] Recorded doc length 86 for '956'.
[indexDocument][L3] Tokens added to the inverted index for '956'.
[indexDocument][L2] Indexing document '1073'.
[indexDocument][L3] Opened file '1073'.
[preprocess][L2] Starting preprocessing for '1073'.
[preprocess][L3] 1073: Tokenized into 69 tokens.
[preprocess][L3] 1073: Removed stop words; 43 tokens remain.
[preprocess][L3] 1073: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1073'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1073'.
[indexDocument][L3] Recorded doc length 43 for '1073'.
[indexDocument][L3] Tokens added to the inverted index for '1073'.
[indexDocument][L2] Indexing document '393'.
[indexDocument][L3] Opened file '393'.
[preprocess][L2] Starting preprocessing for '393'.
[preprocess][L3] 393: Tokenized into 59 tokens.
[preprocess][L3] 393: Removed stop words; 40 tokens remain.
[preprocess][L3] 393: Stemming complete.
[indexDocument][L3] Preprocessing complete for '393'; 40 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '393'.
[indexDocument][L3] Recorded doc length 40 for '393'.
[indexDocument][L3] Tokens added to the inverted index for '393'.
[indexDocument][L2] Indexing document '1241'.
[indexDocument][L3] Opened file '1241'.
[preprocess][L2] Starting preprocessing for '1241'.
[preprocess][L3] 1241: Tokenized into 166 tokens.
[preprocess][L3] 1241: Removed stop words; 99 tokens remain.
[preprocess][L3] 1241: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1241'; 99 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1241'.
[indexDocument][L3] Recorded doc length 99 for '1241'.
[indexDocument][L3] Tokens added to the inverted index for '1241'.
[indexDocument][L2] Indexing document '155'.
[indexDocument][L3] Opened file '155'.
[preprocess][L2] Starting preprocessing for '155'.
[preprocess][L3] 155: Tokenized into 270 tokens.
[preprocess][L3] 155: Removed stop words; 138 tokens remain.
[preprocess][L3] 155: Stemming complete.
[indexDocument][L3] Preprocessing complete for '155'; 138 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '155'.
[indexDocument][L3] Recorded doc length 138 for '155'.
[indexDocument][L3] Tokens added to the inverted index for '155'.
[indexDocument][L2] Indexing document '1087'.
[indexDocument][L3] Opened file '1087'.
[preprocess][L2] Starting preprocessing for '1087'.
[preprocess][L3] 1087: Tokenized into 131 tokens.
[preprocess][L3] 1087: Removed stop words; 83 tokens remain.
[preprocess][L3] 1087: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1087'; 83 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1087'.
[indexDocument][L3] Recorded doc length 83 for '1087'.
[indexDocument][L3] Tokens added to the inverted index for '1087'.
[indexDocument][L2] Indexing document '969'.
[indexDocument][L3] Opened file '969'.
[preprocess][L2] Starting preprocessing for '969'.
[preprocess][L3] 969: Tokenized into 102 tokens.
[preprocess][L3] 969: Removed stop words; 61 tokens remain.
[preprocess][L3] 969: Stemming complete.
[indexDocument][L3] Preprocessing complete for '969'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '969'.
[indexDocument][L3] Recorded doc length 61 for '969'.
[indexDocument][L3] Tokens added to the inverted index for '969'.
[indexDocument][L2] Indexing document '367'.
[indexDocument][L3] Opened file '367'.
[preprocess][L2] Starting preprocessing for '367'.
[preprocess][L3] 367: Tokenized into 124 tokens.
[preprocess][L3] 367: Removed stop words; 80 tokens remain.
[preprocess][L3] 367: Stemming complete.
[indexDocument][L3] Preprocessing complete for '367'; 80 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '367'.
[indexDocument][L3] Recorded doc length 80 for '367'.
[indexDocument][L3] Tokens added to the inverted index for '367'.
[indexDocument][L2] Indexing document '703'.
[indexDocument][L3] Opened file '703'.
[preprocess][L2] Starting preprocessing for '703'.
[preprocess][L3] 703: Tokenized into 90 tokens.
[preprocess][L3] 703: Removed stop words; 62 tokens remain.
[preprocess][L3] 703: Stemming complete.
[indexDocument][L3] Preprocessing complete for '703'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '703'.
[indexDocument][L3] Recorded doc length 62 for '703'.
[indexDocument][L3] Tokens added to the inverted index for '703'.
[indexDocument][L2] Indexing document '531'.
[indexDocument][L3] Opened file '531'.
[preprocess][L2] Starting preprocessing for '531'.
[preprocess][L3] 531: Tokenized into 143 tokens.
[preprocess][L3] 531: Removed stop words; 79 tokens remain.
[preprocess][L3] 531: Stemming complete.
[indexDocument][L3] Preprocessing complete for '531'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '531'.
[indexDocument][L3] Recorded doc length 79 for '531'.
[indexDocument][L3] Tokens added to the inverted index for '531'.
[indexDocument][L2] Indexing document '951'.
[indexDocument][L3] Opened file '951'.
[preprocess][L2] Starting preprocessing for '951'.
[preprocess][L3] 951: Tokenized into 107 tokens.
[preprocess][L3] 951: Removed stop words; 62 tokens remain.
[preprocess][L3] 951: Stemming complete.
[indexDocument][L3] Preprocessing complete for '951'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '951'.
[indexDocument][L3] Recorded doc length 62 for '951'.
[indexDocument][L3] Tokens added to the inverted index for '951'.
[indexDocument][L2] Indexing document '509'.
[indexDocument][L3] Opened file '509'.
[preprocess][L2] Starting preprocessing for '509'.
[preprocess][L3] 509: Tokenized into 68 tokens.
[preprocess][L3] 509: Removed stop words; 47 tokens remain.
[preprocess][L3] 509: Stemming complete.
[indexDocument][L3] Preprocessing complete for '509'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '509'.
[indexDocument][L3] Recorded doc length 47 for '509'.
[indexDocument][L3] Tokens added to the inverted index for '509'.
[indexDocument][L2] Indexing document '199'.
[indexDocument][L3] Opened file '199'.
[preprocess][L2] Starting preprocessing for '199'.
[preprocess][L3] 199: Tokenized into 363 tokens.
[preprocess][L3] 199: Removed stop words; 216 tokens remain.
[preprocess][L3] 199: Stemming complete.
[indexDocument][L3] Preprocessing complete for '199'; 216 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '199'.
[indexDocument][L3] Recorded doc length 216 for '199'.
[indexDocument][L3] Tokens added to the inverted index for '199'.
[indexDocument][L2] Indexing document '1279'.
[indexDocument][L3] Opened file '1279'.
[preprocess][L2] Starting preprocessing for '1279'.
[preprocess][L3] 1279: Tokenized into 170 tokens.
[preprocess][L3] 1279: Removed stop words; 100 tokens remain.
[preprocess][L3] 1279: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1279'; 100 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1279'.
[indexDocument][L3] Recorded doc length 100 for '1279'.
[indexDocument][L3] Tokens added to the inverted index for '1279'.
[indexDocument][L2] Indexing document '536'.
[indexDocument][L3] Opened file '536'.
[preprocess][L2] Starting preprocessing for '536'.
[preprocess][L3] 536: Tokenized into 304 tokens.
[preprocess][L3] 536: Removed stop words; 174 tokens remain.
[preprocess][L3] 536: Stemming complete.
[indexDocument][L3] Preprocessing complete for '536'; 174 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '536'.
[indexDocument][L3] Recorded doc length 174 for '536'.
[indexDocument][L3] Tokens added to the inverted index for '536'.
[indexDocument][L2] Indexing document '704'.
[indexDocument][L3] Opened file '704'.
[preprocess][L2] Starting preprocessing for '704'.
[preprocess][L3] 704: Tokenized into 342 tokens.
[preprocess][L3] 704: Removed stop words; 210 tokens remain.
[preprocess][L3] 704: Stemming complete.
[indexDocument][L3] Preprocessing complete for '704'; 210 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '704'.
[indexDocument][L3] Recorded doc length 210 for '704'.
[indexDocument][L3] Tokens added to the inverted index for '704'.
[indexDocument][L2] Indexing document '360'.
[indexDocument][L3] Opened file '360'.
[preprocess][L2] Starting preprocessing for '360'.
[preprocess][L3] 360: Tokenized into 161 tokens.
[preprocess][L3] 360: Removed stop words; 98 tokens remain.
[preprocess][L3] 360: Stemming complete.
[indexDocument][L3] Preprocessing complete for '360'; 98 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '360'.
[indexDocument][L3] Recorded doc length 98 for '360'.
[indexDocument][L3] Tokens added to the inverted index for '360'.
[indexDocument][L2] Indexing document '152'.
[indexDocument][L3] Opened file '152'.
[preprocess][L2] Starting preprocessing for '152'.
[preprocess][L3] 152: Tokenized into 191 tokens.
[preprocess][L3] 152: Removed stop words; 103 tokens remain.
[preprocess][L3] 152: Stemming complete.
[indexDocument][L3] Preprocessing complete for '152'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '152'.
[indexDocument][L3] Recorded doc length 103 for '152'.
[indexDocument][L3] Tokens added to the inverted index for '152'.
[indexDocument][L2] Indexing document '1080'.
[indexDocument][L3] Opened file '1080'.
[preprocess][L2] Starting preprocessing for '1080'.
[preprocess][L3] 1080: Tokenized into 124 tokens.
[preprocess][L3] 1080: Removed stop words; 83 tokens remain.
[preprocess][L3] 1080: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1080'; 83 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1080'.
[indexDocument][L3] Recorded doc length 83 for '1080'.
[indexDocument][L3] Tokens added to the inverted index for '1080'.
[indexDocument][L2] Indexing document '394'.
[indexDocument][L3] Opened file '394'.
[preprocess][L2] Starting preprocessing for '394'.
[preprocess][L3] 394: Tokenized into 74 tokens.
[preprocess][L3] 394: Removed stop words; 46 tokens remain.
[preprocess][L3] 394: Stemming complete.
[indexDocument][L3] Preprocessing complete for '394'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '394'.
[indexDocument][L3] Recorded doc length 46 for '394'.
[indexDocument][L3] Tokens added to the inverted index for '394'.
[indexDocument][L2] Indexing document '1246'.
[indexDocument][L3] Opened file '1246'.
[preprocess][L2] Starting preprocessing for '1246'.
[preprocess][L3] 1246: Tokenized into 264 tokens.
[preprocess][L3] 1246: Removed stop words; 163 tokens remain.
[preprocess][L3] 1246: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1246'; 163 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1246'.
[indexDocument][L3] Recorded doc length 163 for '1246'.
[indexDocument][L3] Tokens added to the inverted index for '1246'.
[indexDocument][L2] Indexing document '1074'.
[indexDocument][L3] Opened file '1074'.
[preprocess][L2] Starting preprocessing for '1074'.
[preprocess][L3] 1074: Tokenized into 169 tokens.
[preprocess][L3] 1074: Removed stop words; 107 tokens remain.
[preprocess][L3] 1074: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1074'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1074'.
[indexDocument][L3] Recorded doc length 107 for '1074'.
[indexDocument][L3] Tokens added to the inverted index for '1074'.
[indexDocument][L2] Indexing document '1089'.
[indexDocument][L3] Opened file '1089'.
[preprocess][L2] Starting preprocessing for '1089'.
[preprocess][L3] 1089: Tokenized into 130 tokens.
[preprocess][L3] 1089: Removed stop words; 77 tokens remain.
[preprocess][L3] 1089: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1089'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1089'.
[indexDocument][L3] Recorded doc length 77 for '1089'.
[indexDocument][L3] Tokens added to the inverted index for '1089'.
[indexDocument][L2] Indexing document '967'.
[indexDocument][L3] Opened file '967'.
[preprocess][L2] Starting preprocessing for '967'.
[preprocess][L3] 967: Tokenized into 147 tokens.
[preprocess][L3] 967: Removed stop words; 88 tokens remain.
[preprocess][L3] 967: Stemming complete.
[indexDocument][L3] Preprocessing complete for '967'; 88 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '967'.
[indexDocument][L3] Recorded doc length 88 for '967'.
[indexDocument][L3] Tokens added to the inverted index for '967'.
[indexDocument][L2] Indexing document '369'.
[indexDocument][L3] Opened file '369'.
[preprocess][L2] Starting preprocessing for '369'.
[preprocess][L3] 369: Tokenized into 232 tokens.
[preprocess][L3] 369: Removed stop words; 140 tokens remain.
[preprocess][L3] 369: Stemming complete.
[indexDocument][L3] Preprocessing complete for '369'; 140 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '369'.
[indexDocument][L3] Recorded doc length 140 for '369'.
[indexDocument][L3] Tokens added to the inverted index for '369'.
[indexDocument][L2] Indexing document '993'.
[indexDocument][L3] Opened file '993'.
[preprocess][L2] Starting preprocessing for '993'.
[preprocess][L3] 993: Tokenized into 214 tokens.
[preprocess][L3] 993: Removed stop words; 129 tokens remain.
[preprocess][L3] 993: Stemming complete.
[indexDocument][L3] Preprocessing complete for '993'; 129 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '993'.
[indexDocument][L3] Recorded doc length 129 for '993'.
[indexDocument][L3] Tokens added to the inverted index for '993'.
[indexDocument][L2] Indexing document '1284'.
[indexDocument][L3] Opened file '1284'.
[preprocess][L2] Starting preprocessing for '1284'.
[preprocess][L3] 1284: Tokenized into 161 tokens.
[preprocess][L3] 1284: Removed stop words; 92 tokens remain.
[preprocess][L3] 1284: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1284'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1284'.
[indexDocument][L3] Recorded doc length 92 for '1284'.
[indexDocument][L3] Tokens added to the inverted index for '1284'.
[indexDocument][L2] Indexing document '356'.
[indexDocument][L3] Opened file '356'.
[preprocess][L2] Starting preprocessing for '356'.
[preprocess][L3] 356: Tokenized into 90 tokens.
[preprocess][L3] 356: Removed stop words; 62 tokens remain.
[preprocess][L3] 356: Stemming complete.
[indexDocument][L3] Preprocessing complete for '356'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '356'.
[indexDocument][L3] Recorded doc length 62 for '356'.
[indexDocument][L3] Tokens added to the inverted index for '356'.
[indexDocument][L2] Indexing document '164'.
[indexDocument][L3] Opened file '164'.
[preprocess][L2] Starting preprocessing for '164'.
[preprocess][L3] 164: Tokenized into 293 tokens.
[preprocess][L3] 164: Removed stop words; 185 tokens remain.
[preprocess][L3] 164: Stemming complete.
[indexDocument][L3] Preprocessing complete for '164'; 185 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '164'.
[indexDocument][L3] Recorded doc length 185 for '164'.
[indexDocument][L3] Tokens added to the inverted index for '164'.
[indexDocument][L2] Indexing document '958'.
[indexDocument][L3] Opened file '958'.
[preprocess][L2] Starting preprocessing for '958'.
[preprocess][L3] 958: Tokenized into 112 tokens.
[preprocess][L3] 958: Removed stop words; 67 tokens remain.
[preprocess][L3] 958: Stemming complete.
[indexDocument][L3] Preprocessing complete for '958'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '958'.
[indexDocument][L3] Recorded doc length 67 for '958'.
[indexDocument][L3] Tokens added to the inverted index for '958'.
[indexDocument][L2] Indexing document '500'.
[indexDocument][L3] Opened file '500'.
[preprocess][L2] Starting preprocessing for '500'.
[preprocess][L3] 500: Tokenized into 109 tokens.
[preprocess][L3] 500: Removed stop words; 72 tokens remain.
[preprocess][L3] 500: Stemming complete.
[indexDocument][L3] Preprocessing complete for '500'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '500'.
[indexDocument][L3] Recorded doc length 72 for '500'.
[indexDocument][L3] Tokens added to the inverted index for '500'.
[indexDocument][L2] Indexing document '732'.
[indexDocument][L3] Opened file '732'.
[preprocess][L2] Starting preprocessing for '732'.
[preprocess][L3] 732: Tokenized into 188 tokens.
[preprocess][L3] 732: Removed stop words; 109 tokens remain.
[preprocess][L3] 732: Stemming complete.
[indexDocument][L3] Preprocessing complete for '732'; 109 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '732'.
[indexDocument][L3] Recorded doc length 109 for '732'.
[indexDocument][L3] Tokens added to the inverted index for '732'.
[indexDocument][L2] Indexing document '1270'.
[indexDocument][L3] Opened file '1270'.
[preprocess][L2] Starting preprocessing for '1270'.
[preprocess][L3] 1270: Tokenized into 138 tokens.
[preprocess][L3] 1270: Removed stop words; 90 tokens remain.
[preprocess][L3] 1270: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1270'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1270'.
[indexDocument][L3] Recorded doc length 90 for '1270'.
[indexDocument][L3] Tokens added to the inverted index for '1270'.
[indexDocument][L2] Indexing document '1042'.
[indexDocument][L3] Opened file '1042'.
[preprocess][L2] Starting preprocessing for '1042'.
[preprocess][L3] 1042: Tokenized into 84 tokens.
[preprocess][L3] 1042: Removed stop words; 48 tokens remain.
[preprocess][L3] 1042: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1042'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1042'.
[indexDocument][L3] Recorded doc length 48 for '1042'.
[indexDocument][L3] Tokens added to the inverted index for '1042'.
[indexDocument][L2] Indexing document '190'.
[indexDocument][L3] Opened file '190'.
[preprocess][L2] Starting preprocessing for '190'.
[preprocess][L3] 190: Tokenized into 173 tokens.
[preprocess][L3] 190: Removed stop words; 101 tokens remain.
[preprocess][L3] 190: Stemming complete.
[indexDocument][L3] Preprocessing complete for '190'; 101 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '190'.
[indexDocument][L3] Recorded doc length 101 for '190'.
[indexDocument][L3] Tokens added to the inverted index for '190'.
[indexDocument][L2] Indexing document '1248'.
[indexDocument][L3] Opened file '1248'.
[preprocess][L2] Starting preprocessing for '1248'.
[preprocess][L3] 1248: Tokenized into 376 tokens.
[preprocess][L3] 1248: Removed stop words; 222 tokens remain.
[preprocess][L3] 1248: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1248'; 222 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1248'.
[indexDocument][L3] Recorded doc length 222 for '1248'.
[indexDocument][L3] Tokens added to the inverted index for '1248'.
[indexDocument][L2] Indexing document '994'.
[indexDocument][L3] Opened file '994'.
[preprocess][L2] Starting preprocessing for '994'.
[preprocess][L3] 994: Tokenized into 187 tokens.
[preprocess][L3] 994: Removed stop words; 113 tokens remain.
[preprocess][L3] 994: Stemming complete.
[indexDocument][L3] Preprocessing complete for '994'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '994'.
[indexDocument][L3] Recorded doc length 113 for '994'.
[indexDocument][L3] Tokens added to the inverted index for '994'.
[indexDocument][L2] Indexing document '960'.
[indexDocument][L3] Opened file '960'.
[preprocess][L2] Starting preprocessing for '960'.
[preprocess][L3] 960: Tokenized into 151 tokens.
[preprocess][L3] 960: Removed stop words; 84 tokens remain.
[preprocess][L3] 960: Stemming complete.
[indexDocument][L3] Preprocessing complete for '960'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '960'.
[indexDocument][L3] Recorded doc length 84 for '960'.
[indexDocument][L3] Tokens added to the inverted index for '960'.
[indexDocument][L2] Indexing document '538'.
[indexDocument][L3] Opened file '538'.
[preprocess][L2] Starting preprocessing for '538'.
[preprocess][L3] 538: Tokenized into 175 tokens.
[preprocess][L3] 538: Removed stop words; 106 tokens remain.
[preprocess][L3] 538: Stemming complete.
[indexDocument][L3] Preprocessing complete for '538'; 106 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '538'.
[indexDocument][L3] Recorded doc length 106 for '538'.
[indexDocument][L3] Tokens added to the inverted index for '538'.
[indexDocument][L2] Indexing document '1045'.
[indexDocument][L3] Opened file '1045'.
[preprocess][L2] Starting preprocessing for '1045'.
[preprocess][L3] 1045: Tokenized into 32 tokens.
[preprocess][L3] 1045: Removed stop words; 24 tokens remain.
[preprocess][L3] 1045: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1045'; 24 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1045'.
[indexDocument][L3] Recorded doc length 24 for '1045'.
[indexDocument][L3] Tokens added to the inverted index for '1045'.
[indexDocument][L2] Indexing document '197'.
[indexDocument][L3] Opened file '197'.
[preprocess][L2] Starting preprocessing for '197'.
[preprocess][L3] 197: Tokenized into 290 tokens.
[preprocess][L3] 197: Removed stop words; 170 tokens remain.
[preprocess][L3] 197: Stemming complete.
[indexDocument][L3] Preprocessing complete for '197'; 170 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '197'.
[indexDocument][L3] Recorded doc length 170 for '197'.
[indexDocument][L3] Tokens added to the inverted index for '197'.
[indexDocument][L2] Indexing document '1277'.
[indexDocument][L3] Opened file '1277'.
[preprocess][L2] Starting preprocessing for '1277'.
[preprocess][L3] 1277: Tokenized into 272 tokens.
[preprocess][L3] 1277: Removed stop words; 155 tokens remain.
[preprocess][L3] 1277: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1277'; 155 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1277'.
[indexDocument][L3] Recorded doc length 155 for '1277'.
[indexDocument][L3] Tokens added to the inverted index for '1277'.
[indexDocument][L2] Indexing document '735'.
[indexDocument][L3] Opened file '735'.
[preprocess][L2] Starting preprocessing for '735'.
[preprocess][L3] 735: Tokenized into 62 tokens.
[preprocess][L3] 735: Removed stop words; 42 tokens remain.
[preprocess][L3] 735: Stemming complete.
[indexDocument][L3] Preprocessing complete for '735'; 42 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '735'.
[indexDocument][L3] Recorded doc length 42 for '735'.
[indexDocument][L3] Tokens added to the inverted index for '735'.
[indexDocument][L2] Indexing document '507'.
[indexDocument][L3] Opened file '507'.
[preprocess][L2] Starting preprocessing for '507'.
[preprocess][L3] 507: Tokenized into 37 tokens.
[preprocess][L3] 507: Removed stop words; 25 tokens remain.
[preprocess][L3] 507: Stemming complete.
[indexDocument][L3] Preprocessing complete for '507'; 25 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '507'.
[indexDocument][L3] Recorded doc length 25 for '507'.
[indexDocument][L3] Tokens added to the inverted index for '507'.
[indexDocument][L2] Indexing document '163'.
[indexDocument][L3] Opened file '163'.
[preprocess][L2] Starting preprocessing for '163'.
[preprocess][L3] 163: Tokenized into 379 tokens.
[preprocess][L3] 163: Removed stop words; 242 tokens remain.
[preprocess][L3] 163: Stemming complete.
[indexDocument][L3] Preprocessing complete for '163'; 242 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '163'.
[indexDocument][L3] Recorded doc length 242 for '163'.
[indexDocument][L3] Tokens added to the inverted index for '163'.
[indexDocument][L2] Indexing document '1283'.
[indexDocument][L3] Opened file '1283'.
[preprocess][L2] Starting preprocessing for '1283'.
[preprocess][L3] 1283: Tokenized into 86 tokens.
[preprocess][L3] 1283: Removed stop words; 51 tokens remain.
[preprocess][L3] 1283: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1283'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1283'.
[indexDocument][L3] Recorded doc length 51 for '1283'.
[indexDocument][L3] Tokens added to the inverted index for '1283'.
[indexDocument][L2] Indexing document '351'.
[indexDocument][L3] Opened file '351'.
[preprocess][L2] Starting preprocessing for '351'.
[preprocess][L3] 351: Tokenized into 133 tokens.
[preprocess][L3] 351: Removed stop words; 87 tokens remain.
[preprocess][L3] 351: Stemming complete.
[indexDocument][L3] Preprocessing complete for '351'; 87 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '351'.
[indexDocument][L3] Recorded doc length 87 for '351'.
[indexDocument][L3] Tokens added to the inverted index for '351'.
[indexDocument][L2] Indexing document '100'.
[indexDocument][L3] Opened file '100'.
[preprocess][L2] Starting preprocessing for '100'.
[preprocess][L3] 100: Tokenized into 241 tokens.
[preprocess][L3] 100: Removed stop words; 144 tokens remain.
[preprocess][L3] 100: Stemming complete.
[indexDocument][L3] Preprocessing complete for '100'; 144 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '100'.
[indexDocument][L3] Recorded doc length 144 for '100'.
[indexDocument][L3] Tokens added to the inverted index for '100'.
[indexDocument][L2] Indexing document '332'.
[indexDocument][L3] Opened file '332'.
[preprocess][L2] Starting preprocessing for '332'.
[preprocess][L3] 332: Tokenized into 195 tokens.
[preprocess][L3] 332: Removed stop words; 117 tokens remain.
[preprocess][L3] 332: Stemming complete.
[indexDocument][L3] Preprocessing complete for '332'; 117 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '332'.
[indexDocument][L3] Recorded doc length 117 for '332'.
[indexDocument][L3] Tokens added to the inverted index for '332'.
[indexDocument][L2] Indexing document '756'.
[indexDocument][L3] Opened file '756'.
[preprocess][L2] Starting preprocessing for '756'.
[preprocess][L3] 756: Tokenized into 175 tokens.
[preprocess][L3] 756: Removed stop words; 103 tokens remain.
[preprocess][L3] 756: Stemming complete.
[indexDocument][L3] Preprocessing complete for '756'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '756'.
[indexDocument][L3] Recorded doc length 103 for '756'.
[indexDocument][L3] Tokens added to the inverted index for '756'.
[indexDocument][L2] Indexing document '564'.
[indexDocument][L3] Opened file '564'.
[preprocess][L2] Starting preprocessing for '564'.
[preprocess][L3] 564: Tokenized into 255 tokens.
[preprocess][L3] 564: Removed stop words; 160 tokens remain.
[preprocess][L3] 564: Stemming complete.
[indexDocument][L3] Preprocessing complete for '564'; 160 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '564'.
[indexDocument][L3] Recorded doc length 160 for '564'.
[indexDocument][L3] Tokens added to the inverted index for '564'.
[indexDocument][L2] Indexing document '590'.
[indexDocument][L3] Opened file '590'.
[preprocess][L2] Starting preprocessing for '590'.
[preprocess][L3] 590: Tokenized into 72 tokens.
[preprocess][L3] 590: Removed stop words; 50 tokens remain.
[preprocess][L3] 590: Stemming complete.
[indexDocument][L3] Preprocessing complete for '590'; 50 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '590'.
[indexDocument][L3] Recorded doc length 50 for '590'.
[indexDocument][L3] Tokens added to the inverted index for '590'.
[indexDocument][L2] Indexing document '1026'.
[indexDocument][L3] Opened file '1026'.
[preprocess][L2] Starting preprocessing for '1026'.
[preprocess][L3] 1026: Tokenized into 58 tokens.
[preprocess][L3] 1026: Removed stop words; 32 tokens remain.
[preprocess][L3] 1026: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1026'; 32 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1026'.
[indexDocument][L3] Recorded doc length 32 for '1026'.
[indexDocument][L3] Tokens added to the inverted index for '1026'.
[indexDocument][L2] Indexing document '1214'.
[indexDocument][L3] Opened file '1214'.
[preprocess][L2] Starting preprocessing for '1214'.
[preprocess][L3] 1214: Tokenized into 191 tokens.
[preprocess][L3] 1214: Removed stop words; 112 tokens remain.
[preprocess][L3] 1214: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1214'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1214'.
[indexDocument][L3] Recorded doc length 112 for '1214'.
[indexDocument][L3] Tokens added to the inverted index for '1214'.
[indexDocument][L2] Indexing document '769'.
[indexDocument][L3] Opened file '769'.
[preprocess][L2] Starting preprocessing for '769'.
[preprocess][L3] 769: Tokenized into 92 tokens.
[preprocess][L3] 769: Removed stop words; 62 tokens remain.
[preprocess][L3] 769: Stemming complete.
[indexDocument][L3] Preprocessing complete for '769'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '769'.
[indexDocument][L3] Recorded doc length 62 for '769'.
[indexDocument][L3] Tokens added to the inverted index for '769'.
[indexDocument][L2] Indexing document '903'.
[indexDocument][L3] Opened file '903'.
[preprocess][L2] Starting preprocessing for '903'.
[preprocess][L3] 903: Tokenized into 252 tokens.
[preprocess][L3] 903: Removed stop words; 135 tokens remain.
[preprocess][L3] 903: Stemming complete.
[indexDocument][L3] Preprocessing complete for '903'; 135 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '903'.
[indexDocument][L3] Recorded doc length 135 for '903'.
[indexDocument][L3] Tokens added to the inverted index for '903'.
[indexDocument][L2] Indexing document '1019'.
[indexDocument][L3] Opened file '1019'.
[preprocess][L2] Starting preprocessing for '1019'.
[preprocess][L3] 1019: Tokenized into 237 tokens.
[preprocess][L3] 1019: Removed stop words; 135 tokens remain.
[preprocess][L3] 1019: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1019'; 135 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1019'.
[indexDocument][L3] Recorded doc length 135 for '1019'.
[indexDocument][L3] Tokens added to the inverted index for '1019'.
[indexDocument][L2] Indexing document '1213'.
[indexDocument][L3] Opened file '1213'.
[preprocess][L2] Starting preprocessing for '1213'.
[preprocess][L3] 1213: Tokenized into 229 tokens.
[preprocess][L3] 1213: Removed stop words; 141 tokens remain.
[preprocess][L3] 1213: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1213'; 141 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1213'.
[indexDocument][L3] Recorded doc length 141 for '1213'.
[indexDocument][L3] Tokens added to the inverted index for '1213'.
[indexDocument][L2] Indexing document '1021'.
[indexDocument][L3] Opened file '1021'.
[preprocess][L2] Starting preprocessing for '1021'.
[preprocess][L3] 1021: Tokenized into 60 tokens.
[preprocess][L3] 1021: Removed stop words; 38 tokens remain.
[preprocess][L3] 1021: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1021'; 38 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1021'.
[indexDocument][L3] Recorded doc length 38 for '1021'.
[indexDocument][L3] Tokens added to the inverted index for '1021'.
[indexDocument][L2] Indexing document '597'.
[indexDocument][L3] Opened file '597'.
[preprocess][L2] Starting preprocessing for '597'.
[preprocess][L3] 597: Tokenized into 92 tokens.
[preprocess][L3] 597: Removed stop words; 64 tokens remain.
[preprocess][L3] 597: Stemming complete.
[indexDocument][L3] Preprocessing complete for '597'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '597'.
[indexDocument][L3] Recorded doc length 64 for '597'.
[indexDocument][L3] Tokens added to the inverted index for '597'.
[indexDocument][L2] Indexing document '563'.
[indexDocument][L3] Opened file '563'.
[preprocess][L2] Starting preprocessing for '563'.
[preprocess][L3] 563: Tokenized into 272 tokens.
[preprocess][L3] 563: Removed stop words; 149 tokens remain.
[preprocess][L3] 563: Stemming complete.
[indexDocument][L3] Preprocessing complete for '563'; 149 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '563'.
[indexDocument][L3] Recorded doc length 149 for '563'.
[indexDocument][L3] Tokens added to the inverted index for '563'.
[indexDocument][L2] Indexing document '751'.
[indexDocument][L3] Opened file '751'.
[preprocess][L2] Starting preprocessing for '751'.
[preprocess][L3] 751: Tokenized into 72 tokens.
[preprocess][L3] 751: Removed stop words; 39 tokens remain.
[preprocess][L3] 751: Stemming complete.
[indexDocument][L3] Preprocessing complete for '751'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '751'.
[indexDocument][L3] Recorded doc length 39 for '751'.
[indexDocument][L3] Tokens added to the inverted index for '751'.
[indexDocument][L2] Indexing document '335'.
[indexDocument][L3] Opened file '335'.
[preprocess][L2] Starting preprocessing for '335'.
[preprocess][L3] 335: Tokenized into 99 tokens.
[preprocess][L3] 335: Removed stop words; 67 tokens remain.
[preprocess][L3] 335: Stemming complete.
[indexDocument][L3] Preprocessing complete for '335'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '335'.
[indexDocument][L3] Recorded doc length 67 for '335'.
[indexDocument][L3] Tokens added to the inverted index for '335'.
[indexDocument][L2] Indexing document '107'.
[indexDocument][L3] Opened file '107'.
[preprocess][L2] Starting preprocessing for '107'.
[preprocess][L3] 107: Tokenized into 94 tokens.
[preprocess][L3] 107: Removed stop words; 59 tokens remain.
[preprocess][L3] 107: Stemming complete.
[indexDocument][L3] Preprocessing complete for '107'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '107'.
[indexDocument][L3] Recorded doc length 59 for '107'.
[indexDocument][L3] Tokens added to the inverted index for '107'.
[indexDocument][L2] Indexing document '138'.
[indexDocument][L3] Opened file '138'.
[preprocess][L2] Starting preprocessing for '138'.
[preprocess][L3] 138: Tokenized into 239 tokens.
[preprocess][L3] 138: Removed stop words; 132 tokens remain.
[preprocess][L3] 138: Stemming complete.
[indexDocument][L3] Preprocessing complete for '138'; 132 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '138'.
[indexDocument][L3] Recorded doc length 132 for '138'.
[indexDocument][L3] Tokens added to the inverted index for '138'.
[indexDocument][L2] Indexing document '904'.
[indexDocument][L3] Opened file '904'.
[preprocess][L2] Starting preprocessing for '904'.
[preprocess][L3] 904: Tokenized into 178 tokens.
[preprocess][L3] 904: Removed stop words; 111 tokens remain.
[preprocess][L3] 904: Stemming complete.
[indexDocument][L3] Preprocessing complete for '904'; 111 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '904'.
[indexDocument][L3] Recorded doc length 111 for '904'.
[indexDocument][L3] Tokens added to the inverted index for '904'.
[indexDocument][L2] Indexing document '793'.
[indexDocument][L3] Opened file '793'.
[preprocess][L2] Starting preprocessing for '793'.
[preprocess][L3] 793: Tokenized into 158 tokens.
[preprocess][L3] 793: Removed stop words; 103 tokens remain.
[preprocess][L3] 793: Stemming complete.
[indexDocument][L3] Preprocessing complete for '793'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '793'.
[indexDocument][L3] Recorded doc length 103 for '793'.
[indexDocument][L3] Tokens added to the inverted index for '793'.
[indexDocument][L2] Indexing document '1225'.
[indexDocument][L3] Opened file '1225'.
[preprocess][L2] Starting preprocessing for '1225'.
[preprocess][L3] 1225: Tokenized into 291 tokens.
[preprocess][L3] 1225: Removed stop words; 172 tokens remain.
[preprocess][L3] 1225: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1225'; 172 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1225'.
[indexDocument][L3] Recorded doc length 172 for '1225'.
[indexDocument][L3] Tokens added to the inverted index for '1225'.
[indexDocument][L2] Indexing document '1017'.
[indexDocument][L3] Opened file '1017'.
[preprocess][L2] Starting preprocessing for '1017'.
[preprocess][L3] 1017: Tokenized into 159 tokens.
[preprocess][L3] 1017: Removed stop words; 90 tokens remain.
[preprocess][L3] 1017: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1017'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1017'.
[indexDocument][L3] Recorded doc length 90 for '1017'.
[indexDocument][L3] Tokens added to the inverted index for '1017'.
[indexDocument][L2] Indexing document '303'.
[indexDocument][L3] Opened file '303'.
[preprocess][L2] Starting preprocessing for '303'.
[preprocess][L3] 303: Tokenized into 87 tokens.
[preprocess][L3] 303: Removed stop words; 63 tokens remain.
[preprocess][L3] 303: Stemming complete.
[indexDocument][L3] Preprocessing complete for '303'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '303'.
[indexDocument][L3] Recorded doc length 63 for '303'.
[indexDocument][L3] Tokens added to the inverted index for '303'.
[indexDocument][L2] Indexing document '131'.
[indexDocument][L3] Opened file '131'.
[preprocess][L2] Starting preprocessing for '131'.
[preprocess][L3] 131: Tokenized into 362 tokens.
[preprocess][L3] 131: Removed stop words; 193 tokens remain.
[preprocess][L3] 131: Stemming complete.
[indexDocument][L3] Preprocessing complete for '131'; 193 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '131'.
[indexDocument][L3] Recorded doc length 193 for '131'.
[indexDocument][L3] Tokens added to the inverted index for '131'.
[indexDocument][L2] Indexing document '555'.
[indexDocument][L3] Opened file '555'.
[preprocess][L2] Starting preprocessing for '555'.
[preprocess][L3] 555: Tokenized into 164 tokens.
[preprocess][L3] 555: Removed stop words; 104 tokens remain.
[preprocess][L3] 555: Stemming complete.
[indexDocument][L3] Preprocessing complete for '555'; 104 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '555'.
[indexDocument][L3] Recorded doc length 104 for '555'.
[indexDocument][L3] Tokens added to the inverted index for '555'.
[indexDocument][L2] Indexing document '767'.
[indexDocument][L3] Opened file '767'.
[preprocess][L2] Starting preprocessing for '767'.
[preprocess][L3] 767: Tokenized into 263 tokens.
[preprocess][L3] 767: Removed stop words; 154 tokens remain.
[preprocess][L3] 767: Stemming complete.
[indexDocument][L3] Preprocessing complete for '767'; 154 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '767'.
[indexDocument][L3] Recorded doc length 154 for '767'.
[indexDocument][L3] Tokens added to the inverted index for '767'.
[indexDocument][L2] Indexing document '1028'.
[indexDocument][L3] Opened file '1028'.
[preprocess][L2] Starting preprocessing for '1028'.
[preprocess][L3] 1028: Tokenized into 186 tokens.
[preprocess][L3] 1028: Removed stop words; 108 tokens remain.
[preprocess][L3] 1028: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1028'; 108 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1028'.
[indexDocument][L3] Recorded doc length 108 for '1028'.
[indexDocument][L3] Tokens added to the inverted index for '1028'.
[indexDocument][L2] Indexing document '758'.
[indexDocument][L3] Opened file '758'.
[preprocess][L2] Starting preprocessing for '758'.
[preprocess][L3] 758: Tokenized into 128 tokens.
[preprocess][L3] 758: Removed stop words; 81 tokens remain.
[preprocess][L3] 758: Stemming complete.
[indexDocument][L3] Preprocessing complete for '758'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '758'.
[indexDocument][L3] Recorded doc length 81 for '758'.
[indexDocument][L3] Tokens added to the inverted index for '758'.
[indexDocument][L2] Indexing document '932'.
[indexDocument][L3] Opened file '932'.
[preprocess][L2] Starting preprocessing for '932'.
[preprocess][L3] 932: Tokenized into 77 tokens.
[preprocess][L3] 932: Removed stop words; 52 tokens remain.
[preprocess][L3] 932: Stemming complete.
[indexDocument][L3] Preprocessing complete for '932'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '932'.
[indexDocument][L3] Recorded doc length 52 for '932'.
[indexDocument][L3] Tokens added to the inverted index for '932'.
[indexDocument][L2] Indexing document '760'.
[indexDocument][L3] Opened file '760'.
[preprocess][L2] Starting preprocessing for '760'.
[preprocess][L3] 760: Tokenized into 171 tokens.
[preprocess][L3] 760: Removed stop words; 107 tokens remain.
[preprocess][L3] 760: Stemming complete.
[indexDocument][L3] Preprocessing complete for '760'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '760'.
[indexDocument][L3] Recorded doc length 107 for '760'.
[indexDocument][L3] Tokens added to the inverted index for '760'.
[indexDocument][L2] Indexing document '552'.
[indexDocument][L3] Opened file '552'.
[preprocess][L2] Starting preprocessing for '552'.
[preprocess][L3] 552: Tokenized into 221 tokens.
[preprocess][L3] 552: Removed stop words; 139 tokens remain.
[preprocess][L3] 552: Stemming complete.
[indexDocument][L3] Preprocessing complete for '552'; 139 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '552'.
[indexDocument][L3] Recorded doc length 139 for '552'.
[indexDocument][L3] Tokens added to the inverted index for '552'.
[indexDocument][L2] Indexing document '136'.
[indexDocument][L3] Opened file '136'.
[preprocess][L2] Starting preprocessing for '136'.
[preprocess][L3] 136: Tokenized into 90 tokens.
[preprocess][L3] 136: Removed stop words; 62 tokens remain.
[preprocess][L3] 136: Stemming complete.
[indexDocument][L3] Preprocessing complete for '136'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '136'.
[indexDocument][L3] Recorded doc length 62 for '136'.
[indexDocument][L3] Tokens added to the inverted index for '136'.
[indexDocument][L2] Indexing document '304'.
[indexDocument][L3] Opened file '304'.
[preprocess][L2] Starting preprocessing for '304'.
[preprocess][L3] 304: Tokenized into 286 tokens.
[preprocess][L3] 304: Removed stop words; 163 tokens remain.
[preprocess][L3] 304: Stemming complete.
[indexDocument][L3] Preprocessing complete for '304'; 163 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '304'.
[indexDocument][L3] Recorded doc length 163 for '304'.
[indexDocument][L3] Tokens added to the inverted index for '304'.
[indexDocument][L2] Indexing document '1010'.
[indexDocument][L3] Opened file '1010'.
[preprocess][L2] Starting preprocessing for '1010'.
[preprocess][L3] 1010: Tokenized into 138 tokens.
[preprocess][L3] 1010: Removed stop words; 91 tokens remain.
[preprocess][L3] 1010: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1010'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1010'.
[indexDocument][L3] Recorded doc length 91 for '1010'.
[indexDocument][L3] Tokens added to the inverted index for '1010'.
[indexDocument][L2] Indexing document '1222'.
[indexDocument][L3] Opened file '1222'.
[preprocess][L2] Starting preprocessing for '1222'.
[preprocess][L3] 1222: Tokenized into 190 tokens.
[preprocess][L3] 1222: Removed stop words; 119 tokens remain.
[preprocess][L3] 1222: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1222'; 119 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1222'.
[indexDocument][L3] Recorded doc length 119 for '1222'.
[indexDocument][L3] Tokens added to the inverted index for '1222'.
[indexDocument][L2] Indexing document '794'.
[indexDocument][L3] Opened file '794'.
[preprocess][L2] Starting preprocessing for '794'.
[preprocess][L3] 794: Tokenized into 307 tokens.
[preprocess][L3] 794: Removed stop words; 184 tokens remain.
[preprocess][L3] 794: Stemming complete.
[indexDocument][L3] Preprocessing complete for '794'; 184 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '794'.
[indexDocument][L3] Recorded doc length 184 for '794'.
[indexDocument][L3] Tokens added to the inverted index for '794'.
[indexDocument][L2] Indexing document '109'.
[indexDocument][L3] Opened file '109'.
[preprocess][L2] Starting preprocessing for '109'.
[preprocess][L3] 109: Tokenized into 184 tokens.
[preprocess][L3] 109: Removed stop words; 105 tokens remain.
[preprocess][L3] 109: Stemming complete.
[indexDocument][L3] Preprocessing complete for '109'; 105 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '109'.
[indexDocument][L3] Recorded doc length 105 for '109'.
[indexDocument][L3] Tokens added to the inverted index for '109'.
[indexDocument][L2] Indexing document '935'.
[indexDocument][L3] Opened file '935'.
[preprocess][L2] Starting preprocessing for '935'.
[preprocess][L3] 935: Tokenized into 70 tokens.
[preprocess][L3] 935: Removed stop words; 45 tokens remain.
[preprocess][L3] 935: Stemming complete.
[indexDocument][L3] Preprocessing complete for '935'; 45 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '935'.
[indexDocument][L3] Recorded doc length 45 for '935'.
[indexDocument][L3] Tokens added to the inverted index for '935'.
[indexDocument][L2] Indexing document '599'.
[indexDocument][L3] Opened file '599'.
[preprocess][L2] Starting preprocessing for '599'.
[preprocess][L3] 599: Tokenized into 266 tokens.
[preprocess][L3] 599: Removed stop words; 154 tokens remain.
[preprocess][L3] 599: Stemming complete.
[indexDocument][L3] Preprocessing complete for '599'; 154 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '599'.
[indexDocument][L3] Recorded doc length 154 for '599'.
[indexDocument][L3] Tokens added to the inverted index for '599'.
[indexDocument][L2] Indexing document '961'.
[indexDocument][L3] Opened file '961'.
[preprocess][L2] Starting preprocessing for '961'.
[preprocess][L3] 961: Tokenized into 170 tokens.
[preprocess][L3] 961: Removed stop words; 107 tokens remain.
[preprocess][L3] 961: Stemming complete.
[indexDocument][L3] Preprocessing complete for '961'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '961'.
[indexDocument][L3] Recorded doc length 107 for '961'.
[indexDocument][L3] Tokens added to the inverted index for '961'.
[indexDocument][L2] Indexing document '539'.
[indexDocument][L3] Opened file '539'.
[preprocess][L2] Starting preprocessing for '539'.
[preprocess][L3] 539: Tokenized into 76 tokens.
[preprocess][L3] 539: Removed stop words; 49 tokens remain.
[preprocess][L3] 539: Stemming complete.
[indexDocument][L3] Preprocessing complete for '539'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '539'.
[indexDocument][L3] Recorded doc length 49 for '539'.
[indexDocument][L3] Tokens added to the inverted index for '539'.
[indexDocument][L2] Indexing document '995'.
[indexDocument][L3] Opened file '995'.
[preprocess][L2] Starting preprocessing for '995'.
[preprocess][L3] 995: Tokenized into 2 tokens.
[preprocess][L3] 995: Removed stop words; 2 tokens remain.
[preprocess][L3] 995: Stemming complete.
[indexDocument][L3] Preprocessing complete for '995'; 2 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '995'.
[indexDocument][L3] Recorded doc length 2 for '995'.
[indexDocument][L3] Tokens added to the inverted index for '995'.
[indexDocument][L2] Indexing document '1249'.
[indexDocument][L3] Opened file '1249'.
[preprocess][L2] Starting preprocessing for '1249'.
[preprocess][L3] 1249: Tokenized into 80 tokens.
[preprocess][L3] 1249: Removed stop words; 49 tokens remain.
[preprocess][L3] 1249: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1249'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1249'.
[indexDocument][L3] Recorded doc length 49 for '1249'.
[indexDocument][L3] Tokens added to the inverted index for '1249'.
[indexDocument][L2] Indexing document '506'.
[indexDocument][L3] Opened file '506'.
[preprocess][L2] Starting preprocessing for '506'.
[preprocess][L3] 506: Tokenized into 128 tokens.
[preprocess][L3] 506: Removed stop words; 73 tokens remain.
[preprocess][L3] 506: Stemming complete.
[indexDocument][L3] Preprocessing complete for '506'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '506'.
[indexDocument][L3] Recorded doc length 73 for '506'.
[indexDocument][L3] Tokens added to the inverted index for '506'.
[indexDocument][L2] Indexing document '734'.
[indexDocument][L3] Opened file '734'.
[preprocess][L2] Starting preprocessing for '734'.
[preprocess][L3] 734: Tokenized into 228 tokens.
[preprocess][L3] 734: Removed stop words; 120 tokens remain.
[preprocess][L3] 734: Stemming complete.
[indexDocument][L3] Preprocessing complete for '734'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '734'.
[indexDocument][L3] Recorded doc length 120 for '734'.
[indexDocument][L3] Tokens added to the inverted index for '734'.
[indexDocument][L2] Indexing document '1282'.
[indexDocument][L3] Opened file '1282'.
[preprocess][L2] Starting preprocessing for '1282'.
[preprocess][L3] 1282: Tokenized into 170 tokens.
[preprocess][L3] 1282: Removed stop words; 100 tokens remain.
[preprocess][L3] 1282: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1282'; 100 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1282'.
[indexDocument][L3] Recorded doc length 100 for '1282'.
[indexDocument][L3] Tokens added to the inverted index for '1282'.
[indexDocument][L2] Indexing document '350'.
[indexDocument][L3] Opened file '350'.
[preprocess][L2] Starting preprocessing for '350'.
[preprocess][L3] 350: Tokenized into 87 tokens.
[preprocess][L3] 350: Removed stop words; 56 tokens remain.
[preprocess][L3] 350: Stemming complete.
[indexDocument][L3] Preprocessing complete for '350'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '350'.
[indexDocument][L3] Recorded doc length 56 for '350'.
[indexDocument][L3] Tokens added to the inverted index for '350'.
[indexDocument][L2] Indexing document '162'.
[indexDocument][L3] Opened file '162'.
[preprocess][L2] Starting preprocessing for '162'.
[preprocess][L3] 162: Tokenized into 205 tokens.
[preprocess][L3] 162: Removed stop words; 122 tokens remain.
[preprocess][L3] 162: Stemming complete.
[indexDocument][L3] Preprocessing complete for '162'; 122 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '162'.
[indexDocument][L3] Recorded doc length 122 for '162'.
[indexDocument][L3] Tokens added to the inverted index for '162'.
[indexDocument][L2] Indexing document '1276'.
[indexDocument][L3] Opened file '1276'.
[preprocess][L2] Starting preprocessing for '1276'.
[preprocess][L3] 1276: Tokenized into 60 tokens.
[preprocess][L3] 1276: Removed stop words; 37 tokens remain.
[preprocess][L3] 1276: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1276'; 37 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1276'.
[indexDocument][L3] Recorded doc length 37 for '1276'.
[indexDocument][L3] Tokens added to the inverted index for '1276'.
[indexDocument][L2] Indexing document '1044'.
[indexDocument][L3] Opened file '1044'.
[preprocess][L2] Starting preprocessing for '1044'.
[preprocess][L3] 1044: Tokenized into 146 tokens.
[preprocess][L3] 1044: Removed stop words; 84 tokens remain.
[preprocess][L3] 1044: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1044'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1044'.
[indexDocument][L3] Recorded doc length 84 for '1044'.
[indexDocument][L3] Tokens added to the inverted index for '1044'.
[indexDocument][L2] Indexing document '196'.
[indexDocument][L3] Opened file '196'.
[preprocess][L2] Starting preprocessing for '196'.
[preprocess][L3] 196: Tokenized into 182 tokens.
[preprocess][L3] 196: Removed stop words; 102 tokens remain.
[preprocess][L3] 196: Stemming complete.
[indexDocument][L3] Preprocessing complete for '196'; 102 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '196'.
[indexDocument][L3] Recorded doc length 102 for '196'.
[indexDocument][L3] Tokens added to the inverted index for '196'.
[indexDocument][L2] Indexing document '992'.
[indexDocument][L3] Opened file '992'.
[preprocess][L2] Starting preprocessing for '992'.
[preprocess][L3] 992: Tokenized into 336 tokens.
[preprocess][L3] 992: Removed stop words; 184 tokens remain.
[preprocess][L3] 992: Stemming complete.
[indexDocument][L3] Preprocessing complete for '992'; 184 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '992'.
[indexDocument][L3] Recorded doc length 184 for '992'.
[indexDocument][L3] Tokens added to the inverted index for '992'.
[indexDocument][L2] Indexing document '368'.
[indexDocument][L3] Opened file '368'.
[preprocess][L2] Starting preprocessing for '368'.
[preprocess][L3] 368: Tokenized into 144 tokens.
[preprocess][L3] 368: Removed stop words; 79 tokens remain.
[preprocess][L3] 368: Stemming complete.
[indexDocument][L3] Preprocessing complete for '368'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '368'.
[indexDocument][L3] Recorded doc length 79 for '368'.
[indexDocument][L3] Tokens added to the inverted index for '368'.
[indexDocument][L2] Indexing document '966'.
[indexDocument][L3] Opened file '966'.
[preprocess][L2] Starting preprocessing for '966'.
[preprocess][L3] 966: Tokenized into 250 tokens.
[preprocess][L3] 966: Removed stop words; 147 tokens remain.
[preprocess][L3] 966: Stemming complete.
[indexDocument][L3] Preprocessing complete for '966'; 147 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '966'.
[indexDocument][L3] Recorded doc length 147 for '966'.
[indexDocument][L3] Tokens added to the inverted index for '966'.
[indexDocument][L2] Indexing document '1088'.
[indexDocument][L3] Opened file '1088'.
[preprocess][L2] Starting preprocessing for '1088'.
[preprocess][L3] 1088: Tokenized into 196 tokens.
[preprocess][L3] 1088: Removed stop words; 123 tokens remain.
[preprocess][L3] 1088: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1088'; 123 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1088'.
[indexDocument][L3] Recorded doc length 123 for '1088'.
[indexDocument][L3] Tokens added to the inverted index for '1088'.
[indexDocument][L2] Indexing document '1043'.
[indexDocument][L3] Opened file '1043'.
[preprocess][L2] Starting preprocessing for '1043'.
[preprocess][L3] 1043: Tokenized into 167 tokens.
[preprocess][L3] 1043: Removed stop words; 103 tokens remain.
[preprocess][L3] 1043: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1043'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1043'.
[indexDocument][L3] Recorded doc length 103 for '1043'.
[indexDocument][L3] Tokens added to the inverted index for '1043'.
[indexDocument][L2] Indexing document '191'.
[indexDocument][L3] Opened file '191'.
[preprocess][L2] Starting preprocessing for '191'.
[preprocess][L3] 191: Tokenized into 210 tokens.
[preprocess][L3] 191: Removed stop words; 116 tokens remain.
[preprocess][L3] 191: Stemming complete.
[indexDocument][L3] Preprocessing complete for '191'; 116 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '191'.
[indexDocument][L3] Recorded doc length 116 for '191'.
[indexDocument][L3] Tokens added to the inverted index for '191'.
[indexDocument][L2] Indexing document '1271'.
[indexDocument][L3] Opened file '1271'.
[preprocess][L2] Starting preprocessing for '1271'.
[preprocess][L3] 1271: Tokenized into 308 tokens.
[preprocess][L3] 1271: Removed stop words; 176 tokens remain.
[preprocess][L3] 1271: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1271'; 176 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1271'.
[indexDocument][L3] Recorded doc length 176 for '1271'.
[indexDocument][L3] Tokens added to the inverted index for '1271'.
[indexDocument][L2] Indexing document '959'.
[indexDocument][L3] Opened file '959'.
[preprocess][L2] Starting preprocessing for '959'.
[preprocess][L3] 959: Tokenized into 193 tokens.
[preprocess][L3] 959: Removed stop words; 126 tokens remain.
[preprocess][L3] 959: Stemming complete.
[indexDocument][L3] Preprocessing complete for '959'; 126 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '959'.
[indexDocument][L3] Recorded doc length 126 for '959'.
[indexDocument][L3] Tokens added to the inverted index for '959'.
[indexDocument][L2] Indexing document '165'.
[indexDocument][L3] Opened file '165'.
[preprocess][L2] Starting preprocessing for '165'.
[preprocess][L3] 165: Tokenized into 382 tokens.
[preprocess][L3] 165: Removed stop words; 231 tokens remain.
[preprocess][L3] 165: Stemming complete.
[indexDocument][L3] Preprocessing complete for '165'; 231 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '165'.
[indexDocument][L3] Recorded doc length 231 for '165'.
[indexDocument][L3] Tokens added to the inverted index for '165'.
[indexDocument][L2] Indexing document '1285'.
[indexDocument][L3] Opened file '1285'.
[preprocess][L2] Starting preprocessing for '1285'.
[preprocess][L3] 1285: Tokenized into 64 tokens.
[preprocess][L3] 1285: Removed stop words; 43 tokens remain.
[preprocess][L3] 1285: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1285'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1285'.
[indexDocument][L3] Recorded doc length 43 for '1285'.
[indexDocument][L3] Tokens added to the inverted index for '1285'.
[indexDocument][L2] Indexing document '357'.
[indexDocument][L3] Opened file '357'.
[preprocess][L2] Starting preprocessing for '357'.
[preprocess][L3] 357: Tokenized into 182 tokens.
[preprocess][L3] 357: Removed stop words; 108 tokens remain.
[preprocess][L3] 357: Stemming complete.
[indexDocument][L3] Preprocessing complete for '357'; 108 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '357'.
[indexDocument][L3] Recorded doc length 108 for '357'.
[indexDocument][L3] Tokens added to the inverted index for '357'.
[indexDocument][L2] Indexing document '733'.
[indexDocument][L3] Opened file '733'.
[preprocess][L2] Starting preprocessing for '733'.
[preprocess][L3] 733: Tokenized into 194 tokens.
[preprocess][L3] 733: Removed stop words; 112 tokens remain.
[preprocess][L3] 733: Stemming complete.
[indexDocument][L3] Preprocessing complete for '733'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '733'.
[indexDocument][L3] Recorded doc length 112 for '733'.
[indexDocument][L3] Tokens added to the inverted index for '733'.
[indexDocument][L2] Indexing document '501'.
[indexDocument][L3] Opened file '501'.
[preprocess][L2] Starting preprocessing for '501'.
[preprocess][L3] 501: Tokenized into 59 tokens.
[preprocess][L3] 501: Removed stop words; 40 tokens remain.
[preprocess][L3] 501: Stemming complete.
[indexDocument][L3] Preprocessing complete for '501'; 40 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '501'.
[indexDocument][L3] Recorded doc length 40 for '501'.
[indexDocument][L3] Tokens added to the inverted index for '501'.
[indexDocument][L2] Indexing document '1278'.
[indexDocument][L3] Opened file '1278'.
[preprocess][L2] Starting preprocessing for '1278'.
[preprocess][L3] 1278: Tokenized into 185 tokens.
[preprocess][L3] 1278: Removed stop words; 116 tokens remain.
[preprocess][L3] 1278: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1278'; 116 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1278'.
[indexDocument][L3] Recorded doc length 116 for '1278'.
[indexDocument][L3] Tokens added to the inverted index for '1278'.
[indexDocument][L2] Indexing document '198'.
[indexDocument][L3] Opened file '198'.
[preprocess][L2] Starting preprocessing for '198'.
[preprocess][L3] 198: Tokenized into 285 tokens.
[preprocess][L3] 198: Removed stop words; 175 tokens remain.
[preprocess][L3] 198: Stemming complete.
[indexDocument][L3] Preprocessing complete for '198'; 175 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '198'.
[indexDocument][L3] Recorded doc length 175 for '198'.
[indexDocument][L3] Tokens added to the inverted index for '198'.
[indexDocument][L2] Indexing document '950'.
[indexDocument][L3] Opened file '950'.
[preprocess][L2] Starting preprocessing for '950'.
[preprocess][L3] 950: Tokenized into 100 tokens.
[preprocess][L3] 950: Removed stop words; 68 tokens remain.
[preprocess][L3] 950: Stemming complete.
[indexDocument][L3] Preprocessing complete for '950'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '950'.
[indexDocument][L3] Recorded doc length 68 for '950'.
[indexDocument][L3] Tokens added to the inverted index for '950'.
[indexDocument][L2] Indexing document '508'.
[indexDocument][L3] Opened file '508'.
[preprocess][L2] Starting preprocessing for '508'.
[preprocess][L3] 508: Tokenized into 136 tokens.
[preprocess][L3] 508: Removed stop words; 77 tokens remain.
[preprocess][L3] 508: Stemming complete.
[indexDocument][L3] Preprocessing complete for '508'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '508'.
[indexDocument][L3] Recorded doc length 77 for '508'.
[indexDocument][L3] Tokens added to the inverted index for '508'.
[indexDocument][L2] Indexing document '1075'.
[indexDocument][L3] Opened file '1075'.
[preprocess][L2] Starting preprocessing for '1075'.
[preprocess][L3] 1075: Tokenized into 218 tokens.
[preprocess][L3] 1075: Removed stop words; 129 tokens remain.
[preprocess][L3] 1075: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1075'; 129 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1075'.
[indexDocument][L3] Recorded doc length 129 for '1075'.
[indexDocument][L3] Tokens added to the inverted index for '1075'.
[indexDocument][L2] Indexing document '395'.
[indexDocument][L3] Opened file '395'.
[preprocess][L2] Starting preprocessing for '395'.
[preprocess][L3] 395: Tokenized into 242 tokens.
[preprocess][L3] 395: Removed stop words; 147 tokens remain.
[preprocess][L3] 395: Stemming complete.
[indexDocument][L3] Preprocessing complete for '395'; 147 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '395'.
[indexDocument][L3] Recorded doc length 147 for '395'.
[indexDocument][L3] Tokens added to the inverted index for '395'.
[indexDocument][L2] Indexing document '1247'.
[indexDocument][L3] Opened file '1247'.
[preprocess][L2] Starting preprocessing for '1247'.
[preprocess][L3] 1247: Tokenized into 166 tokens.
[preprocess][L3] 1247: Removed stop words; 99 tokens remain.
[preprocess][L3] 1247: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1247'; 99 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1247'.
[indexDocument][L3] Recorded doc length 99 for '1247'.
[indexDocument][L3] Tokens added to the inverted index for '1247'.
[indexDocument][L2] Indexing document '705'.
[indexDocument][L3] Opened file '705'.
[preprocess][L2] Starting preprocessing for '705'.
[preprocess][L3] 705: Tokenized into 226 tokens.
[preprocess][L3] 705: Removed stop words; 138 tokens remain.
[preprocess][L3] 705: Stemming complete.
[indexDocument][L3] Preprocessing complete for '705'; 138 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '705'.
[indexDocument][L3] Recorded doc length 138 for '705'.
[indexDocument][L3] Tokens added to the inverted index for '705'.
[indexDocument][L2] Indexing document '537'.
[indexDocument][L3] Opened file '537'.
[preprocess][L2] Starting preprocessing for '537'.
[preprocess][L3] 537: Tokenized into 83 tokens.
[preprocess][L3] 537: Removed stop words; 57 tokens remain.
[preprocess][L3] 537: Stemming complete.
[indexDocument][L3] Preprocessing complete for '537'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '537'.
[indexDocument][L3] Recorded doc length 57 for '537'.
[indexDocument][L3] Tokens added to the inverted index for '537'.
[indexDocument][L2] Indexing document '153'.
[indexDocument][L3] Opened file '153'.
[preprocess][L2] Starting preprocessing for '153'.
[preprocess][L3] 153: Tokenized into 68 tokens.
[preprocess][L3] 153: Removed stop words; 39 tokens remain.
[preprocess][L3] 153: Stemming complete.
[indexDocument][L3] Preprocessing complete for '153'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '153'.
[indexDocument][L3] Recorded doc length 39 for '153'.
[indexDocument][L3] Tokens added to the inverted index for '153'.
[indexDocument][L2] Indexing document '1081'.
[indexDocument][L3] Opened file '1081'.
[preprocess][L2] Starting preprocessing for '1081'.
[preprocess][L3] 1081: Tokenized into 126 tokens.
[preprocess][L3] 1081: Removed stop words; 78 tokens remain.
[preprocess][L3] 1081: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1081'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1081'.
[indexDocument][L3] Recorded doc length 78 for '1081'.
[indexDocument][L3] Tokens added to the inverted index for '1081'.
[indexDocument][L2] Indexing document '361'.
[indexDocument][L3] Opened file '361'.
[preprocess][L2] Starting preprocessing for '361'.
[preprocess][L3] 361: Tokenized into 77 tokens.
[preprocess][L3] 361: Removed stop words; 48 tokens remain.
[preprocess][L3] 361: Stemming complete.
[indexDocument][L3] Preprocessing complete for '361'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '361'.
[indexDocument][L3] Recorded doc length 48 for '361'.
[indexDocument][L3] Tokens added to the inverted index for '361'.
[indexDocument][L2] Indexing document '957'.
[indexDocument][L3] Opened file '957'.
[preprocess][L2] Starting preprocessing for '957'.
[preprocess][L3] 957: Tokenized into 99 tokens.
[preprocess][L3] 957: Removed stop words; 67 tokens remain.
[preprocess][L3] 957: Stemming complete.
[indexDocument][L3] Preprocessing complete for '957'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '957'.
[indexDocument][L3] Recorded doc length 67 for '957'.
[indexDocument][L3] Tokens added to the inverted index for '957'.
[indexDocument][L2] Indexing document '359'.
[indexDocument][L3] Opened file '359'.
[preprocess][L2] Starting preprocessing for '359'.
[preprocess][L3] 359: Tokenized into 111 tokens.
[preprocess][L3] 359: Removed stop words; 70 tokens remain.
[preprocess][L3] 359: Stemming complete.
[indexDocument][L3] Preprocessing complete for '359'; 70 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '359'.
[indexDocument][L3] Recorded doc length 70 for '359'.
[indexDocument][L3] Tokens added to the inverted index for '359'.
[indexDocument][L2] Indexing document '366'.
[indexDocument][L3] Opened file '366'.
[preprocess][L2] Starting preprocessing for '366'.
[preprocess][L3] 366: Tokenized into 181 tokens.
[preprocess][L3] 366: Removed stop words; 113 tokens remain.
[preprocess][L3] 366: Stemming complete.
[indexDocument][L3] Preprocessing complete for '366'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '366'.
[indexDocument][L3] Recorded doc length 113 for '366'.
[indexDocument][L3] Tokens added to the inverted index for '366'.
[indexDocument][L2] Indexing document '968'.
[indexDocument][L3] Opened file '968'.
[preprocess][L2] Starting preprocessing for '968'.
[preprocess][L3] 968: Tokenized into 157 tokens.
[preprocess][L3] 968: Removed stop words; 111 tokens remain.
[preprocess][L3] 968: Stemming complete.
[indexDocument][L3] Preprocessing complete for '968'; 111 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '968'.
[indexDocument][L3] Recorded doc length 111 for '968'.
[indexDocument][L3] Tokens added to the inverted index for '968'.
[indexDocument][L2] Indexing document '154'.
[indexDocument][L3] Opened file '154'.
[preprocess][L2] Starting preprocessing for '154'.
[preprocess][L3] 154: Tokenized into 94 tokens.
[preprocess][L3] 154: Removed stop words; 61 tokens remain.
[preprocess][L3] 154: Stemming complete.
[indexDocument][L3] Preprocessing complete for '154'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '154'.
[indexDocument][L3] Recorded doc length 61 for '154'.
[indexDocument][L3] Tokens added to the inverted index for '154'.
[indexDocument][L2] Indexing document '1086'.
[indexDocument][L3] Opened file '1086'.
[preprocess][L2] Starting preprocessing for '1086'.
[preprocess][L3] 1086: Tokenized into 81 tokens.
[preprocess][L3] 1086: Removed stop words; 51 tokens remain.
[preprocess][L3] 1086: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1086'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1086'.
[indexDocument][L3] Recorded doc length 51 for '1086'.
[indexDocument][L3] Tokens added to the inverted index for '1086'.
[indexDocument][L2] Indexing document '530'.
[indexDocument][L3] Opened file '530'.
[preprocess][L2] Starting preprocessing for '530'.
[preprocess][L3] 530: Tokenized into 131 tokens.
[preprocess][L3] 530: Removed stop words; 77 tokens remain.
[preprocess][L3] 530: Stemming complete.
[indexDocument][L3] Preprocessing complete for '530'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '530'.
[indexDocument][L3] Recorded doc length 77 for '530'.
[indexDocument][L3] Tokens added to the inverted index for '530'.
[indexDocument][L2] Indexing document '702'.
[indexDocument][L3] Opened file '702'.
[preprocess][L2] Starting preprocessing for '702'.
[preprocess][L3] 702: Tokenized into 181 tokens.
[preprocess][L3] 702: Removed stop words; 116 tokens remain.
[preprocess][L3] 702: Stemming complete.
[indexDocument][L3] Preprocessing complete for '702'; 116 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '702'.
[indexDocument][L3] Recorded doc length 116 for '702'.
[indexDocument][L3] Tokens added to the inverted index for '702'.
[indexDocument][L2] Indexing document '392'.
[indexDocument][L3] Opened file '392'.
[preprocess][L2] Starting preprocessing for '392'.
[preprocess][L3] 392: Tokenized into 93 tokens.
[preprocess][L3] 392: Removed stop words; 63 tokens remain.
[preprocess][L3] 392: Stemming complete.
[indexDocument][L3] Preprocessing complete for '392'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '392'.
[indexDocument][L3] Recorded doc length 63 for '392'.
[indexDocument][L3] Tokens added to the inverted index for '392'.
[indexDocument][L2] Indexing document '1240'.
[indexDocument][L3] Opened file '1240'.
[preprocess][L2] Starting preprocessing for '1240'.
[preprocess][L3] 1240: Tokenized into 195 tokens.
[preprocess][L3] 1240: Removed stop words; 110 tokens remain.
[preprocess][L3] 1240: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1240'; 110 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1240'.
[indexDocument][L3] Recorded doc length 110 for '1240'.
[indexDocument][L3] Tokens added to the inverted index for '1240'.
[indexDocument][L2] Indexing document '1072'.
[indexDocument][L3] Opened file '1072'.
[preprocess][L2] Starting preprocessing for '1072'.
[preprocess][L3] 1072: Tokenized into 387 tokens.
[preprocess][L3] 1072: Removed stop words; 227 tokens remain.
[preprocess][L3] 1072: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1072'; 227 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1072'.
[indexDocument][L3] Recorded doc length 227 for '1072'.
[indexDocument][L3] Tokens added to the inverted index for '1072'.
[indexDocument][L2] Indexing document '1332'.
[indexDocument][L3] Opened file '1332'.
[preprocess][L2] Starting preprocessing for '1332'.
[preprocess][L3] 1332: Tokenized into 135 tokens.
[preprocess][L3] 1332: Removed stop words; 90 tokens remain.
[preprocess][L3] 1332: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1332'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1332'.
[indexDocument][L3] Recorded doc length 90 for '1332'.
[indexDocument][L3] Tokens added to the inverted index for '1332'.
[indexDocument][L2] Indexing document '1100'.
[indexDocument][L3] Opened file '1100'.
[preprocess][L2] Starting preprocessing for '1100'.
[preprocess][L3] 1100: Tokenized into 130 tokens.
[preprocess][L3] 1100: Removed stop words; 85 tokens remain.
[preprocess][L3] 1100: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1100'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1100'.
[indexDocument][L3] Recorded doc length 85 for '1100'.
[indexDocument][L3] Tokens added to the inverted index for '1100'.
[indexDocument][L2] Indexing document '684'.
[indexDocument][L3] Opened file '684'.
[preprocess][L2] Starting preprocessing for '684'.
[preprocess][L3] 684: Tokenized into 79 tokens.
[preprocess][L3] 684: Removed stop words; 48 tokens remain.
[preprocess][L3] 684: Stemming complete.
[indexDocument][L3] Preprocessing complete for '684'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '684'.
[indexDocument][L3] Recorded doc length 48 for '684'.
[indexDocument][L3] Tokens added to the inverted index for '684'.
[indexDocument][L2] Indexing document '442'.
[indexDocument][L3] Opened file '442'.
[preprocess][L2] Starting preprocessing for '442'.
[preprocess][L3] 442: Tokenized into 222 tokens.
[preprocess][L3] 442: Removed stop words; 129 tokens remain.
[preprocess][L3] 442: Stemming complete.
[indexDocument][L3] Preprocessing complete for '442'; 129 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '442'.
[indexDocument][L3] Recorded doc length 129 for '442'.
[indexDocument][L3] Tokens added to the inverted index for '442'.
[indexDocument][L2] Indexing document '670'.
[indexDocument][L3] Opened file '670'.
[preprocess][L2] Starting preprocessing for '670'.
[preprocess][L3] 670: Tokenized into 56 tokens.
[preprocess][L3] 670: Removed stop words; 37 tokens remain.
[preprocess][L3] 670: Stemming complete.
[indexDocument][L3] Preprocessing complete for '670'; 37 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '670'.
[indexDocument][L3] Recorded doc length 37 for '670'.
[indexDocument][L3] Tokens added to the inverted index for '670'.
[indexDocument][L2] Indexing document '214'.
[indexDocument][L3] Opened file '214'.
[preprocess][L2] Starting preprocessing for '214'.
[preprocess][L3] 214: Tokenized into 141 tokens.
[preprocess][L3] 214: Removed stop words; 74 tokens remain.
[preprocess][L3] 214: Stemming complete.
[indexDocument][L3] Preprocessing complete for '214'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '214'.
[indexDocument][L3] Recorded doc length 74 for '214'.
[indexDocument][L3] Tokens added to the inverted index for '214'.
[indexDocument][L2] Indexing document '26'.
[indexDocument][L3] Opened file '26'.
[preprocess][L2] Starting preprocessing for '26'.
[preprocess][L3] 26: Tokenized into 70 tokens.
[preprocess][L3] 26: Removed stop words; 43 tokens remain.
[preprocess][L3] 26: Stemming complete.
[indexDocument][L3] Preprocessing complete for '26'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '26'.
[indexDocument][L3] Recorded doc length 43 for '26'.
[indexDocument][L3] Tokens added to the inverted index for '26'.
[indexDocument][L2] Indexing document '489'.
[indexDocument][L3] Opened file '489'.
[preprocess][L2] Starting preprocessing for '489'.
[preprocess][L3] 489: Tokenized into 130 tokens.
[preprocess][L3] 489: Removed stop words; 86 tokens remain.
[preprocess][L3] 489: Stemming complete.
[indexDocument][L3] Preprocessing complete for '489'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '489'.
[indexDocument][L3] Recorded doc length 86 for '489'.
[indexDocument][L3] Tokens added to the inverted index for '489'.
[indexDocument][L2] Indexing document '19'.
[indexDocument][L3] Opened file '19'.
[preprocess][L2] Starting preprocessing for '19'.
[preprocess][L3] 19: Tokenized into 74 tokens.
[preprocess][L3] 19: Removed stop words; 51 tokens remain.
[preprocess][L3] 19: Stemming complete.
[indexDocument][L3] Preprocessing complete for '19'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '19'.
[indexDocument][L3] Recorded doc length 51 for '19'.
[indexDocument][L3] Tokens added to the inverted index for '19'.
[indexDocument][L2] Indexing document '825'.
[indexDocument][L3] Opened file '825'.
[preprocess][L2] Starting preprocessing for '825'.
[preprocess][L3] 825: Tokenized into 242 tokens.
[preprocess][L3] 825: Removed stop words; 136 tokens remain.
[preprocess][L3] 825: Stemming complete.
[indexDocument][L3] Preprocessing complete for '825'; 136 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '825'.
[indexDocument][L3] Recorded doc length 136 for '825'.
[indexDocument][L3] Tokens added to the inverted index for '825'.
[indexDocument][L2] Indexing document '21'.
[indexDocument][L3] Opened file '21'.
[preprocess][L2] Starting preprocessing for '21'.
[preprocess][L3] 21: Tokenized into 70 tokens.
[preprocess][L3] 21: Removed stop words; 45 tokens remain.
[preprocess][L3] 21: Stemming complete.
[indexDocument][L3] Preprocessing complete for '21'; 45 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '21'.
[indexDocument][L3] Recorded doc length 45 for '21'.
[indexDocument][L3] Tokens added to the inverted index for '21'.
[indexDocument][L2] Indexing document '213'.
[indexDocument][L3] Opened file '213'.
[preprocess][L2] Starting preprocessing for '213'.
[preprocess][L3] 213: Tokenized into 273 tokens.
[preprocess][L3] 213: Removed stop words; 168 tokens remain.
[preprocess][L3] 213: Stemming complete.
[indexDocument][L3] Preprocessing complete for '213'; 168 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '213'.
[indexDocument][L3] Recorded doc length 168 for '213'.
[indexDocument][L3] Tokens added to the inverted index for '213'.
[indexDocument][L2] Indexing document '677'.
[indexDocument][L3] Opened file '677'.
[preprocess][L2] Starting preprocessing for '677'.
[preprocess][L3] 677: Tokenized into 297 tokens.
[preprocess][L3] 677: Removed stop words; 166 tokens remain.
[preprocess][L3] 677: Stemming complete.
[indexDocument][L3] Preprocessing complete for '677'; 166 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '677'.
[indexDocument][L3] Recorded doc length 166 for '677'.
[indexDocument][L3] Tokens added to the inverted index for '677'.
[indexDocument][L2] Indexing document '445'.
[indexDocument][L3] Opened file '445'.
[preprocess][L2] Starting preprocessing for '445'.
[preprocess][L3] 445: Tokenized into 105 tokens.
[preprocess][L3] 445: Removed stop words; 62 tokens remain.
[preprocess][L3] 445: Stemming complete.
[indexDocument][L3] Preprocessing complete for '445'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '445'.
[indexDocument][L3] Recorded doc length 62 for '445'.
[indexDocument][L3] Tokens added to the inverted index for '445'.
[indexDocument][L2] Indexing document '683'.
[indexDocument][L3] Opened file '683'.
[preprocess][L2] Starting preprocessing for '683'.
[preprocess][L3] 683: Tokenized into 278 tokens.
[preprocess][L3] 683: Removed stop words; 168 tokens remain.
[preprocess][L3] 683: Stemming complete.
[indexDocument][L3] Preprocessing complete for '683'; 168 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '683'.
[indexDocument][L3] Recorded doc length 168 for '683'.
[indexDocument][L3] Tokens added to the inverted index for '683'.
[indexDocument][L2] Indexing document '1107'.
[indexDocument][L3] Opened file '1107'.
[preprocess][L2] Starting preprocessing for '1107'.
[preprocess][L3] 1107: Tokenized into 225 tokens.
[preprocess][L3] 1107: Removed stop words; 139 tokens remain.
[preprocess][L3] 1107: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1107'; 139 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1107'.
[indexDocument][L3] Recorded doc length 139 for '1107'.
[indexDocument][L3] Tokens added to the inverted index for '1107'.
[indexDocument][L2] Indexing document '1335'.
[indexDocument][L3] Opened file '1335'.
[preprocess][L2] Starting preprocessing for '1335'.
[preprocess][L3] 1335: Tokenized into 217 tokens.
[preprocess][L3] 1335: Removed stop words; 114 tokens remain.
[preprocess][L3] 1335: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1335'; 114 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1335'.
[indexDocument][L3] Recorded doc length 114 for '1335'.
[indexDocument][L3] Tokens added to the inverted index for '1335'.
[indexDocument][L2] Indexing document '648'.
[indexDocument][L3] Opened file '648'.
[preprocess][L2] Starting preprocessing for '648'.
[preprocess][L3] 648: Tokenized into 102 tokens.
[preprocess][L3] 648: Removed stop words; 62 tokens remain.
[preprocess][L3] 648: Stemming complete.
[indexDocument][L3] Preprocessing complete for '648'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '648'.
[indexDocument][L3] Recorded doc length 62 for '648'.
[indexDocument][L3] Tokens added to the inverted index for '648'.
[indexDocument][L2] Indexing document '822'.
[indexDocument][L3] Opened file '822'.
[preprocess][L2] Starting preprocessing for '822'.
[preprocess][L3] 822: Tokenized into 230 tokens.
[preprocess][L3] 822: Removed stop words; 128 tokens remain.
[preprocess][L3] 822: Stemming complete.
[indexDocument][L3] Preprocessing complete for '822'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '822'.
[indexDocument][L3] Recorded doc length 128 for '822'.
[indexDocument][L3] Tokens added to the inverted index for '822'.
[indexDocument][L2] Indexing document '1138'.
[indexDocument][L3] Opened file '1138'.
[preprocess][L2] Starting preprocessing for '1138'.
[preprocess][L3] 1138: Tokenized into 73 tokens.
[preprocess][L3] 1138: Removed stop words; 48 tokens remain.
[preprocess][L3] 1138: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1138'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1138'.
[indexDocument][L3] Recorded doc length 48 for '1138'.
[indexDocument][L3] Tokens added to the inverted index for '1138'.
[indexDocument][L2] Indexing document '641'.
[indexDocument][L3] Opened file '641'.
[preprocess][L2] Starting preprocessing for '641'.
[preprocess][L3] 641: Tokenized into 132 tokens.
[preprocess][L3] 641: Removed stop words; 80 tokens remain.
[preprocess][L3] 641: Stemming complete.
[indexDocument][L3] Preprocessing complete for '641'; 80 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '641'.
[indexDocument][L3] Recorded doc length 80 for '641'.
[indexDocument][L3] Tokens added to the inverted index for '641'.
[indexDocument][L2] Indexing document '473'.
[indexDocument][L3] Opened file '473'.
[preprocess][L2] Starting preprocessing for '473'.
[preprocess][L3] 473: Tokenized into 113 tokens.
[preprocess][L3] 473: Removed stop words; 66 tokens remain.
[preprocess][L3] 473: Stemming complete.
[indexDocument][L3] Preprocessing complete for '473'; 66 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '473'.
[indexDocument][L3] Recorded doc length 66 for '473'.
[indexDocument][L3] Tokens added to the inverted index for '473'.
[indexDocument][L2] Indexing document '17'.
[indexDocument][L3] Opened file '17'.
[preprocess][L2] Starting preprocessing for '17'.
[preprocess][L3] 17: Tokenized into 156 tokens.
[preprocess][L3] 17: Removed stop words; 92 tokens remain.
[preprocess][L3] 17: Stemming complete.
[indexDocument][L3] Preprocessing complete for '17'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '17'.
[indexDocument][L3] Recorded doc length 92 for '17'.
[indexDocument][L3] Tokens added to the inverted index for '17'.
[indexDocument][L2] Indexing document '225'.
[indexDocument][L3] Opened file '225'.
[preprocess][L2] Starting preprocessing for '225'.
[preprocess][L3] 225: Tokenized into 375 tokens.
[preprocess][L3] 225: Removed stop words; 227 tokens remain.
[preprocess][L3] 225: Stemming complete.
[indexDocument][L3] Preprocessing complete for '225'; 227 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '225'.
[indexDocument][L3] Recorded doc length 227 for '225'.
[indexDocument][L3] Tokens added to the inverted index for '225'.
[indexDocument][L2] Indexing document '1131'.
[indexDocument][L3] Opened file '1131'.
[preprocess][L2] Starting preprocessing for '1131'.
[preprocess][L3] 1131: Tokenized into 123 tokens.
[preprocess][L3] 1131: Removed stop words; 76 tokens remain.
[preprocess][L3] 1131: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1131'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1131'.
[indexDocument][L3] Recorded doc length 76 for '1131'.
[indexDocument][L3] Tokens added to the inverted index for '1131'.
[indexDocument][L2] Indexing document '1303'.
[indexDocument][L3] Opened file '1303'.
[preprocess][L2] Starting preprocessing for '1303'.
[preprocess][L3] 1303: Tokenized into 240 tokens.
[preprocess][L3] 1303: Removed stop words; 118 tokens remain.
[preprocess][L3] 1303: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1303'; 118 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1303'.
[indexDocument][L3] Recorded doc length 118 for '1303'.
[indexDocument][L3] Tokens added to the inverted index for '1303'.
[indexDocument][L2] Indexing document '487'.
[indexDocument][L3] Opened file '487'.
[preprocess][L2] Starting preprocessing for '487'.
[preprocess][L3] 487: Tokenized into 114 tokens.
[preprocess][L3] 487: Removed stop words; 69 tokens remain.
[preprocess][L3] 487: Stemming complete.
[indexDocument][L3] Preprocessing complete for '487'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '487'.
[indexDocument][L3] Recorded doc length 69 for '487'.
[indexDocument][L3] Tokens added to the inverted index for '487'.
[indexDocument][L2] Indexing document '28'.
[indexDocument][L3] Opened file '28'.
[preprocess][L2] Starting preprocessing for '28'.
[preprocess][L3] 28: Tokenized into 165 tokens.
[preprocess][L3] 28: Removed stop words; 89 tokens remain.
[preprocess][L3] 28: Stemming complete.
[indexDocument][L3] Preprocessing complete for '28'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '28'.
[indexDocument][L3] Recorded doc length 89 for '28'.
[indexDocument][L3] Tokens added to the inverted index for '28'.
[indexDocument][L2] Indexing document '814'.
[indexDocument][L3] Opened file '814'.
[preprocess][L2] Starting preprocessing for '814'.
[preprocess][L3] 814: Tokenized into 206 tokens.
[preprocess][L3] 814: Removed stop words; 127 tokens remain.
[preprocess][L3] 814: Stemming complete.
[indexDocument][L3] Preprocessing complete for '814'; 127 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '814'.
[indexDocument][L3] Recorded doc length 127 for '814'.
[indexDocument][L3] Tokens added to the inverted index for '814'.
[indexDocument][L2] Indexing document '480'.
[indexDocument][L3] Opened file '480'.
[preprocess][L2] Starting preprocessing for '480'.
[preprocess][L3] 480: Tokenized into 99 tokens.
[preprocess][L3] 480: Removed stop words; 60 tokens remain.
[preprocess][L3] 480: Stemming complete.
[indexDocument][L3] Preprocessing complete for '480'; 60 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '480'.
[indexDocument][L3] Recorded doc length 60 for '480'.
[indexDocument][L3] Tokens added to the inverted index for '480'.
[indexDocument][L2] Indexing document '1304'.
[indexDocument][L3] Opened file '1304'.
[preprocess][L2] Starting preprocessing for '1304'.
[preprocess][L3] 1304: Tokenized into 166 tokens.
[preprocess][L3] 1304: Removed stop words; 92 tokens remain.
[preprocess][L3] 1304: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1304'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1304'.
[indexDocument][L3] Recorded doc length 92 for '1304'.
[indexDocument][L3] Tokens added to the inverted index for '1304'.
[indexDocument][L2] Indexing document '1136'.
[indexDocument][L3] Opened file '1136'.
[preprocess][L2] Starting preprocessing for '1136'.
[preprocess][L3] 1136: Tokenized into 197 tokens.
[preprocess][L3] 1136: Removed stop words; 131 tokens remain.
[preprocess][L3] 1136: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1136'; 131 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1136'.
[indexDocument][L3] Recorded doc length 131 for '1136'.
[indexDocument][L3] Tokens added to the inverted index for '1136'.
[indexDocument][L2] Indexing document '222'.
[indexDocument][L3] Opened file '222'.
[preprocess][L2] Starting preprocessing for '222'.
[preprocess][L3] 222: Tokenized into 205 tokens.
[preprocess][L3] 222: Removed stop words; 124 tokens remain.
[preprocess][L3] 222: Stemming complete.
[indexDocument][L3] Preprocessing complete for '222'; 124 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '222'.
[indexDocument][L3] Recorded doc length 124 for '222'.
[indexDocument][L3] Tokens added to the inverted index for '222'.
[indexDocument][L2] Indexing document '10'.
[indexDocument][L3] Opened file '10'.
[preprocess][L2] Starting preprocessing for '10'.
[preprocess][L3] 10: Tokenized into 63 tokens.
[preprocess][L3] 10: Removed stop words; 39 tokens remain.
[preprocess][L3] 10: Stemming complete.
[indexDocument][L3] Preprocessing complete for '10'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '10'.
[indexDocument][L3] Recorded doc length 39 for '10'.
[indexDocument][L3] Tokens added to the inverted index for '10'.
[indexDocument][L2] Indexing document '474'.
[indexDocument][L3] Opened file '474'.
[preprocess][L2] Starting preprocessing for '474'.
[preprocess][L3] 474: Tokenized into 170 tokens.
[preprocess][L3] 474: Removed stop words; 93 tokens remain.
[preprocess][L3] 474: Stemming complete.
[indexDocument][L3] Preprocessing complete for '474'; 93 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '474'.
[indexDocument][L3] Recorded doc length 93 for '474'.
[indexDocument][L3] Tokens added to the inverted index for '474'.
[indexDocument][L2] Indexing document '646'.
[indexDocument][L3] Opened file '646'.
[preprocess][L2] Starting preprocessing for '646'.
[preprocess][L3] 646: Tokenized into 272 tokens.
[preprocess][L3] 646: Removed stop words; 166 tokens remain.
[preprocess][L3] 646: Stemming complete.
[indexDocument][L3] Preprocessing complete for '646'; 166 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '646'.
[indexDocument][L3] Recorded doc length 166 for '646'.
[indexDocument][L3] Tokens added to the inverted index for '646'.
[indexDocument][L2] Indexing document '1109'.
[indexDocument][L3] Opened file '1109'.
[preprocess][L2] Starting preprocessing for '1109'.
[preprocess][L3] 1109: Tokenized into 136 tokens.
[preprocess][L3] 1109: Removed stop words; 89 tokens remain.
[preprocess][L3] 1109: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1109'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1109'.
[indexDocument][L3] Recorded doc length 89 for '1109'.
[indexDocument][L3] Tokens added to the inverted index for '1109'.
[indexDocument][L2] Indexing document '679'.
[indexDocument][L3] Opened file '679'.
[preprocess][L2] Starting preprocessing for '679'.
[preprocess][L3] 679: Tokenized into 190 tokens.
[preprocess][L3] 679: Removed stop words; 107 tokens remain.
[preprocess][L3] 679: Stemming complete.
[indexDocument][L3] Preprocessing complete for '679'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '679'.
[indexDocument][L3] Recorded doc length 107 for '679'.
[indexDocument][L3] Tokens added to the inverted index for '679'.
[indexDocument][L2] Indexing document '813'.
[indexDocument][L3] Opened file '813'.
[preprocess][L2] Starting preprocessing for '813'.
[preprocess][L3] 813: Tokenized into 135 tokens.
[preprocess][L3] 813: Removed stop words; 79 tokens remain.
[preprocess][L3] 813: Stemming complete.
[indexDocument][L3] Preprocessing complete for '813'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '813'.
[indexDocument][L3] Recorded doc length 79 for '813'.
[indexDocument][L3] Tokens added to the inverted index for '813'.
[indexDocument][L2] Indexing document '249'.
[indexDocument][L3] Opened file '249'.
[preprocess][L2] Starting preprocessing for '249'.
[preprocess][L3] 249: Tokenized into 165 tokens.
[preprocess][L3] 249: Removed stop words; 83 tokens remain.
[preprocess][L3] 249: Stemming complete.
[indexDocument][L3] Preprocessing complete for '249'; 83 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '249'.
[indexDocument][L3] Recorded doc length 83 for '249'.
[indexDocument][L3] Tokens added to the inverted index for '249'.
[indexDocument][L2] Indexing document '847'.
[indexDocument][L3] Opened file '847'.
[preprocess][L2] Starting preprocessing for '847'.
[preprocess][L3] 847: Tokenized into 110 tokens.
[preprocess][L3] 847: Removed stop words; 64 tokens remain.
[preprocess][L3] 847: Stemming complete.
[indexDocument][L3] Preprocessing complete for '847'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '847'.
[indexDocument][L3] Recorded doc length 64 for '847'.
[indexDocument][L3] Tokens added to the inverted index for '847'.
[indexDocument][L2] Indexing document '1162'.
[indexDocument][L3] Opened file '1162'.
[preprocess][L2] Starting preprocessing for '1162'.
[preprocess][L3] 1162: Tokenized into 143 tokens.
[preprocess][L3] 1162: Removed stop words; 82 tokens remain.
[preprocess][L3] 1162: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1162'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1162'.
[indexDocument][L3] Recorded doc length 82 for '1162'.
[indexDocument][L3] Tokens added to the inverted index for '1162'.
[indexDocument][L2] Indexing document '1350'.
[indexDocument][L3] Opened file '1350'.
[preprocess][L2] Starting preprocessing for '1350'.
[preprocess][L3] 1350: Tokenized into 121 tokens.
[preprocess][L3] 1350: Removed stop words; 78 tokens remain.
[preprocess][L3] 1350: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1350'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1350'.
[indexDocument][L3] Recorded doc length 78 for '1350'.
[indexDocument][L3] Tokens added to the inverted index for '1350'.
[indexDocument][L2] Indexing document '282'.
[indexDocument][L3] Opened file '282'.
[preprocess][L2] Starting preprocessing for '282'.
[preprocess][L3] 282: Tokenized into 242 tokens.
[preprocess][L3] 282: Removed stop words; 150 tokens remain.
[preprocess][L3] 282: Stemming complete.
[indexDocument][L3] Preprocessing complete for '282'; 150 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '282'.
[indexDocument][L3] Recorded doc length 150 for '282'.
[indexDocument][L3] Tokens added to the inverted index for '282'.
[indexDocument][L2] Indexing document '878'.
[indexDocument][L3] Opened file '878'.
[preprocess][L2] Starting preprocessing for '878'.
[preprocess][L3] 878: Tokenized into 97 tokens.
[preprocess][L3] 878: Removed stop words; 54 tokens remain.
[preprocess][L3] 878: Stemming complete.
[indexDocument][L3] Preprocessing complete for '878'; 54 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '878'.
[indexDocument][L3] Recorded doc length 54 for '878'.
[indexDocument][L3] Tokens added to the inverted index for '878'.
[indexDocument][L2] Indexing document '1196'.
[indexDocument][L3] Opened file '1196'.
[preprocess][L2] Starting preprocessing for '1196'.
[preprocess][L3] 1196: Tokenized into 115 tokens.
[preprocess][L3] 1196: Removed stop words; 68 tokens remain.
[preprocess][L3] 1196: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1196'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1196'.
[indexDocument][L3] Recorded doc length 68 for '1196'.
[indexDocument][L3] Tokens added to the inverted index for '1196'.
[indexDocument][L2] Indexing document '44'.
[indexDocument][L3] Opened file '44'.
[preprocess][L2] Starting preprocessing for '44'.
[preprocess][L3] 44: Tokenized into 285 tokens.
[preprocess][L3] 44: Removed stop words; 168 tokens remain.
[preprocess][L3] 44: Stemming complete.
[indexDocument][L3] Preprocessing complete for '44'; 168 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '44'.
[indexDocument][L3] Recorded doc length 168 for '44'.
[indexDocument][L3] Tokens added to the inverted index for '44'.
[indexDocument][L2] Indexing document '276'.
[indexDocument][L3] Opened file '276'.
[preprocess][L2] Starting preprocessing for '276'.
[preprocess][L3] 276: Tokenized into 218 tokens.
[preprocess][L3] 276: Removed stop words; 118 tokens remain.
[preprocess][L3] 276: Stemming complete.
[indexDocument][L3] Preprocessing complete for '276'; 118 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '276'.
[indexDocument][L3] Recorded doc length 118 for '276'.
[indexDocument][L3] Tokens added to the inverted index for '276'.
[indexDocument][L2] Indexing document '612'.
[indexDocument][L3] Opened file '612'.
[preprocess][L2] Starting preprocessing for '612'.
[preprocess][L3] 612: Tokenized into 111 tokens.
[preprocess][L3] 612: Removed stop words; 66 tokens remain.
[preprocess][L3] 612: Stemming complete.
[indexDocument][L3] Preprocessing complete for '612'; 66 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '612'.
[indexDocument][L3] Recorded doc length 66 for '612'.
[indexDocument][L3] Tokens added to the inverted index for '612'.
[indexDocument][L2] Indexing document '420'.
[indexDocument][L3] Opened file '420'.
[preprocess][L2] Starting preprocessing for '420'.
[preprocess][L3] 420: Tokenized into 163 tokens.
[preprocess][L3] 420: Removed stop words; 100 tokens remain.
[preprocess][L3] 420: Stemming complete.
[indexDocument][L3] Preprocessing complete for '420'; 100 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '420'.
[indexDocument][L3] Recorded doc length 100 for '420'.
[indexDocument][L3] Tokens added to the inverted index for '420'.
[indexDocument][L2] Indexing document '840'.
[indexDocument][L3] Opened file '840'.
[preprocess][L2] Starting preprocessing for '840'.
[preprocess][L3] 840: Tokenized into 95 tokens.
[preprocess][L3] 840: Removed stop words; 59 tokens remain.
[preprocess][L3] 840: Stemming complete.
[indexDocument][L3] Preprocessing complete for '840'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '840'.
[indexDocument][L3] Recorded doc length 59 for '840'.
[indexDocument][L3] Tokens added to the inverted index for '840'.
[indexDocument][L2] Indexing document '418'.
[indexDocument][L3] Opened file '418'.
[preprocess][L2] Starting preprocessing for '418'.
[preprocess][L3] 418: Tokenized into 65 tokens.
[preprocess][L3] 418: Removed stop words; 45 tokens remain.
[preprocess][L3] 418: Stemming complete.
[indexDocument][L3] Preprocessing complete for '418'; 45 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '418'.
[indexDocument][L3] Recorded doc length 45 for '418'.
[indexDocument][L3] Tokens added to the inverted index for '418'.
[indexDocument][L2] Indexing document '88'.
[indexDocument][L3] Opened file '88'.
[preprocess][L2] Starting preprocessing for '88'.
[preprocess][L3] 88: Tokenized into 144 tokens.
[preprocess][L3] 88: Removed stop words; 85 tokens remain.
[preprocess][L3] 88: Stemming complete.
[indexDocument][L3] Preprocessing complete for '88'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '88'.
[indexDocument][L3] Recorded doc length 85 for '88'.
[indexDocument][L3] Tokens added to the inverted index for '88'.
[indexDocument][L2] Indexing document '1368'.
[indexDocument][L3] Opened file '1368'.
[preprocess][L2] Starting preprocessing for '1368'.
[preprocess][L3] 1368: Tokenized into 92 tokens.
[preprocess][L3] 1368: Removed stop words; 62 tokens remain.
[preprocess][L3] 1368: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1368'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1368'.
[indexDocument][L3] Recorded doc length 62 for '1368'.
[indexDocument][L3] Tokens added to the inverted index for '1368'.
[indexDocument][L2] Indexing document '427'.
[indexDocument][L3] Opened file '427'.
[preprocess][L2] Starting preprocessing for '427'.
[preprocess][L3] 427: Tokenized into 204 tokens.
[preprocess][L3] 427: Removed stop words; 143 tokens remain.
[preprocess][L3] 427: Stemming complete.
[indexDocument][L3] Preprocessing complete for '427'; 143 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '427'.
[indexDocument][L3] Recorded doc length 143 for '427'.
[indexDocument][L3] Tokens added to the inverted index for '427'.
[indexDocument][L2] Indexing document '615'.
[indexDocument][L3] Opened file '615'.
[preprocess][L2] Starting preprocessing for '615'.
[preprocess][L3] 615: Tokenized into 109 tokens.
[preprocess][L3] 615: Removed stop words; 67 tokens remain.
[preprocess][L3] 615: Stemming complete.
[indexDocument][L3] Preprocessing complete for '615'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '615'.
[indexDocument][L3] Recorded doc length 67 for '615'.
[indexDocument][L3] Tokens added to the inverted index for '615'.
[indexDocument][L2] Indexing document '271'.
[indexDocument][L3] Opened file '271'.
[preprocess][L2] Starting preprocessing for '271'.
[preprocess][L3] 271: Tokenized into 45 tokens.
[preprocess][L3] 271: Removed stop words; 31 tokens remain.
[preprocess][L3] 271: Stemming complete.
[indexDocument][L3] Preprocessing complete for '271'; 31 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '271'.
[indexDocument][L3] Recorded doc length 31 for '271'.
[indexDocument][L3] Tokens added to the inverted index for '271'.
[indexDocument][L2] Indexing document '1191'.
[indexDocument][L3] Opened file '1191'.
[preprocess][L2] Starting preprocessing for '1191'.
[preprocess][L3] 1191: Tokenized into 140 tokens.
[preprocess][L3] 1191: Removed stop words; 97 tokens remain.
[preprocess][L3] 1191: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1191'; 97 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1191'.
[indexDocument][L3] Recorded doc length 97 for '1191'.
[indexDocument][L3] Tokens added to the inverted index for '1191'.
[indexDocument][L2] Indexing document '43'.
[indexDocument][L3] Opened file '43'.
[preprocess][L2] Starting preprocessing for '43'.
[preprocess][L3] 43: Tokenized into 156 tokens.
[preprocess][L3] 43: Removed stop words; 92 tokens remain.
[preprocess][L3] 43: Stemming complete.
[indexDocument][L3] Preprocessing complete for '43'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '43'.
[indexDocument][L3] Recorded doc length 92 for '43'.
[indexDocument][L3] Tokens added to the inverted index for '43'.
[indexDocument][L2] Indexing document '1357'.
[indexDocument][L3] Opened file '1357'.
[preprocess][L2] Starting preprocessing for '1357'.
[preprocess][L3] 1357: Tokenized into 54 tokens.
[preprocess][L3] 1357: Removed stop words; 38 tokens remain.
[preprocess][L3] 1357: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1357'; 38 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1357'.
[indexDocument][L3] Recorded doc length 38 for '1357'.
[indexDocument][L3] Tokens added to the inverted index for '1357'.
[indexDocument][L2] Indexing document '285'.
[indexDocument][L3] Opened file '285'.
[preprocess][L2] Starting preprocessing for '285'.
[preprocess][L3] 285: Tokenized into 65 tokens.
[preprocess][L3] 285: Removed stop words; 34 tokens remain.
[preprocess][L3] 285: Stemming complete.
[indexDocument][L3] Preprocessing complete for '285'; 34 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '285'.
[indexDocument][L3] Recorded doc length 34 for '285'.
[indexDocument][L3] Tokens added to the inverted index for '285'.
[indexDocument][L2] Indexing document '1165'.
[indexDocument][L3] Opened file '1165'.
[preprocess][L2] Starting preprocessing for '1165'.
[preprocess][L3] 1165: Tokenized into 175 tokens.
[preprocess][L3] 1165: Removed stop words; 95 tokens remain.
[preprocess][L3] 1165: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1165'; 95 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1165'.
[indexDocument][L3] Recorded doc length 95 for '1165'.
[indexDocument][L3] Tokens added to the inverted index for '1165'.
[indexDocument][L2] Indexing document '876'.
[indexDocument][L3] Opened file '876'.
[preprocess][L2] Starting preprocessing for '876'.
[preprocess][L3] 876: Tokenized into 184 tokens.
[preprocess][L3] 876: Removed stop words; 100 tokens remain.
[preprocess][L3] 876: Stemming complete.
[indexDocument][L3] Preprocessing complete for '876'; 100 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '876'.
[indexDocument][L3] Recorded doc length 100 for '876'.
[indexDocument][L3] Tokens added to the inverted index for '876'.
[indexDocument][L2] Indexing document '1198'.
[indexDocument][L3] Opened file '1198'.
[preprocess][L2] Starting preprocessing for '1198'.
[preprocess][L3] 1198: Tokenized into 275 tokens.
[preprocess][L3] 1198: Removed stop words; 147 tokens remain.
[preprocess][L3] 1198: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1198'; 147 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1198'.
[indexDocument][L3] Recorded doc length 147 for '1198'.
[indexDocument][L3] Tokens added to the inverted index for '1198'.
[indexDocument][L2] Indexing document '278'.
[indexDocument][L3] Opened file '278'.
[preprocess][L2] Starting preprocessing for '278'.
[preprocess][L3] 278: Tokenized into 139 tokens.
[preprocess][L3] 278: Removed stop words; 81 tokens remain.
[preprocess][L3] 278: Stemming complete.
[indexDocument][L3] Preprocessing complete for '278'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '278'.
[indexDocument][L3] Recorded doc length 81 for '278'.
[indexDocument][L3] Tokens added to the inverted index for '278'.
[indexDocument][L2] Indexing document '882'.
[indexDocument][L3] Opened file '882'.
[preprocess][L2] Starting preprocessing for '882'.
[preprocess][L3] 882: Tokenized into 61 tokens.
[preprocess][L3] 882: Removed stop words; 39 tokens remain.
[preprocess][L3] 882: Stemming complete.
[indexDocument][L3] Preprocessing complete for '882'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '882'.
[indexDocument][L3] Recorded doc length 39 for '882'.
[indexDocument][L3] Tokens added to the inverted index for '882'.
[indexDocument][L2] Indexing document '247'.
[indexDocument][L3] Opened file '247'.
[preprocess][L2] Starting preprocessing for '247'.
[preprocess][L3] 247: Tokenized into 186 tokens.
[preprocess][L3] 247: Removed stop words; 109 tokens remain.
[preprocess][L3] 247: Stemming complete.
[indexDocument][L3] Preprocessing complete for '247'; 109 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '247'.
[indexDocument][L3] Recorded doc length 109 for '247'.
[indexDocument][L3] Tokens added to the inverted index for '247'.
[indexDocument][L2] Indexing document '1395'.
[indexDocument][L3] Opened file '1395'.
[preprocess][L2] Starting preprocessing for '1395'.
[preprocess][L3] 1395: Tokenized into 73 tokens.
[preprocess][L3] 1395: Removed stop words; 62 tokens remain.
[preprocess][L3] 1395: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1395'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1395'.
[indexDocument][L3] Recorded doc length 62 for '1395'.
[indexDocument][L3] Tokens added to the inverted index for '1395'.
[indexDocument][L2] Indexing document '849'.
[indexDocument][L3] Opened file '849'.
[preprocess][L2] Starting preprocessing for '849'.
[preprocess][L3] 849: Tokenized into 102 tokens.
[preprocess][L3] 849: Removed stop words; 54 tokens remain.
[preprocess][L3] 849: Stemming complete.
[indexDocument][L3] Preprocessing complete for '849'; 54 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '849'.
[indexDocument][L3] Recorded doc length 54 for '849'.
[indexDocument][L3] Tokens added to the inverted index for '849'.
[indexDocument][L2] Indexing document '75'.
[indexDocument][L3] Opened file '75'.
[preprocess][L2] Starting preprocessing for '75'.
[preprocess][L3] 75: Tokenized into 117 tokens.
[preprocess][L3] 75: Removed stop words; 68 tokens remain.
[preprocess][L3] 75: Stemming complete.
[indexDocument][L3] Preprocessing complete for '75'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '75'.
[indexDocument][L3] Recorded doc length 68 for '75'.
[indexDocument][L3] Tokens added to the inverted index for '75'.
[indexDocument][L2] Indexing document '411'.
[indexDocument][L3] Opened file '411'.
[preprocess][L2] Starting preprocessing for '411'.
[preprocess][L3] 411: Tokenized into 102 tokens.
[preprocess][L3] 411: Removed stop words; 62 tokens remain.
[preprocess][L3] 411: Stemming complete.
[indexDocument][L3] Preprocessing complete for '411'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '411'.
[indexDocument][L3] Recorded doc length 62 for '411'.
[indexDocument][L3] Tokens added to the inverted index for '411'.
[indexDocument][L2] Indexing document '623'.
[indexDocument][L3] Opened file '623'.
[preprocess][L2] Starting preprocessing for '623'.
[preprocess][L3] 623: Tokenized into 220 tokens.
[preprocess][L3] 623: Removed stop words; 125 tokens remain.
[preprocess][L3] 623: Stemming complete.
[indexDocument][L3] Preprocessing complete for '623'; 125 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '623'.
[indexDocument][L3] Recorded doc length 125 for '623'.
[indexDocument][L3] Tokens added to the inverted index for '623'.
[indexDocument][L2] Indexing document '1361'.
[indexDocument][L3] Opened file '1361'.
[preprocess][L2] Starting preprocessing for '1361'.
[preprocess][L3] 1361: Tokenized into 153 tokens.
[preprocess][L3] 1361: Removed stop words; 95 tokens remain.
[preprocess][L3] 1361: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1361'; 95 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1361'.
[indexDocument][L3] Recorded doc length 95 for '1361'.
[indexDocument][L3] Tokens added to the inverted index for '1361'.
[indexDocument][L2] Indexing document '81'.
[indexDocument][L3] Opened file '81'.
[preprocess][L2] Starting preprocessing for '81'.
[preprocess][L3] 81: Tokenized into 122 tokens.
[preprocess][L3] 81: Removed stop words; 75 tokens remain.
[preprocess][L3] 81: Stemming complete.
[indexDocument][L3] Preprocessing complete for '81'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '81'.
[indexDocument][L3] Recorded doc length 75 for '81'.
[indexDocument][L3] Tokens added to the inverted index for '81'.
[indexDocument][L2] Indexing document '1153'.
[indexDocument][L3] Opened file '1153'.
[preprocess][L2] Starting preprocessing for '1153'.
[preprocess][L3] 1153: Tokenized into 166 tokens.
[preprocess][L3] 1153: Removed stop words; 98 tokens remain.
[preprocess][L3] 1153: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1153'; 98 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1153'.
[indexDocument][L3] Recorded doc length 98 for '1153'.
[indexDocument][L3] Tokens added to the inverted index for '1153'.
[indexDocument][L2] Indexing document '1359'.
[indexDocument][L3] Opened file '1359'.
[preprocess][L2] Starting preprocessing for '1359'.
[preprocess][L3] 1359: Tokenized into 79 tokens.
[preprocess][L3] 1359: Removed stop words; 56 tokens remain.
[preprocess][L3] 1359: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1359'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1359'.
[indexDocument][L3] Recorded doc length 56 for '1359'.
[indexDocument][L3] Tokens added to the inverted index for '1359'.
[indexDocument][L2] Indexing document '885'.
[indexDocument][L3] Opened file '885'.
[preprocess][L2] Starting preprocessing for '885'.
[preprocess][L3] 885: Tokenized into 90 tokens.
[preprocess][L3] 885: Removed stop words; 58 tokens remain.
[preprocess][L3] 885: Stemming complete.
[indexDocument][L3] Preprocessing complete for '885'; 58 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '885'.
[indexDocument][L3] Recorded doc length 58 for '885'.
[indexDocument][L3] Tokens added to the inverted index for '885'.
[indexDocument][L2] Indexing document '871'.
[indexDocument][L3] Opened file '871'.
[preprocess][L2] Starting preprocessing for '871'.
[preprocess][L3] 871: Tokenized into 69 tokens.
[preprocess][L3] 871: Removed stop words; 41 tokens remain.
[preprocess][L3] 871: Stemming complete.
[indexDocument][L3] Preprocessing complete for '871'; 41 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '871'.
[indexDocument][L3] Recorded doc length 41 for '871'.
[indexDocument][L3] Tokens added to the inverted index for '871'.
[indexDocument][L2] Indexing document '429'.
[indexDocument][L3] Opened file '429'.
[preprocess][L2] Starting preprocessing for '429'.
[preprocess][L3] 429: Tokenized into 51 tokens.
[preprocess][L3] 429: Removed stop words; 33 tokens remain.
[preprocess][L3] 429: Stemming complete.
[indexDocument][L3] Preprocessing complete for '429'; 33 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '429'.
[indexDocument][L3] Recorded doc length 33 for '429'.
[indexDocument][L3] Tokens added to the inverted index for '429'.
[indexDocument][L2] Indexing document '86'.
[indexDocument][L3] Opened file '86'.
[preprocess][L2] Starting preprocessing for '86'.
[preprocess][L3] 86: Tokenized into 116 tokens.
[preprocess][L3] 86: Removed stop words; 79 tokens remain.
[preprocess][L3] 86: Stemming complete.
[indexDocument][L3] Preprocessing complete for '86'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '86'.
[indexDocument][L3] Recorded doc length 79 for '86'.
[indexDocument][L3] Tokens added to the inverted index for '86'.
[indexDocument][L2] Indexing document '1154'.
[indexDocument][L3] Opened file '1154'.
[preprocess][L2] Starting preprocessing for '1154'.
[preprocess][L3] 1154: Tokenized into 254 tokens.
[preprocess][L3] 1154: Removed stop words; 154 tokens remain.
[preprocess][L3] 1154: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1154'; 154 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1154'.
[indexDocument][L3] Recorded doc length 154 for '1154'.
[indexDocument][L3] Tokens added to the inverted index for '1154'.
[indexDocument][L2] Indexing document '1366'.
[indexDocument][L3] Opened file '1366'.
[preprocess][L2] Starting preprocessing for '1366'.
[preprocess][L3] 1366: Tokenized into 189 tokens.
[preprocess][L3] 1366: Removed stop words; 122 tokens remain.
[preprocess][L3] 1366: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1366'; 122 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1366'.
[indexDocument][L3] Recorded doc length 122 for '1366'.
[indexDocument][L3] Tokens added to the inverted index for '1366'.
[indexDocument][L2] Indexing document '624'.
[indexDocument][L3] Opened file '624'.
[preprocess][L2] Starting preprocessing for '624'.
[preprocess][L3] 624: Tokenized into 220 tokens.
[preprocess][L3] 624: Removed stop words; 144 tokens remain.
[preprocess][L3] 624: Stemming complete.
[indexDocument][L3] Preprocessing complete for '624'; 144 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '624'.
[indexDocument][L3] Recorded doc length 144 for '624'.
[indexDocument][L3] Tokens added to the inverted index for '624'.
[indexDocument][L2] Indexing document '416'.
[indexDocument][L3] Opened file '416'.
[preprocess][L2] Starting preprocessing for '416'.
[preprocess][L3] 416: Tokenized into 262 tokens.
[preprocess][L3] 416: Removed stop words; 158 tokens remain.
[preprocess][L3] 416: Stemming complete.
[indexDocument][L3] Preprocessing complete for '416'; 158 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '416'.
[indexDocument][L3] Recorded doc length 158 for '416'.
[indexDocument][L3] Tokens added to the inverted index for '416'.
[indexDocument][L2] Indexing document '72'.
[indexDocument][L3] Opened file '72'.
[preprocess][L2] Starting preprocessing for '72'.
[preprocess][L3] 72: Tokenized into 249 tokens.
[preprocess][L3] 72: Removed stop words; 158 tokens remain.
[preprocess][L3] 72: Stemming complete.
[indexDocument][L3] Preprocessing complete for '72'; 158 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '72'.
[indexDocument][L3] Recorded doc length 158 for '72'.
[indexDocument][L3] Tokens added to the inverted index for '72'.
[indexDocument][L2] Indexing document '240'.
[indexDocument][L3] Opened file '240'.
[preprocess][L2] Starting preprocessing for '240'.
[preprocess][L3] 240: Tokenized into 243 tokens.
[preprocess][L3] 240: Removed stop words; 150 tokens remain.
[preprocess][L3] 240: Stemming complete.
[indexDocument][L3] Preprocessing complete for '240'; 150 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '240'.
[indexDocument][L3] Recorded doc length 150 for '240'.
[indexDocument][L3] Tokens added to the inverted index for '240'.
[indexDocument][L2] Indexing document '1392'.
[indexDocument][L3] Opened file '1392'.
[preprocess][L2] Starting preprocessing for '1392'.
[preprocess][L3] 1392: Tokenized into 322 tokens.
[preprocess][L3] 1392: Removed stop words; 172 tokens remain.
[preprocess][L3] 1392: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1392'; 172 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1392'.
[indexDocument][L3] Recorded doc length 172 for '1392'.
[indexDocument][L3] Tokens added to the inverted index for '1392'.
[indexDocument][L2] Indexing document '11'.
[indexDocument][L3] Opened file '11'.
[preprocess][L2] Starting preprocessing for '11'.
[preprocess][L3] 11: Tokenized into 113 tokens.
[preprocess][L3] 11: Removed stop words; 70 tokens remain.
[preprocess][L3] 11: Stemming complete.
[indexDocument][L3] Preprocessing complete for '11'; 70 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '11'.
[indexDocument][L3] Recorded doc length 70 for '11'.
[indexDocument][L3] Tokens added to the inverted index for '11'.
[indexDocument][L2] Indexing document '223'.
[indexDocument][L3] Opened file '223'.
[preprocess][L2] Starting preprocessing for '223'.
[preprocess][L3] 223: Tokenized into 49 tokens.
[preprocess][L3] 223: Removed stop words; 30 tokens remain.
[preprocess][L3] 223: Stemming complete.
[indexDocument][L3] Preprocessing complete for '223'; 30 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '223'.
[indexDocument][L3] Recorded doc length 30 for '223'.
[indexDocument][L3] Tokens added to the inverted index for '223'.
[indexDocument][L2] Indexing document '647'.
[indexDocument][L3] Opened file '647'.
[preprocess][L2] Starting preprocessing for '647'.
[preprocess][L3] 647: Tokenized into 82 tokens.
[preprocess][L3] 647: Removed stop words; 53 tokens remain.
[preprocess][L3] 647: Stemming complete.
[indexDocument][L3] Preprocessing complete for '647'; 53 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '647'.
[indexDocument][L3] Recorded doc length 53 for '647'.
[indexDocument][L3] Tokens added to the inverted index for '647'.
[indexDocument][L2] Indexing document '475'.
[indexDocument][L3] Opened file '475'.
[preprocess][L2] Starting preprocessing for '475'.
[preprocess][L3] 475: Tokenized into 126 tokens.
[preprocess][L3] 475: Removed stop words; 71 tokens remain.
[preprocess][L3] 475: Stemming complete.
[indexDocument][L3] Preprocessing complete for '475'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '475'.
[indexDocument][L3] Recorded doc length 71 for '475'.
[indexDocument][L3] Tokens added to the inverted index for '475'.
[indexDocument][L2] Indexing document '481'.
[indexDocument][L3] Opened file '481'.
[preprocess][L2] Starting preprocessing for '481'.
[preprocess][L3] 481: Tokenized into 112 tokens.
[preprocess][L3] 481: Removed stop words; 72 tokens remain.
[preprocess][L3] 481: Stemming complete.
[indexDocument][L3] Preprocessing complete for '481'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '481'.
[indexDocument][L3] Recorded doc length 72 for '481'.
[indexDocument][L3] Tokens added to the inverted index for '481'.
[indexDocument][L2] Indexing document '1137'.
[indexDocument][L3] Opened file '1137'.
[preprocess][L2] Starting preprocessing for '1137'.
[preprocess][L3] 1137: Tokenized into 253 tokens.
[preprocess][L3] 1137: Removed stop words; 156 tokens remain.
[preprocess][L3] 1137: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1137'; 156 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1137'.
[indexDocument][L3] Recorded doc length 156 for '1137'.
[indexDocument][L3] Tokens added to the inverted index for '1137'.
[indexDocument][L2] Indexing document '1305'.
[indexDocument][L3] Opened file '1305'.
[preprocess][L2] Starting preprocessing for '1305'.
[preprocess][L3] 1305: Tokenized into 76 tokens.
[preprocess][L3] 1305: Removed stop words; 46 tokens remain.
[preprocess][L3] 1305: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1305'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1305'.
[indexDocument][L3] Recorded doc length 46 for '1305'.
[indexDocument][L3] Tokens added to the inverted index for '1305'.
[indexDocument][L2] Indexing document '678'.
[indexDocument][L3] Opened file '678'.
[preprocess][L2] Starting preprocessing for '678'.
[preprocess][L3] 678: Tokenized into 104 tokens.
[preprocess][L3] 678: Removed stop words; 65 tokens remain.
[preprocess][L3] 678: Stemming complete.
[indexDocument][L3] Preprocessing complete for '678'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '678'.
[indexDocument][L3] Recorded doc length 65 for '678'.
[indexDocument][L3] Tokens added to the inverted index for '678'.
[indexDocument][L2] Indexing document '812'.
[indexDocument][L3] Opened file '812'.
[preprocess][L2] Starting preprocessing for '812'.
[preprocess][L3] 812: Tokenized into 181 tokens.
[preprocess][L3] 812: Removed stop words; 117 tokens remain.
[preprocess][L3] 812: Stemming complete.
[indexDocument][L3] Preprocessing complete for '812'; 117 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '812'.
[indexDocument][L3] Recorded doc length 117 for '812'.
[indexDocument][L3] Tokens added to the inverted index for '812'.
[indexDocument][L2] Indexing document '1108'.
[indexDocument][L3] Opened file '1108'.
[preprocess][L2] Starting preprocessing for '1108'.
[preprocess][L3] 1108: Tokenized into 261 tokens.
[preprocess][L3] 1108: Removed stop words; 147 tokens remain.
[preprocess][L3] 1108: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1108'; 147 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1108'.
[indexDocument][L3] Recorded doc length 147 for '1108'.
[indexDocument][L3] Tokens added to the inverted index for '1108'.
[indexDocument][L2] Indexing document '1302'.
[indexDocument][L3] Opened file '1302'.
[preprocess][L2] Starting preprocessing for '1302'.
[preprocess][L3] 1302: Tokenized into 207 tokens.
[preprocess][L3] 1302: Removed stop words; 109 tokens remain.
[preprocess][L3] 1302: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1302'; 109 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1302'.
[indexDocument][L3] Recorded doc length 109 for '1302'.
[indexDocument][L3] Tokens added to the inverted index for '1302'.
[indexDocument][L2] Indexing document '1130'.
[indexDocument][L3] Opened file '1130'.
[preprocess][L2] Starting preprocessing for '1130'.
[preprocess][L3] 1130: Tokenized into 124 tokens.
[preprocess][L3] 1130: Removed stop words; 81 tokens remain.
[preprocess][L3] 1130: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1130'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1130'.
[indexDocument][L3] Recorded doc length 81 for '1130'.
[indexDocument][L3] Tokens added to the inverted index for '1130'.
[indexDocument][L2] Indexing document '486'.
[indexDocument][L3] Opened file '486'.
[preprocess][L2] Starting preprocessing for '486'.
[preprocess][L3] 486: Tokenized into 229 tokens.
[preprocess][L3] 486: Removed stop words; 141 tokens remain.
[preprocess][L3] 486: Stemming complete.
[indexDocument][L3] Preprocessing complete for '486'; 141 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '486'.
[indexDocument][L3] Recorded doc length 141 for '486'.
[indexDocument][L3] Tokens added to the inverted index for '486'.
[indexDocument][L2] Indexing document '472'.
[indexDocument][L3] Opened file '472'.
[preprocess][L2] Starting preprocessing for '472'.
[preprocess][L3] 472: Tokenized into 113 tokens.
[preprocess][L3] 472: Removed stop words; 58 tokens remain.
[preprocess][L3] 472: Stemming complete.
[indexDocument][L3] Preprocessing complete for '472'; 58 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '472'.
[indexDocument][L3] Recorded doc length 58 for '472'.
[indexDocument][L3] Tokens added to the inverted index for '472'.
[indexDocument][L2] Indexing document '640'.
[indexDocument][L3] Opened file '640'.
[preprocess][L2] Starting preprocessing for '640'.
[preprocess][L3] 640: Tokenized into 360 tokens.
[preprocess][L3] 640: Removed stop words; 198 tokens remain.
[preprocess][L3] 640: Stemming complete.
[indexDocument][L3] Preprocessing complete for '640'; 198 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '640'.
[indexDocument][L3] Recorded doc length 198 for '640'.
[indexDocument][L3] Tokens added to the inverted index for '640'.
[indexDocument][L2] Indexing document '224'.
[indexDocument][L3] Opened file '224'.
[preprocess][L2] Starting preprocessing for '224'.
[preprocess][L3] 224: Tokenized into 71 tokens.
[preprocess][L3] 224: Removed stop words; 43 tokens remain.
[preprocess][L3] 224: Stemming complete.
[indexDocument][L3] Preprocessing complete for '224'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '224'.
[indexDocument][L3] Recorded doc length 43 for '224'.
[indexDocument][L3] Tokens added to the inverted index for '224'.
[indexDocument][L2] Indexing document '16'.
[indexDocument][L3] Opened file '16'.
[preprocess][L2] Starting preprocessing for '16'.
[preprocess][L3] 16: Tokenized into 145 tokens.
[preprocess][L3] 16: Removed stop words; 83 tokens remain.
[preprocess][L3] 16: Stemming complete.
[indexDocument][L3] Preprocessing complete for '16'; 83 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '16'.
[indexDocument][L3] Recorded doc length 83 for '16'.
[indexDocument][L3] Tokens added to the inverted index for '16'.
[indexDocument][L2] Indexing document '815'.
[indexDocument][L3] Opened file '815'.
[preprocess][L2] Starting preprocessing for '815'.
[preprocess][L3] 815: Tokenized into 247 tokens.
[preprocess][L3] 815: Removed stop words; 153 tokens remain.
[preprocess][L3] 815: Stemming complete.
[indexDocument][L3] Preprocessing complete for '815'; 153 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '815'.
[indexDocument][L3] Recorded doc length 153 for '815'.
[indexDocument][L3] Tokens added to the inverted index for '815'.
[indexDocument][L2] Indexing document '29'.
[indexDocument][L3] Opened file '29'.
[preprocess][L2] Starting preprocessing for '29'.
[preprocess][L3] 29: Tokenized into 251 tokens.
[preprocess][L3] 29: Removed stop words; 152 tokens remain.
[preprocess][L3] 29: Stemming complete.
[indexDocument][L3] Preprocessing complete for '29'; 152 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '29'.
[indexDocument][L3] Recorded doc length 152 for '29'.
[indexDocument][L3] Tokens added to the inverted index for '29'.
[indexDocument][L2] Indexing document '682'.
[indexDocument][L3] Opened file '682'.
[preprocess][L2] Starting preprocessing for '682'.
[preprocess][L3] 682: Tokenized into 202 tokens.
[preprocess][L3] 682: Removed stop words; 121 tokens remain.
[preprocess][L3] 682: Stemming complete.
[indexDocument][L3] Preprocessing complete for '682'; 121 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '682'.
[indexDocument][L3] Recorded doc length 121 for '682'.
[indexDocument][L3] Tokens added to the inverted index for '682'.
[indexDocument][L2] Indexing document '1334'.
[indexDocument][L3] Opened file '1334'.
[preprocess][L2] Starting preprocessing for '1334'.
[preprocess][L3] 1334: Tokenized into 152 tokens.
[preprocess][L3] 1334: Removed stop words; 97 tokens remain.
[preprocess][L3] 1334: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1334'; 97 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1334'.
[indexDocument][L3] Recorded doc length 97 for '1334'.
[indexDocument][L3] Tokens added to the inverted index for '1334'.
[indexDocument][L2] Indexing document '1106'.
[indexDocument][L3] Opened file '1106'.
[preprocess][L2] Starting preprocessing for '1106'.
[preprocess][L3] 1106: Tokenized into 143 tokens.
[preprocess][L3] 1106: Removed stop words; 89 tokens remain.
[preprocess][L3] 1106: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1106'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1106'.
[indexDocument][L3] Recorded doc length 89 for '1106'.
[indexDocument][L3] Tokens added to the inverted index for '1106'.
[indexDocument][L2] Indexing document '212'.
[indexDocument][L3] Opened file '212'.
[preprocess][L2] Starting preprocessing for '212'.
[preprocess][L3] 212: Tokenized into 358 tokens.
[preprocess][L3] 212: Removed stop words; 217 tokens remain.
[preprocess][L3] 212: Stemming complete.
[indexDocument][L3] Preprocessing complete for '212'; 217 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '212'.
[indexDocument][L3] Recorded doc length 217 for '212'.
[indexDocument][L3] Tokens added to the inverted index for '212'.
[indexDocument][L2] Indexing document '20'.
[indexDocument][L3] Opened file '20'.
[preprocess][L2] Starting preprocessing for '20'.
[preprocess][L3] 20: Tokenized into 168 tokens.
[preprocess][L3] 20: Removed stop words; 107 tokens remain.
[preprocess][L3] 20: Stemming complete.
[indexDocument][L3] Preprocessing complete for '20'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '20'.
[indexDocument][L3] Recorded doc length 107 for '20'.
[indexDocument][L3] Tokens added to the inverted index for '20'.
[indexDocument][L2] Indexing document '444'.
[indexDocument][L3] Opened file '444'.
[preprocess][L2] Starting preprocessing for '444'.
[preprocess][L3] 444: Tokenized into 105 tokens.
[preprocess][L3] 444: Removed stop words; 59 tokens remain.
[preprocess][L3] 444: Stemming complete.
[indexDocument][L3] Preprocessing complete for '444'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '444'.
[indexDocument][L3] Recorded doc length 59 for '444'.
[indexDocument][L3] Tokens added to the inverted index for '444'.
[indexDocument][L2] Indexing document '676'.
[indexDocument][L3] Opened file '676'.
[preprocess][L2] Starting preprocessing for '676'.
[preprocess][L3] 676: Tokenized into 98 tokens.
[preprocess][L3] 676: Removed stop words; 59 tokens remain.
[preprocess][L3] 676: Stemming complete.
[indexDocument][L3] Preprocessing complete for '676'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '676'.
[indexDocument][L3] Recorded doc length 59 for '676'.
[indexDocument][L3] Tokens added to the inverted index for '676'.
[indexDocument][L2] Indexing document '1139'.
[indexDocument][L3] Opened file '1139'.
[preprocess][L2] Starting preprocessing for '1139'.
[preprocess][L3] 1139: Tokenized into 119 tokens.
[preprocess][L3] 1139: Removed stop words; 65 tokens remain.
[preprocess][L3] 1139: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1139'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1139'.
[indexDocument][L3] Recorded doc length 65 for '1139'.
[indexDocument][L3] Tokens added to the inverted index for '1139'.
[indexDocument][L2] Indexing document '649'.
[indexDocument][L3] Opened file '649'.
[preprocess][L2] Starting preprocessing for '649'.
[preprocess][L3] 649: Tokenized into 259 tokens.
[preprocess][L3] 649: Removed stop words; 143 tokens remain.
[preprocess][L3] 649: Stemming complete.
[indexDocument][L3] Preprocessing complete for '649'; 143 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '649'.
[indexDocument][L3] Recorded doc length 143 for '649'.
[indexDocument][L3] Tokens added to the inverted index for '649'.
[indexDocument][L2] Indexing document '823'.
[indexDocument][L3] Opened file '823'.
[preprocess][L2] Starting preprocessing for '823'.
[preprocess][L3] 823: Tokenized into 114 tokens.
[preprocess][L3] 823: Removed stop words; 70 tokens remain.
[preprocess][L3] 823: Stemming complete.
[indexDocument][L3] Preprocessing complete for '823'; 70 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '823'.
[indexDocument][L3] Recorded doc length 70 for '823'.
[indexDocument][L3] Tokens added to the inverted index for '823'.
[indexDocument][L2] Indexing document '671'.
[indexDocument][L3] Opened file '671'.
[preprocess][L2] Starting preprocessing for '671'.
[preprocess][L3] 671: Tokenized into 140 tokens.
[preprocess][L3] 671: Removed stop words; 85 tokens remain.
[preprocess][L3] 671: Stemming complete.
[indexDocument][L3] Preprocessing complete for '671'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '671'.
[indexDocument][L3] Recorded doc length 85 for '671'.
[indexDocument][L3] Tokens added to the inverted index for '671'.
[indexDocument][L2] Indexing document '443'.
[indexDocument][L3] Opened file '443'.
[preprocess][L2] Starting preprocessing for '443'.
[preprocess][L3] 443: Tokenized into 224 tokens.
[preprocess][L3] 443: Removed stop words; 130 tokens remain.
[preprocess][L3] 443: Stemming complete.
[indexDocument][L3] Preprocessing complete for '443'; 130 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '443'.
[indexDocument][L3] Recorded doc length 130 for '443'.
[indexDocument][L3] Tokens added to the inverted index for '443'.
[indexDocument][L2] Indexing document '27'.
[indexDocument][L3] Opened file '27'.
[preprocess][L2] Starting preprocessing for '27'.
[preprocess][L3] 27: Tokenized into 142 tokens.
[preprocess][L3] 27: Removed stop words; 82 tokens remain.
[preprocess][L3] 27: Stemming complete.
[indexDocument][L3] Preprocessing complete for '27'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '27'.
[indexDocument][L3] Recorded doc length 82 for '27'.
[indexDocument][L3] Tokens added to the inverted index for '27'.
[indexDocument][L2] Indexing document '215'.
[indexDocument][L3] Opened file '215'.
[preprocess][L2] Starting preprocessing for '215'.
[preprocess][L3] 215: Tokenized into 82 tokens.
[preprocess][L3] 215: Removed stop words; 52 tokens remain.
[preprocess][L3] 215: Stemming complete.
[indexDocument][L3] Preprocessing complete for '215'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '215'.
[indexDocument][L3] Recorded doc length 52 for '215'.
[indexDocument][L3] Tokens added to the inverted index for '215'.
[indexDocument][L2] Indexing document '1101'.
[indexDocument][L3] Opened file '1101'.
[preprocess][L2] Starting preprocessing for '1101'.
[preprocess][L3] 1101: Tokenized into 102 tokens.
[preprocess][L3] 1101: Removed stop words; 65 tokens remain.
[preprocess][L3] 1101: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1101'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1101'.
[indexDocument][L3] Recorded doc length 65 for '1101'.
[indexDocument][L3] Tokens added to the inverted index for '1101'.
[indexDocument][L2] Indexing document '1333'.
[indexDocument][L3] Opened file '1333'.
[preprocess][L2] Starting preprocessing for '1333'.
[preprocess][L3] 1333: Tokenized into 255 tokens.
[preprocess][L3] 1333: Removed stop words; 152 tokens remain.
[preprocess][L3] 1333: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1333'; 152 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1333'.
[indexDocument][L3] Recorded doc length 152 for '1333'.
[indexDocument][L3] Tokens added to the inverted index for '1333'.
[indexDocument][L2] Indexing document '685'.
[indexDocument][L3] Opened file '685'.
[preprocess][L2] Starting preprocessing for '685'.
[preprocess][L3] 685: Tokenized into 294 tokens.
[preprocess][L3] 685: Removed stop words; 184 tokens remain.
[preprocess][L3] 685: Stemming complete.
[indexDocument][L3] Preprocessing complete for '685'; 184 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '685'.
[indexDocument][L3] Recorded doc length 184 for '685'.
[indexDocument][L3] Tokens added to the inverted index for '685'.
[indexDocument][L2] Indexing document '824'.
[indexDocument][L3] Opened file '824'.
[preprocess][L2] Starting preprocessing for '824'.
[preprocess][L3] 824: Tokenized into 178 tokens.
[preprocess][L3] 824: Removed stop words; 105 tokens remain.
[preprocess][L3] 824: Stemming complete.
[indexDocument][L3] Preprocessing complete for '824'; 105 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '824'.
[indexDocument][L3] Recorded doc length 105 for '824'.
[indexDocument][L3] Tokens added to the inverted index for '824'.
[indexDocument][L2] Indexing document '18'.
[indexDocument][L3] Opened file '18'.
[preprocess][L2] Starting preprocessing for '18'.
[preprocess][L3] 18: Tokenized into 134 tokens.
[preprocess][L3] 18: Removed stop words; 73 tokens remain.
[preprocess][L3] 18: Stemming complete.
[indexDocument][L3] Preprocessing complete for '18'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '18'.
[indexDocument][L3] Recorded doc length 73 for '18'.
[indexDocument][L3] Tokens added to the inverted index for '18'.
[indexDocument][L2] Indexing document '488'.
[indexDocument][L3] Opened file '488'.
[preprocess][L2] Starting preprocessing for '488'.
[preprocess][L3] 488: Tokenized into 131 tokens.
[preprocess][L3] 488: Removed stop words; 94 tokens remain.
[preprocess][L3] 488: Stemming complete.
[indexDocument][L3] Preprocessing complete for '488'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '488'.
[indexDocument][L3] Recorded doc length 94 for '488'.
[indexDocument][L3] Tokens added to the inverted index for '488'.
[indexDocument][L2] Indexing document '870'.
[indexDocument][L3] Opened file '870'.
[preprocess][L2] Starting preprocessing for '870'.
[preprocess][L3] 870: Tokenized into 120 tokens.
[preprocess][L3] 870: Removed stop words; 80 tokens remain.
[preprocess][L3] 870: Stemming complete.
[indexDocument][L3] Preprocessing complete for '870'; 80 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '870'.
[indexDocument][L3] Recorded doc length 80 for '870'.
[indexDocument][L3] Tokens added to the inverted index for '870'.
[indexDocument][L2] Indexing document '428'.
[indexDocument][L3] Opened file '428'.
[preprocess][L2] Starting preprocessing for '428'.
[preprocess][L3] 428: Tokenized into 173 tokens.
[preprocess][L3] 428: Removed stop words; 101 tokens remain.
[preprocess][L3] 428: Stemming complete.
[indexDocument][L3] Preprocessing complete for '428'; 101 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '428'.
[indexDocument][L3] Recorded doc length 101 for '428'.
[indexDocument][L3] Tokens added to the inverted index for '428'.
[indexDocument][L2] Indexing document '884'.
[indexDocument][L3] Opened file '884'.
[preprocess][L2] Starting preprocessing for '884'.
[preprocess][L3] 884: Tokenized into 75 tokens.
[preprocess][L3] 884: Removed stop words; 49 tokens remain.
[preprocess][L3] 884: Stemming complete.
[indexDocument][L3] Preprocessing complete for '884'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '884'.
[indexDocument][L3] Recorded doc length 49 for '884'.
[indexDocument][L3] Tokens added to the inverted index for '884'.
[indexDocument][L2] Indexing document '1358'.
[indexDocument][L3] Opened file '1358'.
[preprocess][L2] Starting preprocessing for '1358'.
[preprocess][L3] 1358: Tokenized into 49 tokens.
[preprocess][L3] 1358: Removed stop words; 34 tokens remain.
[preprocess][L3] 1358: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1358'; 34 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1358'.
[indexDocument][L3] Recorded doc length 34 for '1358'.
[indexDocument][L3] Tokens added to the inverted index for '1358'.
[indexDocument][L2] Indexing document '417'.
[indexDocument][L3] Opened file '417'.
[preprocess][L2] Starting preprocessing for '417'.
[preprocess][L3] 417: Tokenized into 487 tokens.
[preprocess][L3] 417: Removed stop words; 275 tokens remain.
[preprocess][L3] 417: Stemming complete.
[indexDocument][L3] Preprocessing complete for '417'; 275 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '417'.
[indexDocument][L3] Recorded doc length 275 for '417'.
[indexDocument][L3] Tokens added to the inverted index for '417'.
[indexDocument][L2] Indexing document '625'.
[indexDocument][L3] Opened file '625'.
[preprocess][L2] Starting preprocessing for '625'.
[preprocess][L3] 625: Tokenized into 315 tokens.
[preprocess][L3] 625: Removed stop words; 194 tokens remain.
[preprocess][L3] 625: Stemming complete.
[indexDocument][L3] Preprocessing complete for '625'; 194 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '625'.
[indexDocument][L3] Recorded doc length 194 for '625'.
[indexDocument][L3] Tokens added to the inverted index for '625'.
[indexDocument][L2] Indexing document '241'.
[indexDocument][L3] Opened file '241'.
[preprocess][L2] Starting preprocessing for '241'.
[preprocess][L3] 241: Tokenized into 76 tokens.
[preprocess][L3] 241: Removed stop words; 48 tokens remain.
[preprocess][L3] 241: Stemming complete.
[indexDocument][L3] Preprocessing complete for '241'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '241'.
[indexDocument][L3] Recorded doc length 48 for '241'.
[indexDocument][L3] Tokens added to the inverted index for '241'.
[indexDocument][L2] Indexing document '1393'.
[indexDocument][L3] Opened file '1393'.
[preprocess][L2] Starting preprocessing for '1393'.
[preprocess][L3] 1393: Tokenized into 180 tokens.
[preprocess][L3] 1393: Removed stop words; 113 tokens remain.
[preprocess][L3] 1393: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1393'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1393'.
[indexDocument][L3] Recorded doc length 113 for '1393'.
[indexDocument][L3] Tokens added to the inverted index for '1393'.
[indexDocument][L2] Indexing document '73'.
[indexDocument][L3] Opened file '73'.
[preprocess][L2] Starting preprocessing for '73'.
[preprocess][L3] 73: Tokenized into 343 tokens.
[preprocess][L3] 73: Removed stop words; 190 tokens remain.
[preprocess][L3] 73: Stemming complete.
[indexDocument][L3] Preprocessing complete for '73'; 190 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '73'.
[indexDocument][L3] Recorded doc length 190 for '73'.
[indexDocument][L3] Tokens added to the inverted index for '73'.
[indexDocument][L2] Indexing document '1367'.
[indexDocument][L3] Opened file '1367'.
[preprocess][L2] Starting preprocessing for '1367'.
[preprocess][L3] 1367: Tokenized into 79 tokens.
[preprocess][L3] 1367: Removed stop words; 50 tokens remain.
[preprocess][L3] 1367: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1367'; 50 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1367'.
[indexDocument][L3] Recorded doc length 50 for '1367'.
[indexDocument][L3] Tokens added to the inverted index for '1367'.
[indexDocument][L2] Indexing document '87'.
[indexDocument][L3] Opened file '87'.
[preprocess][L2] Starting preprocessing for '87'.
[preprocess][L3] 87: Tokenized into 127 tokens.
[preprocess][L3] 87: Removed stop words; 78 tokens remain.
[preprocess][L3] 87: Stemming complete.
[indexDocument][L3] Preprocessing complete for '87'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '87'.
[indexDocument][L3] Recorded doc length 78 for '87'.
[indexDocument][L3] Tokens added to the inverted index for '87'.
[indexDocument][L2] Indexing document '1155'.
[indexDocument][L3] Opened file '1155'.
[preprocess][L2] Starting preprocessing for '1155'.
[preprocess][L3] 1155: Tokenized into 131 tokens.
[preprocess][L3] 1155: Removed stop words; 78 tokens remain.
[preprocess][L3] 1155: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1155'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1155'.
[indexDocument][L3] Recorded doc length 78 for '1155'.
[indexDocument][L3] Tokens added to the inverted index for '1155'.
[indexDocument][L2] Indexing document '883'.
[indexDocument][L3] Opened file '883'.
[preprocess][L2] Starting preprocessing for '883'.
[preprocess][L3] 883: Tokenized into 115 tokens.
[preprocess][L3] 883: Removed stop words; 68 tokens remain.
[preprocess][L3] 883: Stemming complete.
[indexDocument][L3] Preprocessing complete for '883'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '883'.
[indexDocument][L3] Recorded doc length 68 for '883'.
[indexDocument][L3] Tokens added to the inverted index for '883'.
[indexDocument][L2] Indexing document '279'.
[indexDocument][L3] Opened file '279'.
[preprocess][L2] Starting preprocessing for '279'.
[preprocess][L3] 279: Tokenized into 115 tokens.
[preprocess][L3] 279: Removed stop words; 72 tokens remain.
[preprocess][L3] 279: Stemming complete.
[indexDocument][L3] Preprocessing complete for '279'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '279'.
[indexDocument][L3] Recorded doc length 72 for '279'.
[indexDocument][L3] Tokens added to the inverted index for '279'.
[indexDocument][L2] Indexing document '1199'.
[indexDocument][L3] Opened file '1199'.
[preprocess][L2] Starting preprocessing for '1199'.
[preprocess][L3] 1199: Tokenized into 249 tokens.
[preprocess][L3] 1199: Removed stop words; 148 tokens remain.
[preprocess][L3] 1199: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1199'; 148 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1199'.
[indexDocument][L3] Recorded doc length 148 for '1199'.
[indexDocument][L3] Tokens added to the inverted index for '1199'.
[indexDocument][L2] Indexing document '877'.
[indexDocument][L3] Opened file '877'.
[preprocess][L2] Starting preprocessing for '877'.
[preprocess][L3] 877: Tokenized into 73 tokens.
[preprocess][L3] 877: Removed stop words; 44 tokens remain.
[preprocess][L3] 877: Stemming complete.
[indexDocument][L3] Preprocessing complete for '877'; 44 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '877'.
[indexDocument][L3] Recorded doc length 44 for '877'.
[indexDocument][L3] Tokens added to the inverted index for '877'.
[indexDocument][L2] Indexing document '80'.
[indexDocument][L3] Opened file '80'.
[preprocess][L2] Starting preprocessing for '80'.
[preprocess][L3] 80: Tokenized into 292 tokens.
[preprocess][L3] 80: Removed stop words; 176 tokens remain.
[preprocess][L3] 80: Stemming complete.
[indexDocument][L3] Preprocessing complete for '80'; 176 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '80'.
[indexDocument][L3] Recorded doc length 176 for '80'.
[indexDocument][L3] Tokens added to the inverted index for '80'.
[indexDocument][L2] Indexing document '1152'.
[indexDocument][L3] Opened file '1152'.
[preprocess][L2] Starting preprocessing for '1152'.
[preprocess][L3] 1152: Tokenized into 52 tokens.
[preprocess][L3] 1152: Removed stop words; 36 tokens remain.
[preprocess][L3] 1152: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1152'; 36 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1152'.
[indexDocument][L3] Recorded doc length 36 for '1152'.
[indexDocument][L3] Tokens added to the inverted index for '1152'.
[indexDocument][L2] Indexing document '1360'.
[indexDocument][L3] Opened file '1360'.
[preprocess][L2] Starting preprocessing for '1360'.
[preprocess][L3] 1360: Tokenized into 132 tokens.
[preprocess][L3] 1360: Removed stop words; 74 tokens remain.
[preprocess][L3] 1360: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1360'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1360'.
[indexDocument][L3] Recorded doc length 74 for '1360'.
[indexDocument][L3] Tokens added to the inverted index for '1360'.
[indexDocument][L2] Indexing document '74'.
[indexDocument][L3] Opened file '74'.
[preprocess][L2] Starting preprocessing for '74'.
[preprocess][L3] 74: Tokenized into 91 tokens.
[preprocess][L3] 74: Removed stop words; 59 tokens remain.
[preprocess][L3] 74: Stemming complete.
[indexDocument][L3] Preprocessing complete for '74'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '74'.
[indexDocument][L3] Recorded doc length 59 for '74'.
[indexDocument][L3] Tokens added to the inverted index for '74'.
[indexDocument][L2] Indexing document '848'.
[indexDocument][L3] Opened file '848'.
[preprocess][L2] Starting preprocessing for '848'.
[preprocess][L3] 848: Tokenized into 84 tokens.
[preprocess][L3] 848: Removed stop words; 52 tokens remain.
[preprocess][L3] 848: Stemming complete.
[indexDocument][L3] Preprocessing complete for '848'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '848'.
[indexDocument][L3] Recorded doc length 52 for '848'.
[indexDocument][L3] Tokens added to the inverted index for '848'.
[indexDocument][L2] Indexing document '246'.
[indexDocument][L3] Opened file '246'.
[preprocess][L2] Starting preprocessing for '246'.
[preprocess][L3] 246: Tokenized into 150 tokens.
[preprocess][L3] 246: Removed stop words; 85 tokens remain.
[preprocess][L3] 246: Stemming complete.
[indexDocument][L3] Preprocessing complete for '246'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '246'.
[indexDocument][L3] Recorded doc length 85 for '246'.
[indexDocument][L3] Tokens added to the inverted index for '246'.
[indexDocument][L2] Indexing document '1394'.
[indexDocument][L3] Opened file '1394'.
[preprocess][L2] Starting preprocessing for '1394'.
[preprocess][L3] 1394: Tokenized into 134 tokens.
[preprocess][L3] 1394: Removed stop words; 92 tokens remain.
[preprocess][L3] 1394: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1394'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1394'.
[indexDocument][L3] Recorded doc length 92 for '1394'.
[indexDocument][L3] Tokens added to the inverted index for '1394'.
[indexDocument][L2] Indexing document '622'.
[indexDocument][L3] Opened file '622'.
[preprocess][L2] Starting preprocessing for '622'.
[preprocess][L3] 622: Tokenized into 234 tokens.
[preprocess][L3] 622: Removed stop words; 140 tokens remain.
[preprocess][L3] 622: Stemming complete.
[indexDocument][L3] Preprocessing complete for '622'; 140 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '622'.
[indexDocument][L3] Recorded doc length 140 for '622'.
[indexDocument][L3] Tokens added to the inverted index for '622'.
[indexDocument][L2] Indexing document '410'.
[indexDocument][L3] Opened file '410'.
[preprocess][L2] Starting preprocessing for '410'.
[preprocess][L3] 410: Tokenized into 176 tokens.
[preprocess][L3] 410: Removed stop words; 99 tokens remain.
[preprocess][L3] 410: Stemming complete.
[indexDocument][L3] Preprocessing complete for '410'; 99 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '410'.
[indexDocument][L3] Recorded doc length 99 for '410'.
[indexDocument][L3] Tokens added to the inverted index for '410'.
[indexDocument][L2] Indexing document '1369'.
[indexDocument][L3] Opened file '1369'.
[preprocess][L2] Starting preprocessing for '1369'.
[preprocess][L3] 1369: Tokenized into 75 tokens.
[preprocess][L3] 1369: Removed stop words; 41 tokens remain.
[preprocess][L3] 1369: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1369'; 41 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1369'.
[indexDocument][L3] Recorded doc length 41 for '1369'.
[indexDocument][L3] Tokens added to the inverted index for '1369'.
[indexDocument][L2] Indexing document '89'.
[indexDocument][L3] Opened file '89'.
[preprocess][L2] Starting preprocessing for '89'.
[preprocess][L3] 89: Tokenized into 433 tokens.
[preprocess][L3] 89: Removed stop words; 242 tokens remain.
[preprocess][L3] 89: Stemming complete.
[indexDocument][L3] Preprocessing complete for '89'; 242 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '89'.
[indexDocument][L3] Recorded doc length 242 for '89'.
[indexDocument][L3] Tokens added to the inverted index for '89'.
[indexDocument][L2] Indexing document '841'.
[indexDocument][L3] Opened file '841'.
[preprocess][L2] Starting preprocessing for '841'.
[preprocess][L3] 841: Tokenized into 75 tokens.
[preprocess][L3] 841: Removed stop words; 49 tokens remain.
[preprocess][L3] 841: Stemming complete.
[indexDocument][L3] Preprocessing complete for '841'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '841'.
[indexDocument][L3] Recorded doc length 49 for '841'.
[indexDocument][L3] Tokens added to the inverted index for '841'.
[indexDocument][L2] Indexing document '419'.
[indexDocument][L3] Opened file '419'.
[preprocess][L2] Starting preprocessing for '419'.
[preprocess][L3] 419: Tokenized into 129 tokens.
[preprocess][L3] 419: Removed stop words; 78 tokens remain.
[preprocess][L3] 419: Stemming complete.
[indexDocument][L3] Preprocessing complete for '419'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '419'.
[indexDocument][L3] Recorded doc length 78 for '419'.
[indexDocument][L3] Tokens added to the inverted index for '419'.
[indexDocument][L2] Indexing document '1164'.
[indexDocument][L3] Opened file '1164'.
[preprocess][L2] Starting preprocessing for '1164'.
[preprocess][L3] 1164: Tokenized into 267 tokens.
[preprocess][L3] 1164: Removed stop words; 143 tokens remain.
[preprocess][L3] 1164: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1164'; 143 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1164'.
[indexDocument][L3] Recorded doc length 143 for '1164'.
[indexDocument][L3] Tokens added to the inverted index for '1164'.
[indexDocument][L2] Indexing document '1356'.
[indexDocument][L3] Opened file '1356'.
[preprocess][L2] Starting preprocessing for '1356'.
[preprocess][L3] 1356: Tokenized into 301 tokens.
[preprocess][L3] 1356: Removed stop words; 181 tokens remain.
[preprocess][L3] 1356: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1356'; 181 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1356'.
[indexDocument][L3] Recorded doc length 181 for '1356'.
[indexDocument][L3] Tokens added to the inverted index for '1356'.
[indexDocument][L2] Indexing document '284'.
[indexDocument][L3] Opened file '284'.
[preprocess][L2] Starting preprocessing for '284'.
[preprocess][L3] 284: Tokenized into 105 tokens.
[preprocess][L3] 284: Removed stop words; 64 tokens remain.
[preprocess][L3] 284: Stemming complete.
[indexDocument][L3] Preprocessing complete for '284'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '284'.
[indexDocument][L3] Recorded doc length 64 for '284'.
[indexDocument][L3] Tokens added to the inverted index for '284'.
[indexDocument][L2] Indexing document '614'.
[indexDocument][L3] Opened file '614'.
[preprocess][L2] Starting preprocessing for '614'.
[preprocess][L3] 614: Tokenized into 231 tokens.
[preprocess][L3] 614: Removed stop words; 133 tokens remain.
[preprocess][L3] 614: Stemming complete.
[indexDocument][L3] Preprocessing complete for '614'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '614'.
[indexDocument][L3] Recorded doc length 133 for '614'.
[indexDocument][L3] Tokens added to the inverted index for '614'.
[indexDocument][L2] Indexing document '426'.
[indexDocument][L3] Opened file '426'.
[preprocess][L2] Starting preprocessing for '426'.
[preprocess][L3] 426: Tokenized into 156 tokens.
[preprocess][L3] 426: Removed stop words; 96 tokens remain.
[preprocess][L3] 426: Stemming complete.
[indexDocument][L3] Preprocessing complete for '426'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '426'.
[indexDocument][L3] Recorded doc length 96 for '426'.
[indexDocument][L3] Tokens added to the inverted index for '426'.
[indexDocument][L2] Indexing document '1190'.
[indexDocument][L3] Opened file '1190'.
[preprocess][L2] Starting preprocessing for '1190'.
[preprocess][L3] 1190: Tokenized into 118 tokens.
[preprocess][L3] 1190: Removed stop words; 75 tokens remain.
[preprocess][L3] 1190: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1190'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1190'.
[indexDocument][L3] Recorded doc length 75 for '1190'.
[indexDocument][L3] Tokens added to the inverted index for '1190'.
[indexDocument][L2] Indexing document '42'.
[indexDocument][L3] Opened file '42'.
[preprocess][L2] Starting preprocessing for '42'.
[preprocess][L3] 42: Tokenized into 276 tokens.
[preprocess][L3] 42: Removed stop words; 169 tokens remain.
[preprocess][L3] 42: Stemming complete.
[indexDocument][L3] Preprocessing complete for '42'; 169 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '42'.
[indexDocument][L3] Recorded doc length 169 for '42'.
[indexDocument][L3] Tokens added to the inverted index for '42'.
[indexDocument][L2] Indexing document '270'.
[indexDocument][L3] Opened file '270'.
[preprocess][L2] Starting preprocessing for '270'.
[preprocess][L3] 270: Tokenized into 214 tokens.
[preprocess][L3] 270: Removed stop words; 139 tokens remain.
[preprocess][L3] 270: Stemming complete.
[indexDocument][L3] Preprocessing complete for '270'; 139 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '270'.
[indexDocument][L3] Recorded doc length 139 for '270'.
[indexDocument][L3] Tokens added to the inverted index for '270'.
[indexDocument][L2] Indexing document '846'.
[indexDocument][L3] Opened file '846'.
[preprocess][L2] Starting preprocessing for '846'.
[preprocess][L3] 846: Tokenized into 226 tokens.
[preprocess][L3] 846: Removed stop words; 138 tokens remain.
[preprocess][L3] 846: Stemming complete.
[indexDocument][L3] Preprocessing complete for '846'; 138 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '846'.
[indexDocument][L3] Recorded doc length 138 for '846'.
[indexDocument][L3] Tokens added to the inverted index for '846'.
[indexDocument][L2] Indexing document '248'.
[indexDocument][L3] Opened file '248'.
[preprocess][L2] Starting preprocessing for '248'.
[preprocess][L3] 248: Tokenized into 126 tokens.
[preprocess][L3] 248: Removed stop words; 77 tokens remain.
[preprocess][L3] 248: Stemming complete.
[indexDocument][L3] Preprocessing complete for '248'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '248'.
[indexDocument][L3] Recorded doc length 77 for '248'.
[indexDocument][L3] Tokens added to the inverted index for '248'.
[indexDocument][L2] Indexing document '277'.
[indexDocument][L3] Opened file '277'.
[preprocess][L2] Starting preprocessing for '277'.
[preprocess][L3] 277: Tokenized into 266 tokens.
[preprocess][L3] 277: Removed stop words; 152 tokens remain.
[preprocess][L3] 277: Stemming complete.
[indexDocument][L3] Preprocessing complete for '277'; 152 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '277'.
[indexDocument][L3] Recorded doc length 152 for '277'.
[indexDocument][L3] Tokens added to the inverted index for '277'.
[indexDocument][L2] Indexing document '1197'.
[indexDocument][L3] Opened file '1197'.
[preprocess][L2] Starting preprocessing for '1197'.
[preprocess][L3] 1197: Tokenized into 153 tokens.
[preprocess][L3] 1197: Removed stop words; 97 tokens remain.
[preprocess][L3] 1197: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1197'; 97 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1197'.
[indexDocument][L3] Recorded doc length 97 for '1197'.
[indexDocument][L3] Tokens added to the inverted index for '1197'.
[indexDocument][L2] Indexing document '45'.
[indexDocument][L3] Opened file '45'.
[preprocess][L2] Starting preprocessing for '45'.
[preprocess][L3] 45: Tokenized into 165 tokens.
[preprocess][L3] 45: Removed stop words; 98 tokens remain.
[preprocess][L3] 45: Stemming complete.
[indexDocument][L3] Preprocessing complete for '45'; 98 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '45'.
[indexDocument][L3] Recorded doc length 98 for '45'.
[indexDocument][L3] Tokens added to the inverted index for '45'.
[indexDocument][L2] Indexing document '879'.
[indexDocument][L3] Opened file '879'.
[preprocess][L2] Starting preprocessing for '879'.
[preprocess][L3] 879: Tokenized into 47 tokens.
[preprocess][L3] 879: Removed stop words; 38 tokens remain.
[preprocess][L3] 879: Stemming complete.
[indexDocument][L3] Preprocessing complete for '879'; 38 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '879'.
[indexDocument][L3] Recorded doc length 38 for '879'.
[indexDocument][L3] Tokens added to the inverted index for '879'.
[indexDocument][L2] Indexing document '421'.
[indexDocument][L3] Opened file '421'.
[preprocess][L2] Starting preprocessing for '421'.
[preprocess][L3] 421: Tokenized into 164 tokens.
[preprocess][L3] 421: Removed stop words; 107 tokens remain.
[preprocess][L3] 421: Stemming complete.
[indexDocument][L3] Preprocessing complete for '421'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '421'.
[indexDocument][L3] Recorded doc length 107 for '421'.
[indexDocument][L3] Tokens added to the inverted index for '421'.
[indexDocument][L2] Indexing document '613'.
[indexDocument][L3] Opened file '613'.
[preprocess][L2] Starting preprocessing for '613'.
[preprocess][L3] 613: Tokenized into 149 tokens.
[preprocess][L3] 613: Removed stop words; 88 tokens remain.
[preprocess][L3] 613: Stemming complete.
[indexDocument][L3] Preprocessing complete for '613'; 88 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '613'.
[indexDocument][L3] Recorded doc length 88 for '613'.
[indexDocument][L3] Tokens added to the inverted index for '613'.
[indexDocument][L2] Indexing document '1351'.
[indexDocument][L3] Opened file '1351'.
[preprocess][L2] Starting preprocessing for '1351'.
[preprocess][L3] 1351: Tokenized into 300 tokens.
[preprocess][L3] 1351: Removed stop words; 187 tokens remain.
[preprocess][L3] 1351: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1351'; 187 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1351'.
[indexDocument][L3] Recorded doc length 187 for '1351'.
[indexDocument][L3] Tokens added to the inverted index for '1351'.
[indexDocument][L2] Indexing document '283'.
[indexDocument][L3] Opened file '283'.
[preprocess][L2] Starting preprocessing for '283'.
[preprocess][L3] 283: Tokenized into 190 tokens.
[preprocess][L3] 283: Removed stop words; 119 tokens remain.
[preprocess][L3] 283: Stemming complete.
[indexDocument][L3] Preprocessing complete for '283'; 119 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '283'.
[indexDocument][L3] Recorded doc length 119 for '283'.
[indexDocument][L3] Tokens added to the inverted index for '283'.
[indexDocument][L2] Indexing document '1163'.
[indexDocument][L3] Opened file '1163'.
[preprocess][L2] Starting preprocessing for '1163'.
[preprocess][L3] 1163: Tokenized into 179 tokens.
[preprocess][L3] 1163: Removed stop words; 107 tokens remain.
[preprocess][L3] 1163: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1163'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1163'.
[indexDocument][L3] Recorded doc length 107 for '1163'.
[indexDocument][L3] Tokens added to the inverted index for '1163'.
[indexDocument][L2] Indexing document '631'.
[indexDocument][L3] Opened file '631'.
[preprocess][L2] Starting preprocessing for '631'.
[preprocess][L3] 631: Tokenized into 77 tokens.
[preprocess][L3] 631: Removed stop words; 53 tokens remain.
[preprocess][L3] 631: Stemming complete.
[indexDocument][L3] Preprocessing complete for '631'; 53 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '631'.
[indexDocument][L3] Recorded doc length 53 for '631'.
[indexDocument][L3] Tokens added to the inverted index for '631'.
[indexDocument][L2] Indexing document '403'.
[indexDocument][L3] Opened file '403'.
[preprocess][L2] Starting preprocessing for '403'.
[preprocess][L3] 403: Tokenized into 116 tokens.
[preprocess][L3] 403: Removed stop words; 71 tokens remain.
[preprocess][L3] 403: Stemming complete.
[indexDocument][L3] Preprocessing complete for '403'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '403'.
[indexDocument][L3] Recorded doc length 71 for '403'.
[indexDocument][L3] Tokens added to the inverted index for '403'.
[indexDocument][L2] Indexing document '67'.
[indexDocument][L3] Opened file '67'.
[preprocess][L2] Starting preprocessing for '67'.
[preprocess][L3] 67: Tokenized into 94 tokens.
[preprocess][L3] 67: Removed stop words; 51 tokens remain.
[preprocess][L3] 67: Stemming complete.
[indexDocument][L3] Preprocessing complete for '67'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '67'.
[indexDocument][L3] Recorded doc length 51 for '67'.
[indexDocument][L3] Tokens added to the inverted index for '67'.
[indexDocument][L2] Indexing document '255'.
[indexDocument][L3] Opened file '255'.
[preprocess][L2] Starting preprocessing for '255'.
[preprocess][L3] 255: Tokenized into 211 tokens.
[preprocess][L3] 255: Removed stop words; 121 tokens remain.
[preprocess][L3] 255: Stemming complete.
[indexDocument][L3] Preprocessing complete for '255'; 121 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '255'.
[indexDocument][L3] Recorded doc length 121 for '255'.
[indexDocument][L3] Tokens added to the inverted index for '255'.
[indexDocument][L2] Indexing document '1387'.
[indexDocument][L3] Opened file '1387'.
[preprocess][L2] Starting preprocessing for '1387'.
[preprocess][L3] 1387: Tokenized into 239 tokens.
[preprocess][L3] 1387: Removed stop words; 130 tokens remain.
[preprocess][L3] 1387: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1387'; 130 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1387'.
[indexDocument][L3] Recorded doc length 130 for '1387'.
[indexDocument][L3] Tokens added to the inverted index for '1387'.
[indexDocument][L2] Indexing document '93'.
[indexDocument][L3] Opened file '93'.
[preprocess][L2] Starting preprocessing for '93'.
[preprocess][L3] 93: Tokenized into 96 tokens.
[preprocess][L3] 93: Removed stop words; 64 tokens remain.
[preprocess][L3] 93: Stemming complete.
[indexDocument][L3] Preprocessing complete for '93'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '93'.
[indexDocument][L3] Recorded doc length 64 for '93'.
[indexDocument][L3] Tokens added to the inverted index for '93'.
[indexDocument][L2] Indexing document '1141'.
[indexDocument][L3] Opened file '1141'.
[preprocess][L2] Starting preprocessing for '1141'.
[preprocess][L3] 1141: Tokenized into 74 tokens.
[preprocess][L3] 1141: Removed stop words; 46 tokens remain.
[preprocess][L3] 1141: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1141'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1141'.
[indexDocument][L3] Recorded doc length 46 for '1141'.
[indexDocument][L3] Tokens added to the inverted index for '1141'.
[indexDocument][L2] Indexing document '1373'.
[indexDocument][L3] Opened file '1373'.
[preprocess][L2] Starting preprocessing for '1373'.
[preprocess][L3] 1373: Tokenized into 289 tokens.
[preprocess][L3] 1373: Removed stop words; 172 tokens remain.
[preprocess][L3] 1373: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1373'; 172 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1373'.
[indexDocument][L3] Recorded doc length 172 for '1373'.
[indexDocument][L3] Tokens added to the inverted index for '1373'.
[indexDocument][L2] Indexing document '58'.
[indexDocument][L3] Opened file '58'.
[preprocess][L2] Starting preprocessing for '58'.
[preprocess][L3] 58: Tokenized into 182 tokens.
[preprocess][L3] 58: Removed stop words; 114 tokens remain.
[preprocess][L3] 58: Stemming complete.
[indexDocument][L3] Preprocessing complete for '58'; 114 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '58'.
[indexDocument][L3] Recorded doc length 114 for '58'.
[indexDocument][L3] Tokens added to the inverted index for '58'.
[indexDocument][L2] Indexing document '864'.
[indexDocument][L3] Opened file '864'.
[preprocess][L2] Starting preprocessing for '864'.
[preprocess][L3] 864: Tokenized into 49 tokens.
[preprocess][L3] 864: Removed stop words; 34 tokens remain.
[preprocess][L3] 864: Stemming complete.
[indexDocument][L3] Preprocessing complete for '864'; 34 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '864'.
[indexDocument][L3] Recorded doc length 34 for '864'.
[indexDocument][L3] Tokens added to the inverted index for '864'.
[indexDocument][L2] Indexing document '890'.
[indexDocument][L3] Opened file '890'.
[preprocess][L2] Starting preprocessing for '890'.
[preprocess][L3] 890: Tokenized into 151 tokens.
[preprocess][L3] 890: Removed stop words; 82 tokens remain.
[preprocess][L3] 890: Stemming complete.
[indexDocument][L3] Preprocessing complete for '890'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '890'.
[indexDocument][L3] Recorded doc length 82 for '890'.
[indexDocument][L3] Tokens added to the inverted index for '890'.
[indexDocument][L2] Indexing document '1374'.
[indexDocument][L3] Opened file '1374'.
[preprocess][L2] Starting preprocessing for '1374'.
[preprocess][L3] 1374: Tokenized into 137 tokens.
[preprocess][L3] 1374: Removed stop words; 86 tokens remain.
[preprocess][L3] 1374: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1374'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1374'.
[indexDocument][L3] Recorded doc length 86 for '1374'.
[indexDocument][L3] Tokens added to the inverted index for '1374'.
[indexDocument][L2] Indexing document '94'.
[indexDocument][L3] Opened file '94'.
[preprocess][L2] Starting preprocessing for '94'.
[preprocess][L3] 94: Tokenized into 470 tokens.
[preprocess][L3] 94: Removed stop words; 252 tokens remain.
[preprocess][L3] 94: Stemming complete.
[indexDocument][L3] Preprocessing complete for '94'; 252 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '94'.
[indexDocument][L3] Recorded doc length 252 for '94'.
[indexDocument][L3] Tokens added to the inverted index for '94'.
[indexDocument][L2] Indexing document '1146'.
[indexDocument][L3] Opened file '1146'.
[preprocess][L2] Starting preprocessing for '1146'.
[preprocess][L3] 1146: Tokenized into 48 tokens.
[preprocess][L3] 1146: Removed stop words; 33 tokens remain.
[preprocess][L3] 1146: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1146'; 33 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1146'.
[indexDocument][L3] Recorded doc length 33 for '1146'.
[indexDocument][L3] Tokens added to the inverted index for '1146'.
[indexDocument][L2] Indexing document '252'.
[indexDocument][L3] Opened file '252'.
[preprocess][L2] Starting preprocessing for '252'.
[preprocess][L3] 252: Tokenized into 259 tokens.
[preprocess][L3] 252: Removed stop words; 161 tokens remain.
[preprocess][L3] 252: Stemming complete.
[indexDocument][L3] Preprocessing complete for '252'; 161 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '252'.
[indexDocument][L3] Recorded doc length 161 for '252'.
[indexDocument][L3] Tokens added to the inverted index for '252'.
[indexDocument][L2] Indexing document '1380'.
[indexDocument][L3] Opened file '1380'.
[preprocess][L2] Starting preprocessing for '1380'.
[preprocess][L3] 1380: Tokenized into 307 tokens.
[preprocess][L3] 1380: Removed stop words; 175 tokens remain.
[preprocess][L3] 1380: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1380'; 175 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1380'.
[indexDocument][L3] Recorded doc length 175 for '1380'.
[indexDocument][L3] Tokens added to the inverted index for '1380'.
[indexDocument][L2] Indexing document '60'.
[indexDocument][L3] Opened file '60'.
[preprocess][L2] Starting preprocessing for '60'.
[preprocess][L3] 60: Tokenized into 144 tokens.
[preprocess][L3] 60: Removed stop words; 90 tokens remain.
[preprocess][L3] 60: Stemming complete.
[indexDocument][L3] Preprocessing complete for '60'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '60'.
[indexDocument][L3] Recorded doc length 90 for '60'.
[indexDocument][L3] Tokens added to the inverted index for '60'.
[indexDocument][L2] Indexing document '404'.
[indexDocument][L3] Opened file '404'.
[preprocess][L2] Starting preprocessing for '404'.
[preprocess][L3] 404: Tokenized into 170 tokens.
[preprocess][L3] 404: Removed stop words; 94 tokens remain.
[preprocess][L3] 404: Stemming complete.
[indexDocument][L3] Preprocessing complete for '404'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '404'.
[indexDocument][L3] Recorded doc length 94 for '404'.
[indexDocument][L3] Tokens added to the inverted index for '404'.
[indexDocument][L2] Indexing document '636'.
[indexDocument][L3] Opened file '636'.
[preprocess][L2] Starting preprocessing for '636'.
[preprocess][L3] 636: Tokenized into 236 tokens.
[preprocess][L3] 636: Removed stop words; 144 tokens remain.
[preprocess][L3] 636: Stemming complete.
[indexDocument][L3] Preprocessing complete for '636'; 144 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '636'.
[indexDocument][L3] Recorded doc length 144 for '636'.
[indexDocument][L3] Tokens added to the inverted index for '636'.
[indexDocument][L2] Indexing document '1179'.
[indexDocument][L3] Opened file '1179'.
[preprocess][L2] Starting preprocessing for '1179'.
[preprocess][L3] 1179: Tokenized into 219 tokens.
[preprocess][L3] 1179: Removed stop words; 137 tokens remain.
[preprocess][L3] 1179: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1179'; 137 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1179'.
[indexDocument][L3] Recorded doc length 137 for '1179'.
[indexDocument][L3] Tokens added to the inverted index for '1179'.
[indexDocument][L2] Indexing document '897'.
[indexDocument][L3] Opened file '897'.
[preprocess][L2] Starting preprocessing for '897'.
[preprocess][L3] 897: Tokenized into 112 tokens.
[preprocess][L3] 897: Removed stop words; 67 tokens remain.
[preprocess][L3] 897: Stemming complete.
[indexDocument][L3] Preprocessing complete for '897'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '897'.
[indexDocument][L3] Recorded doc length 67 for '897'.
[indexDocument][L3] Tokens added to the inverted index for '897'.
[indexDocument][L2] Indexing document '299'.
[indexDocument][L3] Opened file '299'.
[preprocess][L2] Starting preprocessing for '299'.
[preprocess][L3] 299: Tokenized into 141 tokens.
[preprocess][L3] 299: Removed stop words; 83 tokens remain.
[preprocess][L3] 299: Stemming complete.
[indexDocument][L3] Preprocessing complete for '299'; 83 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '299'.
[indexDocument][L3] Recorded doc length 83 for '299'.
[indexDocument][L3] Tokens added to the inverted index for '299'.
[indexDocument][L2] Indexing document '609'.
[indexDocument][L3] Opened file '609'.
[preprocess][L2] Starting preprocessing for '609'.
[preprocess][L3] 609: Tokenized into 73 tokens.
[preprocess][L3] 609: Removed stop words; 53 tokens remain.
[preprocess][L3] 609: Stemming complete.
[indexDocument][L3] Preprocessing complete for '609'; 53 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '609'.
[indexDocument][L3] Recorded doc length 53 for '609'.
[indexDocument][L3] Tokens added to the inverted index for '609'.
[indexDocument][L2] Indexing document '863'.
[indexDocument][L3] Opened file '863'.
[preprocess][L2] Starting preprocessing for '863'.
[preprocess][L3] 863: Tokenized into 104 tokens.
[preprocess][L3] 863: Removed stop words; 65 tokens remain.
[preprocess][L3] 863: Stemming complete.
[indexDocument][L3] Preprocessing complete for '863'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '863'.
[indexDocument][L3] Recorded doc length 65 for '863'.
[indexDocument][L3] Tokens added to the inverted index for '863'.
[indexDocument][L2] Indexing document '1342'.
[indexDocument][L3] Opened file '1342'.
[preprocess][L2] Starting preprocessing for '1342'.
[preprocess][L3] 1342: Tokenized into 292 tokens.
[preprocess][L3] 1342: Removed stop words; 155 tokens remain.
[preprocess][L3] 1342: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1342'; 155 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1342'.
[indexDocument][L3] Recorded doc length 155 for '1342'.
[indexDocument][L3] Tokens added to the inverted index for '1342'.
[indexDocument][L2] Indexing document '290'.
[indexDocument][L3] Opened file '290'.
[preprocess][L2] Starting preprocessing for '290'.
[preprocess][L3] 290: Tokenized into 128 tokens.
[preprocess][L3] 290: Removed stop words; 79 tokens remain.
[preprocess][L3] 290: Stemming complete.
[indexDocument][L3] Preprocessing complete for '290'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '290'.
[indexDocument][L3] Recorded doc length 79 for '290'.
[indexDocument][L3] Tokens added to the inverted index for '290'.
[indexDocument][L2] Indexing document '1170'.
[indexDocument][L3] Opened file '1170'.
[preprocess][L2] Starting preprocessing for '1170'.
[preprocess][L3] 1170: Tokenized into 109 tokens.
[preprocess][L3] 1170: Removed stop words; 78 tokens remain.
[preprocess][L3] 1170: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1170'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1170'.
[indexDocument][L3] Recorded doc length 78 for '1170'.
[indexDocument][L3] Tokens added to the inverted index for '1170'.
[indexDocument][L2] Indexing document '432'.
[indexDocument][L3] Opened file '432'.
[preprocess][L2] Starting preprocessing for '432'.
[preprocess][L3] 432: Tokenized into 207 tokens.
[preprocess][L3] 432: Removed stop words; 130 tokens remain.
[preprocess][L3] 432: Stemming complete.
[indexDocument][L3] Preprocessing complete for '432'; 130 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '432'.
[indexDocument][L3] Recorded doc length 130 for '432'.
[indexDocument][L3] Tokens added to the inverted index for '432'.
[indexDocument][L2] Indexing document '600'.
[indexDocument][L3] Opened file '600'.
[preprocess][L2] Starting preprocessing for '600'.
[preprocess][L3] 600: Tokenized into 186 tokens.
[preprocess][L3] 600: Removed stop words; 105 tokens remain.
[preprocess][L3] 600: Stemming complete.
[indexDocument][L3] Preprocessing complete for '600'; 105 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '600'.
[indexDocument][L3] Recorded doc length 105 for '600'.
[indexDocument][L3] Tokens added to the inverted index for '600'.
[indexDocument][L2] Indexing document '264'.
[indexDocument][L3] Opened file '264'.
[preprocess][L2] Starting preprocessing for '264'.
[preprocess][L3] 264: Tokenized into 63 tokens.
[preprocess][L3] 264: Removed stop words; 43 tokens remain.
[preprocess][L3] 264: Stemming complete.
[indexDocument][L3] Preprocessing complete for '264'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '264'.
[indexDocument][L3] Recorded doc length 43 for '264'.
[indexDocument][L3] Tokens added to the inverted index for '264'.
[indexDocument][L2] Indexing document '1184'.
[indexDocument][L3] Opened file '1184'.
[preprocess][L2] Starting preprocessing for '1184'.
[preprocess][L3] 1184: Tokenized into 206 tokens.
[preprocess][L3] 1184: Removed stop words; 132 tokens remain.
[preprocess][L3] 1184: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1184'; 132 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1184'.
[indexDocument][L3] Recorded doc length 132 for '1184'.
[indexDocument][L3] Tokens added to the inverted index for '1184'.
[indexDocument][L2] Indexing document '56'.
[indexDocument][L3] Opened file '56'.
[preprocess][L2] Starting preprocessing for '56'.
[preprocess][L3] 56: Tokenized into 222 tokens.
[preprocess][L3] 56: Removed stop words; 131 tokens remain.
[preprocess][L3] 56: Stemming complete.
[indexDocument][L3] Preprocessing complete for '56'; 131 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '56'.
[indexDocument][L3] Recorded doc length 131 for '56'.
[indexDocument][L3] Tokens added to the inverted index for '56'.
[indexDocument][L2] Indexing document '69'.
[indexDocument][L3] Opened file '69'.
[preprocess][L2] Starting preprocessing for '69'.
[preprocess][L3] 69: Tokenized into 136 tokens.
[preprocess][L3] 69: Removed stop words; 86 tokens remain.
[preprocess][L3] 69: Stemming complete.
[indexDocument][L3] Preprocessing complete for '69'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '69'.
[indexDocument][L3] Recorded doc length 86 for '69'.
[indexDocument][L3] Tokens added to the inverted index for '69'.
[indexDocument][L2] Indexing document '855'.
[indexDocument][L3] Opened file '855'.
[preprocess][L2] Starting preprocessing for '855'.
[preprocess][L3] 855: Tokenized into 72 tokens.
[preprocess][L3] 855: Removed stop words; 48 tokens remain.
[preprocess][L3] 855: Stemming complete.
[indexDocument][L3] Preprocessing complete for '855'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '855'.
[indexDocument][L3] Recorded doc length 48 for '855'.
[indexDocument][L3] Tokens added to the inverted index for '855'.
[indexDocument][L2] Indexing document '1389'.
[indexDocument][L3] Opened file '1389'.
[preprocess][L2] Starting preprocessing for '1389'.
[preprocess][L3] 1389: Tokenized into 138 tokens.
[preprocess][L3] 1389: Removed stop words; 88 tokens remain.
[preprocess][L3] 1389: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1389'; 88 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1389'.
[indexDocument][L3] Recorded doc length 88 for '1389'.
[indexDocument][L3] Tokens added to the inverted index for '1389'.
[indexDocument][L2] Indexing document '1183'.
[indexDocument][L3] Opened file '1183'.
[preprocess][L2] Starting preprocessing for '1183'.
[preprocess][L3] 1183: Tokenized into 140 tokens.
[preprocess][L3] 1183: Removed stop words; 74 tokens remain.
[preprocess][L3] 1183: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1183'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1183'.
[indexDocument][L3] Recorded doc length 74 for '1183'.
[indexDocument][L3] Tokens added to the inverted index for '1183'.
[indexDocument][L2] Indexing document '51'.
[indexDocument][L3] Opened file '51'.
[preprocess][L2] Starting preprocessing for '51'.
[preprocess][L3] 51: Tokenized into 209 tokens.
[preprocess][L3] 51: Removed stop words; 116 tokens remain.
[preprocess][L3] 51: Stemming complete.
[indexDocument][L3] Preprocessing complete for '51'; 116 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '51'.
[indexDocument][L3] Recorded doc length 116 for '51'.
[indexDocument][L3] Tokens added to the inverted index for '51'.
[indexDocument][L2] Indexing document '263'.
[indexDocument][L3] Opened file '263'.
[preprocess][L2] Starting preprocessing for '263'.
[preprocess][L3] 263: Tokenized into 157 tokens.
[preprocess][L3] 263: Removed stop words; 92 tokens remain.
[preprocess][L3] 263: Stemming complete.
[indexDocument][L3] Preprocessing complete for '263'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '263'.
[indexDocument][L3] Recorded doc length 92 for '263'.
[indexDocument][L3] Tokens added to the inverted index for '263'.
[indexDocument][L2] Indexing document '607'.
[indexDocument][L3] Opened file '607'.
[preprocess][L2] Starting preprocessing for '607'.
[preprocess][L3] 607: Tokenized into 55 tokens.
[preprocess][L3] 607: Removed stop words; 36 tokens remain.
[preprocess][L3] 607: Stemming complete.
[indexDocument][L3] Preprocessing complete for '607'; 36 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '607'.
[indexDocument][L3] Recorded doc length 36 for '607'.
[indexDocument][L3] Tokens added to the inverted index for '607'.
[indexDocument][L2] Indexing document '435'.
[indexDocument][L3] Opened file '435'.
[preprocess][L2] Starting preprocessing for '435'.
[preprocess][L3] 435: Tokenized into 193 tokens.
[preprocess][L3] 435: Removed stop words; 119 tokens remain.
[preprocess][L3] 435: Stemming complete.
[indexDocument][L3] Preprocessing complete for '435'; 119 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '435'.
[indexDocument][L3] Recorded doc length 119 for '435'.
[indexDocument][L3] Tokens added to the inverted index for '435'.
[indexDocument][L2] Indexing document '1177'.
[indexDocument][L3] Opened file '1177'.
[preprocess][L2] Starting preprocessing for '1177'.
[preprocess][L3] 1177: Tokenized into 113 tokens.
[preprocess][L3] 1177: Removed stop words; 67 tokens remain.
[preprocess][L3] 1177: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1177'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1177'.
[indexDocument][L3] Recorded doc length 67 for '1177'.
[indexDocument][L3] Tokens added to the inverted index for '1177'.
[indexDocument][L2] Indexing document '899'.
[indexDocument][L3] Opened file '899'.
[preprocess][L2] Starting preprocessing for '899'.
[preprocess][L3] 899: Tokenized into 169 tokens.
[preprocess][L3] 899: Removed stop words; 108 tokens remain.
[preprocess][L3] 899: Stemming complete.
[indexDocument][L3] Preprocessing complete for '899'; 108 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '899'.
[indexDocument][L3] Recorded doc length 108 for '899'.
[indexDocument][L3] Tokens added to the inverted index for '899'.
[indexDocument][L2] Indexing document '1345'.
[indexDocument][L3] Opened file '1345'.
[preprocess][L2] Starting preprocessing for '1345'.
[preprocess][L3] 1345: Tokenized into 133 tokens.
[preprocess][L3] 1345: Removed stop words; 80 tokens remain.
[preprocess][L3] 1345: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1345'; 80 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1345'.
[indexDocument][L3] Recorded doc length 80 for '1345'.
[indexDocument][L3] Tokens added to the inverted index for '1345'.
[indexDocument][L2] Indexing document '297'.
[indexDocument][L3] Opened file '297'.
[preprocess][L2] Starting preprocessing for '297'.
[preprocess][L3] 297: Tokenized into 154 tokens.
[preprocess][L3] 297: Removed stop words; 92 tokens remain.
[preprocess][L3] 297: Stemming complete.
[indexDocument][L3] Preprocessing complete for '297'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '297'.
[indexDocument][L3] Recorded doc length 92 for '297'.
[indexDocument][L3] Tokens added to the inverted index for '297'.
[indexDocument][L2] Indexing document '638'.
[indexDocument][L3] Opened file '638'.
[preprocess][L2] Starting preprocessing for '638'.
[preprocess][L3] 638: Tokenized into 224 tokens.
[preprocess][L3] 638: Removed stop words; 142 tokens remain.
[preprocess][L3] 638: Stemming complete.
[indexDocument][L3] Preprocessing complete for '638'; 142 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '638'.
[indexDocument][L3] Recorded doc length 142 for '638'.
[indexDocument][L3] Tokens added to the inverted index for '638'.
[indexDocument][L2] Indexing document '852'.
[indexDocument][L3] Opened file '852'.
[preprocess][L2] Starting preprocessing for '852'.
[preprocess][L3] 852: Tokenized into 162 tokens.
[preprocess][L3] 852: Removed stop words; 101 tokens remain.
[preprocess][L3] 852: Stemming complete.
[indexDocument][L3] Preprocessing complete for '852'; 101 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '852'.
[indexDocument][L3] Recorded doc length 101 for '852'.
[indexDocument][L3] Tokens added to the inverted index for '852'.
[indexDocument][L2] Indexing document '1148'.
[indexDocument][L3] Opened file '1148'.
[preprocess][L2] Starting preprocessing for '1148'.
[preprocess][L3] 1148: Tokenized into 99 tokens.
[preprocess][L3] 1148: Removed stop words; 63 tokens remain.
[preprocess][L3] 1148: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1148'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1148'.
[indexDocument][L3] Recorded doc length 63 for '1148'.
[indexDocument][L3] Tokens added to the inverted index for '1148'.
[indexDocument][L2] Indexing document '806'.
[indexDocument][L3] Opened file '806'.
[preprocess][L2] Starting preprocessing for '806'.
[preprocess][L3] 806: Tokenized into 277 tokens.
[preprocess][L3] 806: Removed stop words; 170 tokens remain.
[preprocess][L3] 806: Stemming complete.
[indexDocument][L3] Preprocessing complete for '806'; 170 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '806'.
[indexDocument][L3] Recorded doc length 170 for '806'.
[indexDocument][L3] Tokens added to the inverted index for '806'.
[indexDocument][L2] Indexing document '208'.
[indexDocument][L3] Opened file '208'.
[preprocess][L2] Starting preprocessing for '208'.
[preprocess][L3] 208: Tokenized into 127 tokens.
[preprocess][L3] 208: Removed stop words; 79 tokens remain.
[preprocess][L3] 208: Stemming complete.
[indexDocument][L3] Preprocessing complete for '208'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '208'.
[indexDocument][L3] Recorded doc length 79 for '208'.
[indexDocument][L3] Tokens added to the inverted index for '208'.
[indexDocument][L2] Indexing document '698'.
[indexDocument][L3] Opened file '698'.
[preprocess][L2] Starting preprocessing for '698'.
[preprocess][L3] 698: Tokenized into 161 tokens.
[preprocess][L3] 698: Removed stop words; 89 tokens remain.
[preprocess][L3] 698: Stemming complete.
[indexDocument][L3] Preprocessing complete for '698'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '698'.
[indexDocument][L3] Recorded doc length 89 for '698'.
[indexDocument][L3] Tokens added to the inverted index for '698'.
[indexDocument][L2] Indexing document '237'.
[indexDocument][L3] Opened file '237'.
[preprocess][L2] Starting preprocessing for '237'.
[preprocess][L3] 237: Tokenized into 155 tokens.
[preprocess][L3] 237: Removed stop words; 90 tokens remain.
[preprocess][L3] 237: Stemming complete.
[indexDocument][L3] Preprocessing complete for '237'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '237'.
[indexDocument][L3] Recorded doc length 90 for '237'.
[indexDocument][L3] Tokens added to the inverted index for '237'.
[indexDocument][L2] Indexing document '839'.
[indexDocument][L3] Opened file '839'.
[preprocess][L2] Starting preprocessing for '839'.
[preprocess][L3] 839: Tokenized into 172 tokens.
[preprocess][L3] 839: Removed stop words; 107 tokens remain.
[preprocess][L3] 839: Stemming complete.
[indexDocument][L3] Preprocessing complete for '839'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '839'.
[indexDocument][L3] Recorded doc length 107 for '839'.
[indexDocument][L3] Tokens added to the inverted index for '839'.
[indexDocument][L2] Indexing document '5'.
[indexDocument][L3] Opened file '5'.
[preprocess][L2] Starting preprocessing for '5'.
[preprocess][L3] 5: Tokenized into 61 tokens.
[preprocess][L3] 5: Removed stop words; 44 tokens remain.
[preprocess][L3] 5: Stemming complete.
[indexDocument][L3] Preprocessing complete for '5'; 44 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '5'.
[indexDocument][L3] Recorded doc length 44 for '5'.
[indexDocument][L3] Tokens added to the inverted index for '5'.
[indexDocument][L2] Indexing document '461'.
[indexDocument][L3] Opened file '461'.
[preprocess][L2] Starting preprocessing for '461'.
[preprocess][L3] 461: Tokenized into 110 tokens.
[preprocess][L3] 461: Removed stop words; 69 tokens remain.
[preprocess][L3] 461: Stemming complete.
[indexDocument][L3] Preprocessing complete for '461'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '461'.
[indexDocument][L3] Recorded doc length 69 for '461'.
[indexDocument][L3] Tokens added to the inverted index for '461'.
[indexDocument][L2] Indexing document '653'.
[indexDocument][L3] Opened file '653'.
[preprocess][L2] Starting preprocessing for '653'.
[preprocess][L3] 653: Tokenized into 150 tokens.
[preprocess][L3] 653: Removed stop words; 89 tokens remain.
[preprocess][L3] 653: Stemming complete.
[indexDocument][L3] Preprocessing complete for '653'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '653'.
[indexDocument][L3] Recorded doc length 89 for '653'.
[indexDocument][L3] Tokens added to the inverted index for '653'.
[indexDocument][L2] Indexing document '495'.
[indexDocument][L3] Opened file '495'.
[preprocess][L2] Starting preprocessing for '495'.
[preprocess][L3] 495: Tokenized into 108 tokens.
[preprocess][L3] 495: Removed stop words; 68 tokens remain.
[preprocess][L3] 495: Stemming complete.
[indexDocument][L3] Preprocessing complete for '495'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '495'.
[indexDocument][L3] Recorded doc length 68 for '495'.
[indexDocument][L3] Tokens added to the inverted index for '495'.
[indexDocument][L2] Indexing document '1311'.
[indexDocument][L3] Opened file '1311'.
[preprocess][L2] Starting preprocessing for '1311'.
[preprocess][L3] 1311: Tokenized into 73 tokens.
[preprocess][L3] 1311: Removed stop words; 51 tokens remain.
[preprocess][L3] 1311: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1311'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1311'.
[indexDocument][L3] Recorded doc length 51 for '1311'.
[indexDocument][L3] Tokens added to the inverted index for '1311'.
[indexDocument][L2] Indexing document '1123'.
[indexDocument][L3] Opened file '1123'.
[preprocess][L2] Starting preprocessing for '1123'.
[preprocess][L3] 1123: Tokenized into 110 tokens.
[preprocess][L3] 1123: Removed stop words; 69 tokens remain.
[preprocess][L3] 1123: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1123'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1123'.
[indexDocument][L3] Recorded doc length 69 for '1123'.
[indexDocument][L3] Tokens added to the inverted index for '1123'.
[indexDocument][L2] Indexing document '1329'.
[indexDocument][L3] Opened file '1329'.
[preprocess][L2] Starting preprocessing for '1329'.
[preprocess][L3] 1329: Tokenized into 176 tokens.
[preprocess][L3] 1329: Removed stop words; 94 tokens remain.
[preprocess][L3] 1329: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1329'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1329'.
[indexDocument][L3] Recorded doc length 94 for '1329'.
[indexDocument][L3] Tokens added to the inverted index for '1329'.
[indexDocument][L2] Indexing document '801'.
[indexDocument][L3] Opened file '801'.
[preprocess][L2] Starting preprocessing for '801'.
[preprocess][L3] 801: Tokenized into 209 tokens.
[preprocess][L3] 801: Removed stop words; 120 tokens remain.
[preprocess][L3] 801: Stemming complete.
[indexDocument][L3] Preprocessing complete for '801'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '801'.
[indexDocument][L3] Recorded doc length 120 for '801'.
[indexDocument][L3] Tokens added to the inverted index for '801'.
[indexDocument][L2] Indexing document '459'.
[indexDocument][L3] Opened file '459'.
[preprocess][L2] Starting preprocessing for '459'.
[preprocess][L3] 459: Tokenized into 278 tokens.
[preprocess][L3] 459: Removed stop words; 158 tokens remain.
[preprocess][L3] 459: Stemming complete.
[indexDocument][L3] Preprocessing complete for '459'; 158 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '459'.
[indexDocument][L3] Recorded doc length 158 for '459'.
[indexDocument][L3] Tokens added to the inverted index for '459'.
[indexDocument][L2] Indexing document '1124'.
[indexDocument][L3] Opened file '1124'.
[preprocess][L2] Starting preprocessing for '1124'.
[preprocess][L3] 1124: Tokenized into 91 tokens.
[preprocess][L3] 1124: Removed stop words; 56 tokens remain.
[preprocess][L3] 1124: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1124'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1124'.
[indexDocument][L3] Recorded doc length 56 for '1124'.
[indexDocument][L3] Tokens added to the inverted index for '1124'.
[indexDocument][L2] Indexing document '1316'.
[indexDocument][L3] Opened file '1316'.
[preprocess][L2] Starting preprocessing for '1316'.
[preprocess][L3] 1316: Tokenized into 237 tokens.
[preprocess][L3] 1316: Removed stop words; 141 tokens remain.
[preprocess][L3] 1316: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1316'; 141 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1316'.
[indexDocument][L3] Recorded doc length 141 for '1316'.
[indexDocument][L3] Tokens added to the inverted index for '1316'.
[indexDocument][L2] Indexing document '492'.
[indexDocument][L3] Opened file '492'.
[preprocess][L2] Starting preprocessing for '492'.
[preprocess][L3] 492: Tokenized into 68 tokens.
[preprocess][L3] 492: Removed stop words; 44 tokens remain.
[preprocess][L3] 492: Stemming complete.
[indexDocument][L3] Preprocessing complete for '492'; 44 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '492'.
[indexDocument][L3] Recorded doc length 44 for '492'.
[indexDocument][L3] Tokens added to the inverted index for '492'.
[indexDocument][L2] Indexing document '654'.
[indexDocument][L3] Opened file '654'.
[preprocess][L2] Starting preprocessing for '654'.
[preprocess][L3] 654: Tokenized into 116 tokens.
[preprocess][L3] 654: Removed stop words; 71 tokens remain.
[preprocess][L3] 654: Stemming complete.
[indexDocument][L3] Preprocessing complete for '654'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '654'.
[indexDocument][L3] Recorded doc length 71 for '654'.
[indexDocument][L3] Tokens added to the inverted index for '654'.
[indexDocument][L2] Indexing document '466'.
[indexDocument][L3] Opened file '466'.
[preprocess][L2] Starting preprocessing for '466'.
[preprocess][L3] 466: Tokenized into 237 tokens.
[preprocess][L3] 466: Removed stop words; 134 tokens remain.
[preprocess][L3] 466: Stemming complete.
[indexDocument][L3] Preprocessing complete for '466'; 134 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '466'.
[indexDocument][L3] Recorded doc length 134 for '466'.
[indexDocument][L3] Tokens added to the inverted index for '466'.
[indexDocument][L2] Indexing document '2'.
[indexDocument][L3] Opened file '2'.
[preprocess][L2] Starting preprocessing for '2'.
[preprocess][L3] 2: Tokenized into 202 tokens.
[preprocess][L3] 2: Removed stop words; 124 tokens remain.
[preprocess][L3] 2: Stemming complete.
[indexDocument][L3] Preprocessing complete for '2'; 124 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '2'.
[indexDocument][L3] Recorded doc length 124 for '2'.
[indexDocument][L3] Tokens added to the inverted index for '2'.
[indexDocument][L2] Indexing document '230'.
[indexDocument][L3] Opened file '230'.
[preprocess][L2] Starting preprocessing for '230'.
[preprocess][L3] 230: Tokenized into 225 tokens.
[preprocess][L3] 230: Removed stop words; 129 tokens remain.
[preprocess][L3] 230: Stemming complete.
[indexDocument][L3] Preprocessing complete for '230'; 129 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '230'.
[indexDocument][L3] Recorded doc length 129 for '230'.
[indexDocument][L3] Tokens added to the inverted index for '230'.
[indexDocument][L2] Indexing document '239'.
[indexDocument][L3] Opened file '239'.
[preprocess][L2] Starting preprocessing for '239'.
[preprocess][L3] 239: Tokenized into 183 tokens.
[preprocess][L3] 239: Removed stop words; 102 tokens remain.
[preprocess][L3] 239: Stemming complete.
[indexDocument][L3] Preprocessing complete for '239'; 102 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '239'.
[indexDocument][L3] Recorded doc length 102 for '239'.
[indexDocument][L3] Tokens added to the inverted index for '239'.
[indexDocument][L2] Indexing document '837'.
[indexDocument][L3] Opened file '837'.
[preprocess][L2] Starting preprocessing for '837'.
[preprocess][L3] 837: Tokenized into 219 tokens.
[preprocess][L3] 837: Removed stop words; 146 tokens remain.
[preprocess][L3] 837: Stemming complete.
[indexDocument][L3] Preprocessing complete for '837'; 146 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '837'.
[indexDocument][L3] Recorded doc length 146 for '837'.
[indexDocument][L3] Tokens added to the inverted index for '837'.
[indexDocument][L2] Indexing document '696'.
[indexDocument][L3] Opened file '696'.
[preprocess][L2] Starting preprocessing for '696'.
[preprocess][L3] 696: Tokenized into 292 tokens.
[preprocess][L3] 696: Removed stop words; 186 tokens remain.
[preprocess][L3] 696: Stemming complete.
[indexDocument][L3] Preprocessing complete for '696'; 186 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '696'.
[indexDocument][L3] Recorded doc length 186 for '696'.
[indexDocument][L3] Tokens added to the inverted index for '696'.
[indexDocument][L2] Indexing document '1112'.
[indexDocument][L3] Opened file '1112'.
[preprocess][L2] Starting preprocessing for '1112'.
[preprocess][L3] 1112: Tokenized into 142 tokens.
[preprocess][L3] 1112: Removed stop words; 84 tokens remain.
[preprocess][L3] 1112: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1112'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1112'.
[indexDocument][L3] Recorded doc length 84 for '1112'.
[indexDocument][L3] Tokens added to the inverted index for '1112'.
[indexDocument][L2] Indexing document '1320'.
[indexDocument][L3] Opened file '1320'.
[preprocess][L2] Starting preprocessing for '1320'.
[preprocess][L3] 1320: Tokenized into 210 tokens.
[preprocess][L3] 1320: Removed stop words; 128 tokens remain.
[preprocess][L3] 1320: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1320'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1320'.
[indexDocument][L3] Recorded doc length 128 for '1320'.
[indexDocument][L3] Tokens added to the inverted index for '1320'.
[indexDocument][L2] Indexing document '808'.
[indexDocument][L3] Opened file '808'.
[preprocess][L2] Starting preprocessing for '808'.
[preprocess][L3] 808: Tokenized into 187 tokens.
[preprocess][L3] 808: Removed stop words; 112 tokens remain.
[preprocess][L3] 808: Stemming complete.
[indexDocument][L3] Preprocessing complete for '808'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '808'.
[indexDocument][L3] Recorded doc length 112 for '808'.
[indexDocument][L3] Tokens added to the inverted index for '808'.
[indexDocument][L2] Indexing document '34'.
[indexDocument][L3] Opened file '34'.
[preprocess][L2] Starting preprocessing for '34'.
[preprocess][L3] 34: Tokenized into 182 tokens.
[preprocess][L3] 34: Removed stop words; 113 tokens remain.
[preprocess][L3] 34: Stemming complete.
[indexDocument][L3] Preprocessing complete for '34'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '34'.
[indexDocument][L3] Recorded doc length 113 for '34'.
[indexDocument][L3] Tokens added to the inverted index for '34'.
[indexDocument][L2] Indexing document '206'.
[indexDocument][L3] Opened file '206'.
[preprocess][L2] Starting preprocessing for '206'.
[preprocess][L3] 206: Tokenized into 300 tokens.
[preprocess][L3] 206: Removed stop words; 164 tokens remain.
[preprocess][L3] 206: Stemming complete.
[indexDocument][L3] Preprocessing complete for '206'; 164 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '206'.
[indexDocument][L3] Recorded doc length 164 for '206'.
[indexDocument][L3] Tokens added to the inverted index for '206'.
[indexDocument][L2] Indexing document '662'.
[indexDocument][L3] Opened file '662'.
[preprocess][L2] Starting preprocessing for '662'.
[preprocess][L3] 662: Tokenized into 355 tokens.
[preprocess][L3] 662: Removed stop words; 221 tokens remain.
[preprocess][L3] 662: Stemming complete.
[indexDocument][L3] Preprocessing complete for '662'; 221 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '662'.
[indexDocument][L3] Recorded doc length 221 for '662'.
[indexDocument][L3] Tokens added to the inverted index for '662'.
[indexDocument][L2] Indexing document '450'.
[indexDocument][L3] Opened file '450'.
[preprocess][L2] Starting preprocessing for '450'.
[preprocess][L3] 450: Tokenized into 82 tokens.
[preprocess][L3] 450: Removed stop words; 54 tokens remain.
[preprocess][L3] 450: Stemming complete.
[indexDocument][L3] Preprocessing complete for '450'; 54 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '450'.
[indexDocument][L3] Recorded doc length 54 for '450'.
[indexDocument][L3] Tokens added to the inverted index for '450'.
[indexDocument][L2] Indexing document '830'.
[indexDocument][L3] Opened file '830'.
[preprocess][L2] Starting preprocessing for '830'.
[preprocess][L3] 830: Tokenized into 197 tokens.
[preprocess][L3] 830: Removed stop words; 120 tokens remain.
[preprocess][L3] 830: Stemming complete.
[indexDocument][L3] Preprocessing complete for '830'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '830'.
[indexDocument][L3] Recorded doc length 120 for '830'.
[indexDocument][L3] Tokens added to the inverted index for '830'.
[indexDocument][L2] Indexing document '468'.
[indexDocument][L3] Opened file '468'.
[preprocess][L2] Starting preprocessing for '468'.
[preprocess][L3] 468: Tokenized into 118 tokens.
[preprocess][L3] 468: Removed stop words; 67 tokens remain.
[preprocess][L3] 468: Stemming complete.
[indexDocument][L3] Preprocessing complete for '468'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '468'.
[indexDocument][L3] Recorded doc length 67 for '468'.
[indexDocument][L3] Tokens added to the inverted index for '468'.
[indexDocument][L2] Indexing document '1318'.
[indexDocument][L3] Opened file '1318'.
[preprocess][L2] Starting preprocessing for '1318'.
[preprocess][L3] 1318: Tokenized into 85 tokens.
[preprocess][L3] 1318: Removed stop words; 57 tokens remain.
[preprocess][L3] 1318: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1318'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1318'.
[indexDocument][L3] Recorded doc length 57 for '1318'.
[indexDocument][L3] Tokens added to the inverted index for '1318'.
[indexDocument][L2] Indexing document '457'.
[indexDocument][L3] Opened file '457'.
[preprocess][L2] Starting preprocessing for '457'.
[preprocess][L3] 457: Tokenized into 131 tokens.
[preprocess][L3] 457: Removed stop words; 79 tokens remain.
[preprocess][L3] 457: Stemming complete.
[indexDocument][L3] Preprocessing complete for '457'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '457'.
[indexDocument][L3] Recorded doc length 79 for '457'.
[indexDocument][L3] Tokens added to the inverted index for '457'.
[indexDocument][L2] Indexing document '665'.
[indexDocument][L3] Opened file '665'.
[preprocess][L2] Starting preprocessing for '665'.
[preprocess][L3] 665: Tokenized into 143 tokens.
[preprocess][L3] 665: Removed stop words; 78 tokens remain.
[preprocess][L3] 665: Stemming complete.
[indexDocument][L3] Preprocessing complete for '665'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '665'.
[indexDocument][L3] Recorded doc length 78 for '665'.
[indexDocument][L3] Tokens added to the inverted index for '665'.
[indexDocument][L2] Indexing document '201'.
[indexDocument][L3] Opened file '201'.
[preprocess][L2] Starting preprocessing for '201'.
[preprocess][L3] 201: Tokenized into 188 tokens.
[preprocess][L3] 201: Removed stop words; 122 tokens remain.
[preprocess][L3] 201: Stemming complete.
[indexDocument][L3] Preprocessing complete for '201'; 122 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '201'.
[indexDocument][L3] Recorded doc length 122 for '201'.
[indexDocument][L3] Tokens added to the inverted index for '201'.
[indexDocument][L2] Indexing document '33'.
[indexDocument][L3] Opened file '33'.
[preprocess][L2] Starting preprocessing for '33'.
[preprocess][L3] 33: Tokenized into 271 tokens.
[preprocess][L3] 33: Removed stop words; 159 tokens remain.
[preprocess][L3] 33: Stemming complete.
[indexDocument][L3] Preprocessing complete for '33'; 159 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '33'.
[indexDocument][L3] Recorded doc length 159 for '33'.
[indexDocument][L3] Tokens added to the inverted index for '33'.
[indexDocument][L2] Indexing document '1327'.
[indexDocument][L3] Opened file '1327'.
[preprocess][L2] Starting preprocessing for '1327'.
[preprocess][L3] 1327: Tokenized into 167 tokens.
[preprocess][L3] 1327: Removed stop words; 95 tokens remain.
[preprocess][L3] 1327: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1327'; 95 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1327'.
[indexDocument][L3] Recorded doc length 95 for '1327'.
[indexDocument][L3] Tokens added to the inverted index for '1327'.
[indexDocument][L2] Indexing document '1115'.
[indexDocument][L3] Opened file '1115'.
[preprocess][L2] Starting preprocessing for '1115'.
[preprocess][L3] 1115: Tokenized into 190 tokens.
[preprocess][L3] 1115: Removed stop words; 109 tokens remain.
[preprocess][L3] 1115: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1115'; 109 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1115'.
[indexDocument][L3] Recorded doc length 109 for '1115'.
[indexDocument][L3] Tokens added to the inverted index for '1115'.
[indexDocument][L2] Indexing document '691'.
[indexDocument][L3] Opened file '691'.
[preprocess][L2] Starting preprocessing for '691'.
[preprocess][L3] 691: Tokenized into 126 tokens.
[preprocess][L3] 691: Removed stop words; 82 tokens remain.
[preprocess][L3] 691: Stemming complete.
[indexDocument][L3] Preprocessing complete for '691'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '691'.
[indexDocument][L3] Recorded doc length 82 for '691'.
[indexDocument][L3] Tokens added to the inverted index for '691'.
[indexDocument][L2] Indexing document '1344'.
[indexDocument][L3] Opened file '1344'.
[preprocess][L2] Starting preprocessing for '1344'.
[preprocess][L3] 1344: Tokenized into 209 tokens.
[preprocess][L3] 1344: Removed stop words; 133 tokens remain.
[preprocess][L3] 1344: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1344'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1344'.
[indexDocument][L3] Recorded doc length 133 for '1344'.
[indexDocument][L3] Tokens added to the inverted index for '1344'.
[indexDocument][L2] Indexing document '296'.
[indexDocument][L3] Opened file '296'.
[preprocess][L2] Starting preprocessing for '296'.
[preprocess][L3] 296: Tokenized into 240 tokens.
[preprocess][L3] 296: Removed stop words; 140 tokens remain.
[preprocess][L3] 296: Stemming complete.
[indexDocument][L3] Preprocessing complete for '296'; 140 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '296'.
[indexDocument][L3] Recorded doc length 140 for '296'.
[indexDocument][L3] Tokens added to the inverted index for '296'.
[indexDocument][L2] Indexing document '898'.
[indexDocument][L3] Opened file '898'.
[preprocess][L2] Starting preprocessing for '898'.
[preprocess][L3] 898: Tokenized into 58 tokens.
[preprocess][L3] 898: Removed stop words; 37 tokens remain.
[preprocess][L3] 898: Stemming complete.
[indexDocument][L3] Preprocessing complete for '898'; 37 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '898'.
[indexDocument][L3] Recorded doc length 37 for '898'.
[indexDocument][L3] Tokens added to the inverted index for '898'.
[indexDocument][L2] Indexing document '1176'.
[indexDocument][L3] Opened file '1176'.
[preprocess][L2] Starting preprocessing for '1176'.
[preprocess][L3] 1176: Tokenized into 49 tokens.
[preprocess][L3] 1176: Removed stop words; 32 tokens remain.
[preprocess][L3] 1176: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1176'; 32 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1176'.
[indexDocument][L3] Recorded doc length 32 for '1176'.
[indexDocument][L3] Tokens added to the inverted index for '1176'.
[indexDocument][L2] Indexing document '262'.
[indexDocument][L3] Opened file '262'.
[preprocess][L2] Starting preprocessing for '262'.
[preprocess][L3] 262: Tokenized into 455 tokens.
[preprocess][L3] 262: Removed stop words; 231 tokens remain.
[preprocess][L3] 262: Stemming complete.
[indexDocument][L3] Preprocessing complete for '262'; 231 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '262'.
[indexDocument][L3] Recorded doc length 231 for '262'.
[indexDocument][L3] Tokens added to the inverted index for '262'.
[indexDocument][L2] Indexing document '1182'.
[indexDocument][L3] Opened file '1182'.
[preprocess][L2] Starting preprocessing for '1182'.
[preprocess][L3] 1182: Tokenized into 216 tokens.
[preprocess][L3] 1182: Removed stop words; 134 tokens remain.
[preprocess][L3] 1182: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1182'; 134 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1182'.
[indexDocument][L3] Recorded doc length 134 for '1182'.
[indexDocument][L3] Tokens added to the inverted index for '1182'.
[indexDocument][L2] Indexing document '50'.
[indexDocument][L3] Opened file '50'.
[preprocess][L2] Starting preprocessing for '50'.
[preprocess][L3] 50: Tokenized into 156 tokens.
[preprocess][L3] 50: Removed stop words; 101 tokens remain.
[preprocess][L3] 50: Stemming complete.
[indexDocument][L3] Preprocessing complete for '50'; 101 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '50'.
[indexDocument][L3] Recorded doc length 101 for '50'.
[indexDocument][L3] Tokens added to the inverted index for '50'.
[indexDocument][L2] Indexing document '434'.
[indexDocument][L3] Opened file '434'.
[preprocess][L2] Starting preprocessing for '434'.
[preprocess][L3] 434: Tokenized into 243 tokens.
[preprocess][L3] 434: Removed stop words; 138 tokens remain.
[preprocess][L3] 434: Stemming complete.
[indexDocument][L3] Preprocessing complete for '434'; 138 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '434'.
[indexDocument][L3] Recorded doc length 138 for '434'.
[indexDocument][L3] Tokens added to the inverted index for '434'.
[indexDocument][L2] Indexing document '606'.
[indexDocument][L3] Opened file '606'.
[preprocess][L2] Starting preprocessing for '606'.
[preprocess][L3] 606: Tokenized into 166 tokens.
[preprocess][L3] 606: Removed stop words; 104 tokens remain.
[preprocess][L3] 606: Stemming complete.
[indexDocument][L3] Preprocessing complete for '606'; 104 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '606'.
[indexDocument][L3] Recorded doc length 104 for '606'.
[indexDocument][L3] Tokens added to the inverted index for '606'.
[indexDocument][L2] Indexing document '1149'.
[indexDocument][L3] Opened file '1149'.
[preprocess][L2] Starting preprocessing for '1149'.
[preprocess][L3] 1149: Tokenized into 205 tokens.
[preprocess][L3] 1149: Removed stop words; 119 tokens remain.
[preprocess][L3] 1149: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1149'; 119 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1149'.
[indexDocument][L3] Recorded doc length 119 for '1149'.
[indexDocument][L3] Tokens added to the inverted index for '1149'.
[indexDocument][L2] Indexing document '639'.
[indexDocument][L3] Opened file '639'.
[preprocess][L2] Starting preprocessing for '639'.
[preprocess][L3] 639: Tokenized into 92 tokens.
[preprocess][L3] 639: Removed stop words; 58 tokens remain.
[preprocess][L3] 639: Stemming complete.
[indexDocument][L3] Preprocessing complete for '639'; 58 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '639'.
[indexDocument][L3] Recorded doc length 58 for '639'.
[indexDocument][L3] Tokens added to the inverted index for '639'.
[indexDocument][L2] Indexing document '853'.
[indexDocument][L3] Opened file '853'.
[preprocess][L2] Starting preprocessing for '853'.
[preprocess][L3] 853: Tokenized into 57 tokens.
[preprocess][L3] 853: Removed stop words; 38 tokens remain.
[preprocess][L3] 853: Stemming complete.
[indexDocument][L3] Preprocessing complete for '853'; 38 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '853'.
[indexDocument][L3] Recorded doc length 38 for '853'.
[indexDocument][L3] Tokens added to the inverted index for '853'.
[indexDocument][L2] Indexing document '601'.
[indexDocument][L3] Opened file '601'.
[preprocess][L2] Starting preprocessing for '601'.
[preprocess][L3] 601: Tokenized into 250 tokens.
[preprocess][L3] 601: Removed stop words; 139 tokens remain.
[preprocess][L3] 601: Stemming complete.
[indexDocument][L3] Preprocessing complete for '601'; 139 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '601'.
[indexDocument][L3] Recorded doc length 139 for '601'.
[indexDocument][L3] Tokens added to the inverted index for '601'.
[indexDocument][L2] Indexing document '433'.
[indexDocument][L3] Opened file '433'.
[preprocess][L2] Starting preprocessing for '433'.
[preprocess][L3] 433: Tokenized into 387 tokens.
[preprocess][L3] 433: Removed stop words; 241 tokens remain.
[preprocess][L3] 433: Stemming complete.
[indexDocument][L3] Preprocessing complete for '433'; 241 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '433'.
[indexDocument][L3] Recorded doc length 241 for '433'.
[indexDocument][L3] Tokens added to the inverted index for '433'.
[indexDocument][L2] Indexing document '1185'.
[indexDocument][L3] Opened file '1185'.
[preprocess][L2] Starting preprocessing for '1185'.
[preprocess][L3] 1185: Tokenized into 153 tokens.
[preprocess][L3] 1185: Removed stop words; 99 tokens remain.
[preprocess][L3] 1185: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1185'; 99 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1185'.
[indexDocument][L3] Recorded doc length 99 for '1185'.
[indexDocument][L3] Tokens added to the inverted index for '1185'.
[indexDocument][L2] Indexing document '57'.
[indexDocument][L3] Opened file '57'.
[preprocess][L2] Starting preprocessing for '57'.
[preprocess][L3] 57: Tokenized into 184 tokens.
[preprocess][L3] 57: Removed stop words; 112 tokens remain.
[preprocess][L3] 57: Stemming complete.
[indexDocument][L3] Preprocessing complete for '57'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '57'.
[indexDocument][L3] Recorded doc length 112 for '57'.
[indexDocument][L3] Tokens added to the inverted index for '57'.
[indexDocument][L2] Indexing document '265'.
[indexDocument][L3] Opened file '265'.
[preprocess][L2] Starting preprocessing for '265'.
[preprocess][L3] 265: Tokenized into 90 tokens.
[preprocess][L3] 265: Removed stop words; 52 tokens remain.
[preprocess][L3] 265: Stemming complete.
[indexDocument][L3] Preprocessing complete for '265'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '265'.
[indexDocument][L3] Recorded doc length 52 for '265'.
[indexDocument][L3] Tokens added to the inverted index for '265'.
[indexDocument][L2] Indexing document '1171'.
[indexDocument][L3] Opened file '1171'.
[preprocess][L2] Starting preprocessing for '1171'.
[preprocess][L3] 1171: Tokenized into 82 tokens.
[preprocess][L3] 1171: Removed stop words; 52 tokens remain.
[preprocess][L3] 1171: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1171'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1171'.
[indexDocument][L3] Recorded doc length 52 for '1171'.
[indexDocument][L3] Tokens added to the inverted index for '1171'.
[indexDocument][L2] Indexing document '1343'.
[indexDocument][L3] Opened file '1343'.
[preprocess][L2] Starting preprocessing for '1343'.
[preprocess][L3] 1343: Tokenized into 262 tokens.
[preprocess][L3] 1343: Removed stop words; 148 tokens remain.
[preprocess][L3] 1343: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1343'; 148 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1343'.
[indexDocument][L3] Recorded doc length 148 for '1343'.
[indexDocument][L3] Tokens added to the inverted index for '1343'.
[indexDocument][L2] Indexing document '291'.
[indexDocument][L3] Opened file '291'.
[preprocess][L2] Starting preprocessing for '291'.
[preprocess][L3] 291: Tokenized into 68 tokens.
[preprocess][L3] 291: Removed stop words; 46 tokens remain.
[preprocess][L3] 291: Stemming complete.
[indexDocument][L3] Preprocessing complete for '291'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '291'.
[indexDocument][L3] Recorded doc length 46 for '291'.
[indexDocument][L3] Tokens added to the inverted index for '291'.
[indexDocument][L2] Indexing document '1388'.
[indexDocument][L3] Opened file '1388'.
[preprocess][L2] Starting preprocessing for '1388'.
[preprocess][L3] 1388: Tokenized into 100 tokens.
[preprocess][L3] 1388: Removed stop words; 60 tokens remain.
[preprocess][L3] 1388: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1388'; 60 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1388'.
[indexDocument][L3] Recorded doc length 60 for '1388'.
[indexDocument][L3] Tokens added to the inverted index for '1388'.
[indexDocument][L2] Indexing document '854'.
[indexDocument][L3] Opened file '854'.
[preprocess][L2] Starting preprocessing for '854'.
[preprocess][L3] 854: Tokenized into 44 tokens.
[preprocess][L3] 854: Removed stop words; 33 tokens remain.
[preprocess][L3] 854: Stemming complete.
[indexDocument][L3] Preprocessing complete for '854'; 33 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '854'.
[indexDocument][L3] Recorded doc length 33 for '854'.
[indexDocument][L3] Tokens added to the inverted index for '854'.
[indexDocument][L2] Indexing document '68'.
[indexDocument][L3] Opened file '68'.
[preprocess][L2] Starting preprocessing for '68'.
[preprocess][L3] 68: Tokenized into 116 tokens.
[preprocess][L3] 68: Removed stop words; 70 tokens remain.
[preprocess][L3] 68: Stemming complete.
[indexDocument][L3] Preprocessing complete for '68'; 70 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '68'.
[indexDocument][L3] Recorded doc length 70 for '68'.
[indexDocument][L3] Tokens added to the inverted index for '68'.
[indexDocument][L2] Indexing document '61'.
[indexDocument][L3] Opened file '61'.
[preprocess][L2] Starting preprocessing for '61'.
[preprocess][L3] 61: Tokenized into 137 tokens.
[preprocess][L3] 61: Removed stop words; 75 tokens remain.
[preprocess][L3] 61: Stemming complete.
[indexDocument][L3] Preprocessing complete for '61'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '61'.
[indexDocument][L3] Recorded doc length 75 for '61'.
[indexDocument][L3] Tokens added to the inverted index for '61'.
[indexDocument][L2] Indexing document '253'.
[indexDocument][L3] Opened file '253'.
[preprocess][L2] Starting preprocessing for '253'.
[preprocess][L3] 253: Tokenized into 169 tokens.
[preprocess][L3] 253: Removed stop words; 94 tokens remain.
[preprocess][L3] 253: Stemming complete.
[indexDocument][L3] Preprocessing complete for '253'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '253'.
[indexDocument][L3] Recorded doc length 94 for '253'.
[indexDocument][L3] Tokens added to the inverted index for '253'.
[indexDocument][L2] Indexing document '1381'.
[indexDocument][L3] Opened file '1381'.
[preprocess][L2] Starting preprocessing for '1381'.
[preprocess][L3] 1381: Tokenized into 293 tokens.
[preprocess][L3] 1381: Removed stop words; 179 tokens remain.
[preprocess][L3] 1381: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1381'; 179 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1381'.
[indexDocument][L3] Recorded doc length 179 for '1381'.
[indexDocument][L3] Tokens added to the inverted index for '1381'.
[indexDocument][L2] Indexing document '637'.
[indexDocument][L3] Opened file '637'.
[preprocess][L2] Starting preprocessing for '637'.
[preprocess][L3] 637: Tokenized into 119 tokens.
[preprocess][L3] 637: Removed stop words; 73 tokens remain.
[preprocess][L3] 637: Stemming complete.
[indexDocument][L3] Preprocessing complete for '637'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '637'.
[indexDocument][L3] Recorded doc length 73 for '637'.
[indexDocument][L3] Tokens added to the inverted index for '637'.
[indexDocument][L2] Indexing document '405'.
[indexDocument][L3] Opened file '405'.
[preprocess][L2] Starting preprocessing for '405'.
[preprocess][L3] 405: Tokenized into 47 tokens.
[preprocess][L3] 405: Removed stop words; 41 tokens remain.
[preprocess][L3] 405: Stemming complete.
[indexDocument][L3] Preprocessing complete for '405'; 41 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '405'.
[indexDocument][L3] Recorded doc length 41 for '405'.
[indexDocument][L3] Tokens added to the inverted index for '405'.
[indexDocument][L2] Indexing document '95'.
[indexDocument][L3] Opened file '95'.
[preprocess][L2] Starting preprocessing for '95'.
[preprocess][L3] 95: Tokenized into 129 tokens.
[preprocess][L3] 95: Removed stop words; 71 tokens remain.
[preprocess][L3] 95: Stemming complete.
[indexDocument][L3] Preprocessing complete for '95'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '95'.
[indexDocument][L3] Recorded doc length 71 for '95'.
[indexDocument][L3] Tokens added to the inverted index for '95'.
[indexDocument][L2] Indexing document '1147'.
[indexDocument][L3] Opened file '1147'.
[preprocess][L2] Starting preprocessing for '1147'.
[preprocess][L3] 1147: Tokenized into 455 tokens.
[preprocess][L3] 1147: Removed stop words; 252 tokens remain.
[preprocess][L3] 1147: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1147'; 252 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1147'.
[indexDocument][L3] Recorded doc length 252 for '1147'.
[indexDocument][L3] Tokens added to the inverted index for '1147'.
[indexDocument][L2] Indexing document '1375'.
[indexDocument][L3] Opened file '1375'.
[preprocess][L2] Starting preprocessing for '1375'.
[preprocess][L3] 1375: Tokenized into 312 tokens.
[preprocess][L3] 1375: Removed stop words; 182 tokens remain.
[preprocess][L3] 1375: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1375'; 182 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1375'.
[indexDocument][L3] Recorded doc length 182 for '1375'.
[indexDocument][L3] Tokens added to the inverted index for '1375'.
[indexDocument][L2] Indexing document '608'.
[indexDocument][L3] Opened file '608'.
[preprocess][L2] Starting preprocessing for '608'.
[preprocess][L3] 608: Tokenized into 140 tokens.
[preprocess][L3] 608: Removed stop words; 84 tokens remain.
[preprocess][L3] 608: Stemming complete.
[indexDocument][L3] Preprocessing complete for '608'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '608'.
[indexDocument][L3] Recorded doc length 84 for '608'.
[indexDocument][L3] Tokens added to the inverted index for '608'.
[indexDocument][L2] Indexing document '862'.
[indexDocument][L3] Opened file '862'.
[preprocess][L2] Starting preprocessing for '862'.
[preprocess][L3] 862: Tokenized into 64 tokens.
[preprocess][L3] 862: Removed stop words; 39 tokens remain.
[preprocess][L3] 862: Stemming complete.
[indexDocument][L3] Preprocessing complete for '862'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '862'.
[indexDocument][L3] Recorded doc length 39 for '862'.
[indexDocument][L3] Tokens added to the inverted index for '862'.
[indexDocument][L2] Indexing document '298'.
[indexDocument][L3] Opened file '298'.
[preprocess][L2] Starting preprocessing for '298'.
[preprocess][L3] 298: Tokenized into 62 tokens.
[preprocess][L3] 298: Removed stop words; 39 tokens remain.
[preprocess][L3] 298: Stemming complete.
[indexDocument][L3] Preprocessing complete for '298'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '298'.
[indexDocument][L3] Recorded doc length 39 for '298'.
[indexDocument][L3] Tokens added to the inverted index for '298'.
[indexDocument][L2] Indexing document '896'.
[indexDocument][L3] Opened file '896'.
[preprocess][L2] Starting preprocessing for '896'.
[preprocess][L3] 896: Tokenized into 105 tokens.
[preprocess][L3] 896: Removed stop words; 59 tokens remain.
[preprocess][L3] 896: Stemming complete.
[indexDocument][L3] Preprocessing complete for '896'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '896'.
[indexDocument][L3] Recorded doc length 59 for '896'.
[indexDocument][L3] Tokens added to the inverted index for '896'.
[indexDocument][L2] Indexing document '1178'.
[indexDocument][L3] Opened file '1178'.
[preprocess][L2] Starting preprocessing for '1178'.
[preprocess][L3] 1178: Tokenized into 119 tokens.
[preprocess][L3] 1178: Removed stop words; 71 tokens remain.
[preprocess][L3] 1178: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1178'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1178'.
[indexDocument][L3] Recorded doc length 71 for '1178'.
[indexDocument][L3] Tokens added to the inverted index for '1178'.
[indexDocument][L2] Indexing document '1372'.
[indexDocument][L3] Opened file '1372'.
[preprocess][L2] Starting preprocessing for '1372'.
[preprocess][L3] 1372: Tokenized into 192 tokens.
[preprocess][L3] 1372: Removed stop words; 110 tokens remain.
[preprocess][L3] 1372: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1372'; 110 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1372'.
[indexDocument][L3] Recorded doc length 110 for '1372'.
[indexDocument][L3] Tokens added to the inverted index for '1372'.
[indexDocument][L2] Indexing document '92'.
[indexDocument][L3] Opened file '92'.
[preprocess][L2] Starting preprocessing for '92'.
[preprocess][L3] 92: Tokenized into 203 tokens.
[preprocess][L3] 92: Removed stop words; 132 tokens remain.
[preprocess][L3] 92: Stemming complete.
[indexDocument][L3] Preprocessing complete for '92'; 132 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '92'.
[indexDocument][L3] Recorded doc length 132 for '92'.
[indexDocument][L3] Tokens added to the inverted index for '92'.
[indexDocument][L2] Indexing document '1140'.
[indexDocument][L3] Opened file '1140'.
[preprocess][L2] Starting preprocessing for '1140'.
[preprocess][L3] 1140: Tokenized into 55 tokens.
[preprocess][L3] 1140: Removed stop words; 37 tokens remain.
[preprocess][L3] 1140: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1140'; 37 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1140'.
[indexDocument][L3] Recorded doc length 37 for '1140'.
[indexDocument][L3] Tokens added to the inverted index for '1140'.
[indexDocument][L2] Indexing document '402'.
[indexDocument][L3] Opened file '402'.
[preprocess][L2] Starting preprocessing for '402'.
[preprocess][L3] 402: Tokenized into 122 tokens.
[preprocess][L3] 402: Removed stop words; 73 tokens remain.
[preprocess][L3] 402: Stemming complete.
[indexDocument][L3] Preprocessing complete for '402'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '402'.
[indexDocument][L3] Recorded doc length 73 for '402'.
[indexDocument][L3] Tokens added to the inverted index for '402'.
[indexDocument][L2] Indexing document '630'.
[indexDocument][L3] Opened file '630'.
[preprocess][L2] Starting preprocessing for '630'.
[preprocess][L3] 630: Tokenized into 151 tokens.
[preprocess][L3] 630: Removed stop words; 105 tokens remain.
[preprocess][L3] 630: Stemming complete.
[indexDocument][L3] Preprocessing complete for '630'; 105 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '630'.
[indexDocument][L3] Recorded doc length 105 for '630'.
[indexDocument][L3] Tokens added to the inverted index for '630'.
[indexDocument][L2] Indexing document '254'.
[indexDocument][L3] Opened file '254'.
[preprocess][L2] Starting preprocessing for '254'.
[preprocess][L3] 254: Tokenized into 65 tokens.
[preprocess][L3] 254: Removed stop words; 40 tokens remain.
[preprocess][L3] 254: Stemming complete.
[indexDocument][L3] Preprocessing complete for '254'; 40 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '254'.
[indexDocument][L3] Recorded doc length 40 for '254'.
[indexDocument][L3] Tokens added to the inverted index for '254'.
[indexDocument][L2] Indexing document '1386'.
[indexDocument][L3] Opened file '1386'.
[preprocess][L2] Starting preprocessing for '1386'.
[preprocess][L3] 1386: Tokenized into 226 tokens.
[preprocess][L3] 1386: Removed stop words; 135 tokens remain.
[preprocess][L3] 1386: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1386'; 135 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1386'.
[indexDocument][L3] Recorded doc length 135 for '1386'.
[indexDocument][L3] Tokens added to the inverted index for '1386'.
[indexDocument][L2] Indexing document '66'.
[indexDocument][L3] Opened file '66'.
[preprocess][L2] Starting preprocessing for '66'.
[preprocess][L3] 66: Tokenized into 172 tokens.
[preprocess][L3] 66: Removed stop words; 115 tokens remain.
[preprocess][L3] 66: Stemming complete.
[indexDocument][L3] Preprocessing complete for '66'; 115 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '66'.
[indexDocument][L3] Recorded doc length 115 for '66'.
[indexDocument][L3] Tokens added to the inverted index for '66'.
[indexDocument][L2] Indexing document '891'.
[indexDocument][L3] Opened file '891'.
[preprocess][L2] Starting preprocessing for '891'.
[preprocess][L3] 891: Tokenized into 180 tokens.
[preprocess][L3] 891: Removed stop words; 113 tokens remain.
[preprocess][L3] 891: Stemming complete.
[indexDocument][L3] Preprocessing complete for '891'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '891'.
[indexDocument][L3] Recorded doc length 113 for '891'.
[indexDocument][L3] Tokens added to the inverted index for '891'.
[indexDocument][L2] Indexing document '865'.
[indexDocument][L3] Opened file '865'.
[preprocess][L2] Starting preprocessing for '865'.
[preprocess][L3] 865: Tokenized into 134 tokens.
[preprocess][L3] 865: Removed stop words; 79 tokens remain.
[preprocess][L3] 865: Stemming complete.
[indexDocument][L3] Preprocessing complete for '865'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '865'.
[indexDocument][L3] Recorded doc length 79 for '865'.
[indexDocument][L3] Tokens added to the inverted index for '865'.
[indexDocument][L2] Indexing document '59'.
[indexDocument][L3] Opened file '59'.
[preprocess][L2] Starting preprocessing for '59'.
[preprocess][L3] 59: Tokenized into 235 tokens.
[preprocess][L3] 59: Removed stop words; 147 tokens remain.
[preprocess][L3] 59: Stemming complete.
[indexDocument][L3] Preprocessing complete for '59'; 147 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '59'.
[indexDocument][L3] Recorded doc length 147 for '59'.
[indexDocument][L3] Tokens added to the inverted index for '59'.
[indexDocument][L2] Indexing document '1319'.
[indexDocument][L3] Opened file '1319'.
[preprocess][L2] Starting preprocessing for '1319'.
[preprocess][L3] 1319: Tokenized into 253 tokens.
[preprocess][L3] 1319: Removed stop words; 157 tokens remain.
[preprocess][L3] 1319: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1319'; 157 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1319'.
[indexDocument][L3] Recorded doc length 157 for '1319'.
[indexDocument][L3] Tokens added to the inverted index for '1319'.
[indexDocument][L2] Indexing document '831'.
[indexDocument][L3] Opened file '831'.
[preprocess][L2] Starting preprocessing for '831'.
[preprocess][L3] 831: Tokenized into 100 tokens.
[preprocess][L3] 831: Removed stop words; 61 tokens remain.
[preprocess][L3] 831: Stemming complete.
[indexDocument][L3] Preprocessing complete for '831'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '831'.
[indexDocument][L3] Recorded doc length 61 for '831'.
[indexDocument][L3] Tokens added to the inverted index for '831'.
[indexDocument][L2] Indexing document '469'.
[indexDocument][L3] Opened file '469'.
[preprocess][L2] Starting preprocessing for '469'.
[preprocess][L3] 469: Tokenized into 111 tokens.
[preprocess][L3] 469: Removed stop words; 66 tokens remain.
[preprocess][L3] 469: Stemming complete.
[indexDocument][L3] Preprocessing complete for '469'; 66 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '469'.
[indexDocument][L3] Recorded doc length 66 for '469'.
[indexDocument][L3] Tokens added to the inverted index for '469'.
[indexDocument][L2] Indexing document '1114'.
[indexDocument][L3] Opened file '1114'.
[preprocess][L2] Starting preprocessing for '1114'.
[preprocess][L3] 1114: Tokenized into 212 tokens.
[preprocess][L3] 1114: Removed stop words; 128 tokens remain.
[preprocess][L3] 1114: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1114'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1114'.
[indexDocument][L3] Recorded doc length 128 for '1114'.
[indexDocument][L3] Tokens added to the inverted index for '1114'.
[indexDocument][L2] Indexing document '1326'.
[indexDocument][L3] Opened file '1326'.
[preprocess][L2] Starting preprocessing for '1326'.
[preprocess][L3] 1326: Tokenized into 152 tokens.
[preprocess][L3] 1326: Removed stop words; 96 tokens remain.
[preprocess][L3] 1326: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1326'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1326'.
[indexDocument][L3] Recorded doc length 96 for '1326'.
[indexDocument][L3] Tokens added to the inverted index for '1326'.
[indexDocument][L2] Indexing document '690'.
[indexDocument][L3] Opened file '690'.
[preprocess][L2] Starting preprocessing for '690'.
[preprocess][L3] 690: Tokenized into 130 tokens.
[preprocess][L3] 690: Removed stop words; 75 tokens remain.
[preprocess][L3] 690: Stemming complete.
[indexDocument][L3] Preprocessing complete for '690'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '690'.
[indexDocument][L3] Recorded doc length 75 for '690'.
[indexDocument][L3] Tokens added to the inverted index for '690'.
[indexDocument][L2] Indexing document '664'.
[indexDocument][L3] Opened file '664'.
[preprocess][L2] Starting preprocessing for '664'.
[preprocess][L3] 664: Tokenized into 80 tokens.
[preprocess][L3] 664: Removed stop words; 50 tokens remain.
[preprocess][L3] 664: Stemming complete.
[indexDocument][L3] Preprocessing complete for '664'; 50 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '664'.
[indexDocument][L3] Recorded doc length 50 for '664'.
[indexDocument][L3] Tokens added to the inverted index for '664'.
[indexDocument][L2] Indexing document '456'.
[indexDocument][L3] Opened file '456'.
[preprocess][L2] Starting preprocessing for '456'.
[preprocess][L3] 456: Tokenized into 247 tokens.
[preprocess][L3] 456: Removed stop words; 159 tokens remain.
[preprocess][L3] 456: Stemming complete.
[indexDocument][L3] Preprocessing complete for '456'; 159 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '456'.
[indexDocument][L3] Recorded doc length 159 for '456'.
[indexDocument][L3] Tokens added to the inverted index for '456'.
[indexDocument][L2] Indexing document '32'.
[indexDocument][L3] Opened file '32'.
[preprocess][L2] Starting preprocessing for '32'.
[preprocess][L3] 32: Tokenized into 188 tokens.
[preprocess][L3] 32: Removed stop words; 108 tokens remain.
[preprocess][L3] 32: Stemming complete.
[indexDocument][L3] Preprocessing complete for '32'; 108 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '32'.
[indexDocument][L3] Recorded doc length 108 for '32'.
[indexDocument][L3] Tokens added to the inverted index for '32'.
[indexDocument][L2] Indexing document '200'.
[indexDocument][L3] Opened file '200'.
[preprocess][L2] Starting preprocessing for '200'.
[preprocess][L3] 200: Tokenized into 134 tokens.
[preprocess][L3] 200: Removed stop words; 92 tokens remain.
[preprocess][L3] 200: Stemming complete.
[indexDocument][L3] Preprocessing complete for '200'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '200'.
[indexDocument][L3] Recorded doc length 92 for '200'.
[indexDocument][L3] Tokens added to the inverted index for '200'.
[indexDocument][L2] Indexing document '836'.
[indexDocument][L3] Opened file '836'.
[preprocess][L2] Starting preprocessing for '836'.
[preprocess][L3] 836: Tokenized into 132 tokens.
[preprocess][L3] 836: Removed stop words; 90 tokens remain.
[preprocess][L3] 836: Stemming complete.
[indexDocument][L3] Preprocessing complete for '836'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '836'.
[indexDocument][L3] Recorded doc length 90 for '836'.
[indexDocument][L3] Tokens added to the inverted index for '836'.
[indexDocument][L2] Indexing document '238'.
[indexDocument][L3] Opened file '238'.
[preprocess][L2] Starting preprocessing for '238'.
[preprocess][L3] 238: Tokenized into 48 tokens.
[preprocess][L3] 238: Removed stop words; 31 tokens remain.
[preprocess][L3] 238: Stemming complete.
[indexDocument][L3] Preprocessing complete for '238'; 31 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '238'.
[indexDocument][L3] Recorded doc length 31 for '238'.
[indexDocument][L3] Tokens added to the inverted index for '238'.
[indexDocument][L2] Indexing document '207'.
[indexDocument][L3] Opened file '207'.
[preprocess][L2] Starting preprocessing for '207'.
[preprocess][L3] 207: Tokenized into 213 tokens.
[preprocess][L3] 207: Removed stop words; 109 tokens remain.
[preprocess][L3] 207: Stemming complete.
[indexDocument][L3] Preprocessing complete for '207'; 109 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '207'.
[indexDocument][L3] Recorded doc length 109 for '207'.
[indexDocument][L3] Tokens added to the inverted index for '207'.
[indexDocument][L2] Indexing document '35'.
[indexDocument][L3] Opened file '35'.
[preprocess][L2] Starting preprocessing for '35'.
[preprocess][L3] 35: Tokenized into 161 tokens.
[preprocess][L3] 35: Removed stop words; 98 tokens remain.
[preprocess][L3] 35: Stemming complete.
[indexDocument][L3] Preprocessing complete for '35'; 98 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '35'.
[indexDocument][L3] Recorded doc length 98 for '35'.
[indexDocument][L3] Tokens added to the inverted index for '35'.
[indexDocument][L2] Indexing document '809'.
[indexDocument][L3] Opened file '809'.
[preprocess][L2] Starting preprocessing for '809'.
[preprocess][L3] 809: Tokenized into 123 tokens.
[preprocess][L3] 809: Removed stop words; 78 tokens remain.
[preprocess][L3] 809: Stemming complete.
[indexDocument][L3] Preprocessing complete for '809'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '809'.
[indexDocument][L3] Recorded doc length 78 for '809'.
[indexDocument][L3] Tokens added to the inverted index for '809'.
[indexDocument][L2] Indexing document '451'.
[indexDocument][L3] Opened file '451'.
[preprocess][L2] Starting preprocessing for '451'.
[preprocess][L3] 451: Tokenized into 78 tokens.
[preprocess][L3] 451: Removed stop words; 49 tokens remain.
[preprocess][L3] 451: Stemming complete.
[indexDocument][L3] Preprocessing complete for '451'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '451'.
[indexDocument][L3] Recorded doc length 49 for '451'.
[indexDocument][L3] Tokens added to the inverted index for '451'.
[indexDocument][L2] Indexing document '663'.
[indexDocument][L3] Opened file '663'.
[preprocess][L2] Starting preprocessing for '663'.
[preprocess][L3] 663: Tokenized into 127 tokens.
[preprocess][L3] 663: Removed stop words; 81 tokens remain.
[preprocess][L3] 663: Stemming complete.
[indexDocument][L3] Preprocessing complete for '663'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '663'.
[indexDocument][L3] Recorded doc length 81 for '663'.
[indexDocument][L3] Tokens added to the inverted index for '663'.
[indexDocument][L2] Indexing document '697'.
[indexDocument][L3] Opened file '697'.
[preprocess][L2] Starting preprocessing for '697'.
[preprocess][L3] 697: Tokenized into 179 tokens.
[preprocess][L3] 697: Removed stop words; 102 tokens remain.
[preprocess][L3] 697: Stemming complete.
[indexDocument][L3] Preprocessing complete for '697'; 102 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '697'.
[indexDocument][L3] Recorded doc length 102 for '697'.
[indexDocument][L3] Tokens added to the inverted index for '697'.
[indexDocument][L2] Indexing document '1321'.
[indexDocument][L3] Opened file '1321'.
[preprocess][L2] Starting preprocessing for '1321'.
[preprocess][L3] 1321: Tokenized into 262 tokens.
[preprocess][L3] 1321: Removed stop words; 160 tokens remain.
[preprocess][L3] 1321: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1321'; 160 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1321'.
[indexDocument][L3] Recorded doc length 160 for '1321'.
[indexDocument][L3] Tokens added to the inverted index for '1321'.
[indexDocument][L2] Indexing document '1113'.
[indexDocument][L3] Opened file '1113'.
[preprocess][L2] Starting preprocessing for '1113'.
[preprocess][L3] 1113: Tokenized into 179 tokens.
[preprocess][L3] 1113: Removed stop words; 96 tokens remain.
[preprocess][L3] 1113: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1113'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1113'.
[indexDocument][L3] Recorded doc length 96 for '1113'.
[indexDocument][L3] Tokens added to the inverted index for '1113'.
[indexDocument][L2] Indexing document '800'.
[indexDocument][L3] Opened file '800'.
[preprocess][L2] Starting preprocessing for '800'.
[preprocess][L3] 800: Tokenized into 166 tokens.
[preprocess][L3] 800: Removed stop words; 103 tokens remain.
[preprocess][L3] 800: Stemming complete.
[indexDocument][L3] Preprocessing complete for '800'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '800'.
[indexDocument][L3] Recorded doc length 103 for '800'.
[indexDocument][L3] Tokens added to the inverted index for '800'.
[indexDocument][L2] Indexing document '458'.
[indexDocument][L3] Opened file '458'.
[preprocess][L2] Starting preprocessing for '458'.
[preprocess][L3] 458: Tokenized into 235 tokens.
[preprocess][L3] 458: Removed stop words; 146 tokens remain.
[preprocess][L3] 458: Stemming complete.
[indexDocument][L3] Preprocessing complete for '458'; 146 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '458'.
[indexDocument][L3] Recorded doc length 146 for '458'.
[indexDocument][L3] Tokens added to the inverted index for '458'.
[indexDocument][L2] Indexing document '1328'.
[indexDocument][L3] Opened file '1328'.
[preprocess][L2] Starting preprocessing for '1328'.
[preprocess][L3] 1328: Tokenized into 222 tokens.
[preprocess][L3] 1328: Removed stop words; 135 tokens remain.
[preprocess][L3] 1328: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1328'; 135 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1328'.
[indexDocument][L3] Recorded doc length 135 for '1328'.
[indexDocument][L3] Tokens added to the inverted index for '1328'.
[indexDocument][L2] Indexing document '467'.
[indexDocument][L3] Opened file '467'.
[preprocess][L2] Starting preprocessing for '467'.
[preprocess][L3] 467: Tokenized into 198 tokens.
[preprocess][L3] 467: Removed stop words; 122 tokens remain.
[preprocess][L3] 467: Stemming complete.
[indexDocument][L3] Preprocessing complete for '467'; 122 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '467'.
[indexDocument][L3] Recorded doc length 122 for '467'.
[indexDocument][L3] Tokens added to the inverted index for '467'.
[indexDocument][L2] Indexing document '655'.
[indexDocument][L3] Opened file '655'.
[preprocess][L2] Starting preprocessing for '655'.
[preprocess][L3] 655: Tokenized into 101 tokens.
[preprocess][L3] 655: Removed stop words; 65 tokens remain.
[preprocess][L3] 655: Stemming complete.
[indexDocument][L3] Preprocessing complete for '655'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '655'.
[indexDocument][L3] Recorded doc length 65 for '655'.
[indexDocument][L3] Tokens added to the inverted index for '655'.
[indexDocument][L2] Indexing document '231'.
[indexDocument][L3] Opened file '231'.
[preprocess][L2] Starting preprocessing for '231'.
[preprocess][L3] 231: Tokenized into 103 tokens.
[preprocess][L3] 231: Removed stop words; 67 tokens remain.
[preprocess][L3] 231: Stemming complete.
[indexDocument][L3] Preprocessing complete for '231'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '231'.
[indexDocument][L3] Recorded doc length 67 for '231'.
[indexDocument][L3] Tokens added to the inverted index for '231'.
[indexDocument][L2] Indexing document '3'.
[indexDocument][L3] Opened file '3'.
[preprocess][L2] Starting preprocessing for '3'.
[preprocess][L3] 3: Tokenized into 36 tokens.
[preprocess][L3] 3: Removed stop words; 26 tokens remain.
[preprocess][L3] 3: Stemming complete.
[indexDocument][L3] Preprocessing complete for '3'; 26 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '3'.
[indexDocument][L3] Recorded doc length 26 for '3'.
[indexDocument][L3] Tokens added to the inverted index for '3'.
[indexDocument][L2] Indexing document '1317'.
[indexDocument][L3] Opened file '1317'.
[preprocess][L2] Starting preprocessing for '1317'.
[preprocess][L3] 1317: Tokenized into 53 tokens.
[preprocess][L3] 1317: Removed stop words; 36 tokens remain.
[preprocess][L3] 1317: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1317'; 36 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1317'.
[indexDocument][L3] Recorded doc length 36 for '1317'.
[indexDocument][L3] Tokens added to the inverted index for '1317'.
[indexDocument][L2] Indexing document '1125'.
[indexDocument][L3] Opened file '1125'.
[preprocess][L2] Starting preprocessing for '1125'.
[preprocess][L3] 1125: Tokenized into 172 tokens.
[preprocess][L3] 1125: Removed stop words; 103 tokens remain.
[preprocess][L3] 1125: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1125'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1125'.
[indexDocument][L3] Recorded doc length 103 for '1125'.
[indexDocument][L3] Tokens added to the inverted index for '1125'.
[indexDocument][L2] Indexing document '493'.
[indexDocument][L3] Opened file '493'.
[preprocess][L2] Starting preprocessing for '493'.
[preprocess][L3] 493: Tokenized into 281 tokens.
[preprocess][L3] 493: Removed stop words; 164 tokens remain.
[preprocess][L3] 493: Stemming complete.
[indexDocument][L3] Preprocessing complete for '493'; 164 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '493'.
[indexDocument][L3] Recorded doc length 164 for '493'.
[indexDocument][L3] Tokens added to the inverted index for '493'.
[indexDocument][L2] Indexing document '699'.
[indexDocument][L3] Opened file '699'.
[preprocess][L2] Starting preprocessing for '699'.
[preprocess][L3] 699: Tokenized into 209 tokens.
[preprocess][L3] 699: Removed stop words; 123 tokens remain.
[preprocess][L3] 699: Stemming complete.
[indexDocument][L3] Preprocessing complete for '699'; 123 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '699'.
[indexDocument][L3] Recorded doc length 123 for '699'.
[indexDocument][L3] Tokens added to the inverted index for '699'.
[indexDocument][L2] Indexing document '209'.
[indexDocument][L3] Opened file '209'.
[preprocess][L2] Starting preprocessing for '209'.
[preprocess][L3] 209: Tokenized into 232 tokens.
[preprocess][L3] 209: Removed stop words; 148 tokens remain.
[preprocess][L3] 209: Stemming complete.
[indexDocument][L3] Preprocessing complete for '209'; 148 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '209'.
[indexDocument][L3] Recorded doc length 148 for '209'.
[indexDocument][L3] Tokens added to the inverted index for '209'.
[indexDocument][L2] Indexing document '807'.
[indexDocument][L3] Opened file '807'.
[preprocess][L2] Starting preprocessing for '807'.
[preprocess][L3] 807: Tokenized into 122 tokens.
[preprocess][L3] 807: Removed stop words; 74 tokens remain.
[preprocess][L3] 807: Stemming complete.
[indexDocument][L3] Preprocessing complete for '807'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '807'.
[indexDocument][L3] Recorded doc length 74 for '807'.
[indexDocument][L3] Tokens added to the inverted index for '807'.
[indexDocument][L2] Indexing document '494'.
[indexDocument][L3] Opened file '494'.
[preprocess][L2] Starting preprocessing for '494'.
[preprocess][L3] 494: Tokenized into 154 tokens.
[preprocess][L3] 494: Removed stop words; 99 tokens remain.
[preprocess][L3] 494: Stemming complete.
[indexDocument][L3] Preprocessing complete for '494'; 99 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '494'.
[indexDocument][L3] Recorded doc length 99 for '494'.
[indexDocument][L3] Tokens added to the inverted index for '494'.
[indexDocument][L2] Indexing document '1122'.
[indexDocument][L3] Opened file '1122'.
[preprocess][L2] Starting preprocessing for '1122'.
[preprocess][L3] 1122: Tokenized into 204 tokens.
[preprocess][L3] 1122: Removed stop words; 122 tokens remain.
[preprocess][L3] 1122: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1122'; 122 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1122'.
[indexDocument][L3] Recorded doc length 122 for '1122'.
[indexDocument][L3] Tokens added to the inverted index for '1122'.
[indexDocument][L2] Indexing document '1310'.
[indexDocument][L3] Opened file '1310'.
[preprocess][L2] Starting preprocessing for '1310'.
[preprocess][L3] 1310: Tokenized into 292 tokens.
[preprocess][L3] 1310: Removed stop words; 169 tokens remain.
[preprocess][L3] 1310: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1310'; 169 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1310'.
[indexDocument][L3] Recorded doc length 169 for '1310'.
[indexDocument][L3] Tokens added to the inverted index for '1310'.
[indexDocument][L2] Indexing document '4'.
[indexDocument][L3] Opened file '4'.
[preprocess][L2] Starting preprocessing for '4'.
[preprocess][L3] 4: Tokenized into 82 tokens.
[preprocess][L3] 4: Removed stop words; 52 tokens remain.
[preprocess][L3] 4: Stemming complete.
[indexDocument][L3] Preprocessing complete for '4'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '4'.
[indexDocument][L3] Recorded doc length 52 for '4'.
[indexDocument][L3] Tokens added to the inverted index for '4'.
[indexDocument][L2] Indexing document '838'.
[indexDocument][L3] Opened file '838'.
[preprocess][L2] Starting preprocessing for '838'.
[preprocess][L3] 838: Tokenized into 87 tokens.
[preprocess][L3] 838: Removed stop words; 56 tokens remain.
[preprocess][L3] 838: Stemming complete.
[indexDocument][L3] Preprocessing complete for '838'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '838'.
[indexDocument][L3] Recorded doc length 56 for '838'.
[indexDocument][L3] Tokens added to the inverted index for '838'.
[indexDocument][L2] Indexing document '236'.
[indexDocument][L3] Opened file '236'.
[preprocess][L2] Starting preprocessing for '236'.
[preprocess][L3] 236: Tokenized into 172 tokens.
[preprocess][L3] 236: Removed stop words; 97 tokens remain.
[preprocess][L3] 236: Stemming complete.
[indexDocument][L3] Preprocessing complete for '236'; 97 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '236'.
[indexDocument][L3] Recorded doc length 97 for '236'.
[indexDocument][L3] Tokens added to the inverted index for '236'.
[indexDocument][L2] Indexing document '652'.
[indexDocument][L3] Opened file '652'.
[preprocess][L2] Starting preprocessing for '652'.
[preprocess][L3] 652: Tokenized into 92 tokens.
[preprocess][L3] 652: Removed stop words; 55 tokens remain.
[preprocess][L3] 652: Stemming complete.
[indexDocument][L3] Preprocessing complete for '652'; 55 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '652'.
[indexDocument][L3] Recorded doc length 55 for '652'.
[indexDocument][L3] Tokens added to the inverted index for '652'.
[indexDocument][L2] Indexing document '460'.
[indexDocument][L3] Opened file '460'.
[preprocess][L2] Starting preprocessing for '460'.
[preprocess][L3] 460: Tokenized into 116 tokens.
[preprocess][L3] 460: Removed stop words; 66 tokens remain.
[preprocess][L3] 460: Stemming complete.
[indexDocument][L3] Preprocessing complete for '460'; 66 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '460'.
[indexDocument][L3] Recorded doc length 66 for '460'.
[indexDocument][L3] Tokens added to the inverted index for '460'.
[indexDocument][L2] Indexing document '720'.
[indexDocument][L3] Opened file '720'.
[preprocess][L2] Starting preprocessing for '720'.
[preprocess][L3] 720: Tokenized into 184 tokens.
[preprocess][L3] 720: Removed stop words; 116 tokens remain.
[preprocess][L3] 720: Stemming complete.
[indexDocument][L3] Preprocessing complete for '720'; 116 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '720'.
[indexDocument][L3] Recorded doc length 116 for '720'.
[indexDocument][L3] Tokens added to the inverted index for '720'.
[indexDocument][L2] Indexing document '512'.
[indexDocument][L3] Opened file '512'.
[preprocess][L2] Starting preprocessing for '512'.
[preprocess][L3] 512: Tokenized into 71 tokens.
[preprocess][L3] 512: Removed stop words; 45 tokens remain.
[preprocess][L3] 512: Stemming complete.
[indexDocument][L3] Preprocessing complete for '512'; 45 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '512'.
[indexDocument][L3] Recorded doc length 45 for '512'.
[indexDocument][L3] Tokens added to the inverted index for '512'.
[indexDocument][L2] Indexing document '176'.
[indexDocument][L3] Opened file '176'.
[preprocess][L2] Starting preprocessing for '176'.
[preprocess][L3] 176: Tokenized into 117 tokens.
[preprocess][L3] 176: Removed stop words; 71 tokens remain.
[preprocess][L3] 176: Stemming complete.
[indexDocument][L3] Preprocessing complete for '176'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '176'.
[indexDocument][L3] Recorded doc length 71 for '176'.
[indexDocument][L3] Tokens added to the inverted index for '176'.
[indexDocument][L2] Indexing document '1296'.
[indexDocument][L3] Opened file '1296'.
[preprocess][L2] Starting preprocessing for '1296'.
[preprocess][L3] 1296: Tokenized into 189 tokens.
[preprocess][L3] 1296: Removed stop words; 128 tokens remain.
[preprocess][L3] 1296: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1296'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1296'.
[indexDocument][L3] Recorded doc length 128 for '1296'.
[indexDocument][L3] Tokens added to the inverted index for '1296'.
[indexDocument][L2] Indexing document '344'.
[indexDocument][L3] Opened file '344'.
[preprocess][L2] Starting preprocessing for '344'.
[preprocess][L3] 344: Tokenized into 386 tokens.
[preprocess][L3] 344: Removed stop words; 242 tokens remain.
[preprocess][L3] 344: Stemming complete.
[indexDocument][L3] Preprocessing complete for '344'; 242 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '344'.
[indexDocument][L3] Recorded doc length 242 for '344'.
[indexDocument][L3] Tokens added to the inverted index for '344'.
[indexDocument][L2] Indexing document '1050'.
[indexDocument][L3] Opened file '1050'.
[preprocess][L2] Starting preprocessing for '1050'.
[preprocess][L3] 1050: Tokenized into 80 tokens.
[preprocess][L3] 1050: Removed stop words; 52 tokens remain.
[preprocess][L3] 1050: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1050'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1050'.
[indexDocument][L3] Recorded doc length 52 for '1050'.
[indexDocument][L3] Tokens added to the inverted index for '1050'.
[indexDocument][L2] Indexing document '182'.
[indexDocument][L3] Opened file '182'.
[preprocess][L2] Starting preprocessing for '182'.
[preprocess][L3] 182: Tokenized into 122 tokens.
[preprocess][L3] 182: Removed stop words; 74 tokens remain.
[preprocess][L3] 182: Stemming complete.
[indexDocument][L3] Preprocessing complete for '182'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '182'.
[indexDocument][L3] Recorded doc length 74 for '182'.
[indexDocument][L3] Tokens added to the inverted index for '182'.
[indexDocument][L2] Indexing document '1262'.
[indexDocument][L3] Opened file '1262'.
[preprocess][L2] Starting preprocessing for '1262'.
[preprocess][L3] 1262: Tokenized into 126 tokens.
[preprocess][L3] 1262: Removed stop words; 78 tokens remain.
[preprocess][L3] 1262: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1262'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1262'.
[indexDocument][L3] Recorded doc length 78 for '1262'.
[indexDocument][L3] Tokens added to the inverted index for '1262'.
[indexDocument][L2] Indexing document '975'.
[indexDocument][L3] Opened file '975'.
[preprocess][L2] Starting preprocessing for '975'.
[preprocess][L3] 975: Tokenized into 196 tokens.
[preprocess][L3] 975: Removed stop words; 114 tokens remain.
[preprocess][L3] 975: Stemming complete.
[indexDocument][L3] Preprocessing complete for '975'; 114 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '975'.
[indexDocument][L3] Recorded doc length 114 for '975'.
[indexDocument][L3] Tokens added to the inverted index for '975'.
[indexDocument][L2] Indexing document '149'.
[indexDocument][L3] Opened file '149'.
[preprocess][L2] Starting preprocessing for '149'.
[preprocess][L3] 149: Tokenized into 284 tokens.
[preprocess][L3] 149: Removed stop words; 153 tokens remain.
[preprocess][L3] 149: Stemming complete.
[indexDocument][L3] Preprocessing complete for '149'; 153 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '149'.
[indexDocument][L3] Recorded doc length 153 for '149'.
[indexDocument][L3] Tokens added to the inverted index for '149'.
[indexDocument][L2] Indexing document '981'.
[indexDocument][L3] Opened file '981'.
[preprocess][L2] Starting preprocessing for '981'.
[preprocess][L3] 981: Tokenized into 123 tokens.
[preprocess][L3] 981: Removed stop words; 79 tokens remain.
[preprocess][L3] 981: Stemming complete.
[indexDocument][L3] Preprocessing complete for '981'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '981'.
[indexDocument][L3] Recorded doc length 79 for '981'.
[indexDocument][L3] Tokens added to the inverted index for '981'.
[indexDocument][L2] Indexing document '1265'.
[indexDocument][L3] Opened file '1265'.
[preprocess][L2] Starting preprocessing for '1265'.
[preprocess][L3] 1265: Tokenized into 179 tokens.
[preprocess][L3] 1265: Removed stop words; 101 tokens remain.
[preprocess][L3] 1265: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1265'; 101 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1265'.
[indexDocument][L3] Recorded doc length 101 for '1265'.
[indexDocument][L3] Tokens added to the inverted index for '1265'.
[indexDocument][L2] Indexing document '1057'.
[indexDocument][L3] Opened file '1057'.
[preprocess][L2] Starting preprocessing for '1057'.
[preprocess][L3] 1057: Tokenized into 153 tokens.
[preprocess][L3] 1057: Removed stop words; 88 tokens remain.
[preprocess][L3] 1057: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1057'; 88 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1057'.
[indexDocument][L3] Recorded doc length 88 for '1057'.
[indexDocument][L3] Tokens added to the inverted index for '1057'.
[indexDocument][L2] Indexing document '185'.
[indexDocument][L3] Opened file '185'.
[preprocess][L2] Starting preprocessing for '185'.
[preprocess][L3] 185: Tokenized into 297 tokens.
[preprocess][L3] 185: Removed stop words; 182 tokens remain.
[preprocess][L3] 185: Stemming complete.
[indexDocument][L3] Preprocessing complete for '185'; 182 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '185'.
[indexDocument][L3] Recorded doc length 182 for '185'.
[indexDocument][L3] Tokens added to the inverted index for '185'.
[indexDocument][L2] Indexing document '1291'.
[indexDocument][L3] Opened file '1291'.
[preprocess][L2] Starting preprocessing for '1291'.
[preprocess][L3] 1291: Tokenized into 259 tokens.
[preprocess][L3] 1291: Removed stop words; 166 tokens remain.
[preprocess][L3] 1291: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1291'; 166 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1291'.
[indexDocument][L3] Recorded doc length 166 for '1291'.
[indexDocument][L3] Tokens added to the inverted index for '1291'.
[indexDocument][L2] Indexing document '343'.
[indexDocument][L3] Opened file '343'.
[preprocess][L2] Starting preprocessing for '343'.
[preprocess][L3] 343: Tokenized into 115 tokens.
[preprocess][L3] 343: Removed stop words; 75 tokens remain.
[preprocess][L3] 343: Stemming complete.
[indexDocument][L3] Preprocessing complete for '343'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '343'.
[indexDocument][L3] Recorded doc length 75 for '343'.
[indexDocument][L3] Tokens added to the inverted index for '343'.
[indexDocument][L2] Indexing document '171'.
[indexDocument][L3] Opened file '171'.
[preprocess][L2] Starting preprocessing for '171'.
[preprocess][L3] 171: Tokenized into 177 tokens.
[preprocess][L3] 171: Removed stop words; 107 tokens remain.
[preprocess][L3] 171: Stemming complete.
[indexDocument][L3] Preprocessing complete for '171'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '171'.
[indexDocument][L3] Recorded doc length 107 for '171'.
[indexDocument][L3] Tokens added to the inverted index for '171'.
[indexDocument][L2] Indexing document '515'.
[indexDocument][L3] Opened file '515'.
[preprocess][L2] Starting preprocessing for '515'.
[preprocess][L3] 515: Tokenized into 132 tokens.
[preprocess][L3] 515: Removed stop words; 79 tokens remain.
[preprocess][L3] 515: Stemming complete.
[indexDocument][L3] Preprocessing complete for '515'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '515'.
[indexDocument][L3] Recorded doc length 79 for '515'.
[indexDocument][L3] Tokens added to the inverted index for '515'.
[indexDocument][L2] Indexing document '727'.
[indexDocument][L3] Opened file '727'.
[preprocess][L2] Starting preprocessing for '727'.
[preprocess][L3] 727: Tokenized into 195 tokens.
[preprocess][L3] 727: Removed stop words; 124 tokens remain.
[preprocess][L3] 727: Stemming complete.
[indexDocument][L3] Preprocessing complete for '727'; 124 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '727'.
[indexDocument][L3] Recorded doc length 124 for '727'.
[indexDocument][L3] Tokens added to the inverted index for '727'.
[indexDocument][L2] Indexing document '986'.
[indexDocument][L3] Opened file '986'.
[preprocess][L2] Starting preprocessing for '986'.
[preprocess][L3] 986: Tokenized into 291 tokens.
[preprocess][L3] 986: Removed stop words; 187 tokens remain.
[preprocess][L3] 986: Stemming complete.
[indexDocument][L3] Preprocessing complete for '986'; 187 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '986'.
[indexDocument][L3] Recorded doc length 187 for '986'.
[indexDocument][L3] Tokens added to the inverted index for '986'.
[indexDocument][L2] Indexing document '1068'.
[indexDocument][L3] Opened file '1068'.
[preprocess][L2] Starting preprocessing for '1068'.
[preprocess][L3] 1068: Tokenized into 143 tokens.
[preprocess][L3] 1068: Removed stop words; 90 tokens remain.
[preprocess][L3] 1068: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1068'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1068'.
[indexDocument][L3] Recorded doc length 90 for '1068'.
[indexDocument][L3] Tokens added to the inverted index for '1068'.
[indexDocument][L2] Indexing document '388'.
[indexDocument][L3] Opened file '388'.
[preprocess][L2] Starting preprocessing for '388'.
[preprocess][L3] 388: Tokenized into 165 tokens.
[preprocess][L3] 388: Removed stop words; 104 tokens remain.
[preprocess][L3] 388: Stemming complete.
[indexDocument][L3] Preprocessing complete for '388'; 104 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '388'.
[indexDocument][L3] Recorded doc length 104 for '388'.
[indexDocument][L3] Tokens added to the inverted index for '388'.
[indexDocument][L2] Indexing document '718'.
[indexDocument][L3] Opened file '718'.
[preprocess][L2] Starting preprocessing for '718'.
[preprocess][L3] 718: Tokenized into 168 tokens.
[preprocess][L3] 718: Removed stop words; 94 tokens remain.
[preprocess][L3] 718: Stemming complete.
[indexDocument][L3] Preprocessing complete for '718'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '718'.
[indexDocument][L3] Recorded doc length 94 for '718'.
[indexDocument][L3] Tokens added to the inverted index for '718'.
[indexDocument][L2] Indexing document '972'.
[indexDocument][L3] Opened file '972'.
[preprocess][L2] Starting preprocessing for '972'.
[preprocess][L3] 972: Tokenized into 195 tokens.
[preprocess][L3] 972: Removed stop words; 119 tokens remain.
[preprocess][L3] 972: Stemming complete.
[indexDocument][L3] Preprocessing complete for '972'; 119 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '972'.
[indexDocument][L3] Recorded doc length 119 for '972'.
[indexDocument][L3] Tokens added to the inverted index for '972'.
[indexDocument][L2] Indexing document '381'.
[indexDocument][L3] Opened file '381'.
[preprocess][L2] Starting preprocessing for '381'.
[preprocess][L3] 381: Tokenized into 247 tokens.
[preprocess][L3] 381: Removed stop words; 144 tokens remain.
[preprocess][L3] 381: Stemming complete.
[indexDocument][L3] Preprocessing complete for '381'; 144 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '381'.
[indexDocument][L3] Recorded doc length 144 for '381'.
[indexDocument][L3] Tokens added to the inverted index for '381'.
[indexDocument][L2] Indexing document '1253'.
[indexDocument][L3] Opened file '1253'.
[preprocess][L2] Starting preprocessing for '1253'.
[preprocess][L3] 1253: Tokenized into 87 tokens.
[preprocess][L3] 1253: Removed stop words; 62 tokens remain.
[preprocess][L3] 1253: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1253'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1253'.
[indexDocument][L3] Recorded doc length 62 for '1253'.
[indexDocument][L3] Tokens added to the inverted index for '1253'.
[indexDocument][L2] Indexing document '1061'.
[indexDocument][L3] Opened file '1061'.
[preprocess][L2] Starting preprocessing for '1061'.
[preprocess][L3] 1061: Tokenized into 303 tokens.
[preprocess][L3] 1061: Removed stop words; 182 tokens remain.
[preprocess][L3] 1061: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1061'; 182 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1061'.
[indexDocument][L3] Recorded doc length 182 for '1061'.
[indexDocument][L3] Tokens added to the inverted index for '1061'.
[indexDocument][L2] Indexing document '523'.
[indexDocument][L3] Opened file '523'.
[preprocess][L2] Starting preprocessing for '523'.
[preprocess][L3] 523: Tokenized into 182 tokens.
[preprocess][L3] 523: Removed stop words; 95 tokens remain.
[preprocess][L3] 523: Stemming complete.
[indexDocument][L3] Preprocessing complete for '523'; 95 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '523'.
[indexDocument][L3] Recorded doc length 95 for '523'.
[indexDocument][L3] Tokens added to the inverted index for '523'.
[indexDocument][L2] Indexing document '711'.
[indexDocument][L3] Opened file '711'.
[preprocess][L2] Starting preprocessing for '711'.
[preprocess][L3] 711: Tokenized into 145 tokens.
[preprocess][L3] 711: Removed stop words; 91 tokens remain.
[preprocess][L3] 711: Stemming complete.
[indexDocument][L3] Preprocessing complete for '711'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '711'.
[indexDocument][L3] Recorded doc length 91 for '711'.
[indexDocument][L3] Tokens added to the inverted index for '711'.
[indexDocument][L2] Indexing document '375'.
[indexDocument][L3] Opened file '375'.
[preprocess][L2] Starting preprocessing for '375'.
[preprocess][L3] 375: Tokenized into 257 tokens.
[preprocess][L3] 375: Removed stop words; 143 tokens remain.
[preprocess][L3] 375: Stemming complete.
[indexDocument][L3] Preprocessing complete for '375'; 143 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '375'.
[indexDocument][L3] Recorded doc length 143 for '375'.
[indexDocument][L3] Tokens added to the inverted index for '375'.
[indexDocument][L2] Indexing document '147'.
[indexDocument][L3] Opened file '147'.
[preprocess][L2] Starting preprocessing for '147'.
[preprocess][L3] 147: Tokenized into 167 tokens.
[preprocess][L3] 147: Removed stop words; 108 tokens remain.
[preprocess][L3] 147: Stemming complete.
[indexDocument][L3] Preprocessing complete for '147'; 108 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '147'.
[indexDocument][L3] Recorded doc length 108 for '147'.
[indexDocument][L3] Tokens added to the inverted index for '147'.
[indexDocument][L2] Indexing document '1095'.
[indexDocument][L3] Opened file '1095'.
[preprocess][L2] Starting preprocessing for '1095'.
[preprocess][L3] 1095: Tokenized into 207 tokens.
[preprocess][L3] 1095: Removed stop words; 120 tokens remain.
[preprocess][L3] 1095: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1095'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1095'.
[indexDocument][L3] Recorded doc length 120 for '1095'.
[indexDocument][L3] Tokens added to the inverted index for '1095'.
[indexDocument][L2] Indexing document '944'.
[indexDocument][L3] Opened file '944'.
[preprocess][L2] Starting preprocessing for '944'.
[preprocess][L3] 944: Tokenized into 129 tokens.
[preprocess][L3] 944: Removed stop words; 76 tokens remain.
[preprocess][L3] 944: Stemming complete.
[indexDocument][L3] Preprocessing complete for '944'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '944'.
[indexDocument][L3] Recorded doc length 76 for '944'.
[indexDocument][L3] Tokens added to the inverted index for '944'.
[indexDocument][L2] Indexing document '178'.
[indexDocument][L3] Opened file '178'.
[preprocess][L2] Starting preprocessing for '178'.
[preprocess][L3] 178: Tokenized into 81 tokens.
[preprocess][L3] 178: Removed stop words; 47 tokens remain.
[preprocess][L3] 178: Stemming complete.
[indexDocument][L3] Preprocessing complete for '178'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '178'.
[indexDocument][L3] Recorded doc length 47 for '178'.
[indexDocument][L3] Tokens added to the inverted index for '178'.
[indexDocument][L2] Indexing document '1298'.
[indexDocument][L3] Opened file '1298'.
[preprocess][L2] Starting preprocessing for '1298'.
[preprocess][L3] 1298: Tokenized into 126 tokens.
[preprocess][L3] 1298: Removed stop words; 80 tokens remain.
[preprocess][L3] 1298: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1298'; 80 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1298'.
[indexDocument][L3] Recorded doc length 80 for '1298'.
[indexDocument][L3] Tokens added to the inverted index for '1298'.
[indexDocument][L2] Indexing document '140'.
[indexDocument][L3] Opened file '140'.
[preprocess][L2] Starting preprocessing for '140'.
[preprocess][L3] 140: Tokenized into 363 tokens.
[preprocess][L3] 140: Removed stop words; 202 tokens remain.
[preprocess][L3] 140: Stemming complete.
[indexDocument][L3] Preprocessing complete for '140'; 202 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '140'.
[indexDocument][L3] Recorded doc length 202 for '140'.
[indexDocument][L3] Tokens added to the inverted index for '140'.
[indexDocument][L2] Indexing document '1092'.
[indexDocument][L3] Opened file '1092'.
[preprocess][L2] Starting preprocessing for '1092'.
[preprocess][L3] 1092: Tokenized into 280 tokens.
[preprocess][L3] 1092: Removed stop words; 168 tokens remain.
[preprocess][L3] 1092: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1092'; 168 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1092'.
[indexDocument][L3] Recorded doc length 168 for '1092'.
[indexDocument][L3] Tokens added to the inverted index for '1092'.
[indexDocument][L2] Indexing document '372'.
[indexDocument][L3] Opened file '372'.
[preprocess][L2] Starting preprocessing for '372'.
[preprocess][L3] 372: Tokenized into 113 tokens.
[preprocess][L3] 372: Removed stop words; 71 tokens remain.
[preprocess][L3] 372: Stemming complete.
[indexDocument][L3] Preprocessing complete for '372'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '372'.
[indexDocument][L3] Recorded doc length 71 for '372'.
[indexDocument][L3] Tokens added to the inverted index for '372'.
[indexDocument][L2] Indexing document '716'.
[indexDocument][L3] Opened file '716'.
[preprocess][L2] Starting preprocessing for '716'.
[preprocess][L3] 716: Tokenized into 104 tokens.
[preprocess][L3] 716: Removed stop words; 60 tokens remain.
[preprocess][L3] 716: Stemming complete.
[indexDocument][L3] Preprocessing complete for '716'; 60 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '716'.
[indexDocument][L3] Recorded doc length 60 for '716'.
[indexDocument][L3] Tokens added to the inverted index for '716'.
[indexDocument][L2] Indexing document '524'.
[indexDocument][L3] Opened file '524'.
[preprocess][L2] Starting preprocessing for '524'.
[preprocess][L3] 524: Tokenized into 75 tokens.
[preprocess][L3] 524: Removed stop words; 53 tokens remain.
[preprocess][L3] 524: Stemming complete.
[indexDocument][L3] Preprocessing complete for '524'; 53 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '524'.
[indexDocument][L3] Recorded doc length 53 for '524'.
[indexDocument][L3] Tokens added to the inverted index for '524'.
[indexDocument][L2] Indexing document '988'.
[indexDocument][L3] Opened file '988'.
[preprocess][L2] Starting preprocessing for '988'.
[preprocess][L3] 988: Tokenized into 96 tokens.
[preprocess][L3] 988: Removed stop words; 60 tokens remain.
[preprocess][L3] 988: Stemming complete.
[indexDocument][L3] Preprocessing complete for '988'; 60 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '988'.
[indexDocument][L3] Recorded doc length 60 for '988'.
[indexDocument][L3] Tokens added to the inverted index for '988'.
[indexDocument][L2] Indexing document '1066'.
[indexDocument][L3] Opened file '1066'.
[preprocess][L2] Starting preprocessing for '1066'.
[preprocess][L3] 1066: Tokenized into 350 tokens.
[preprocess][L3] 1066: Removed stop words; 216 tokens remain.
[preprocess][L3] 1066: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1066'; 216 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1066'.
[indexDocument][L3] Recorded doc length 216 for '1066'.
[indexDocument][L3] Tokens added to the inverted index for '1066'.
[indexDocument][L2] Indexing document '386'.
[indexDocument][L3] Opened file '386'.
[preprocess][L2] Starting preprocessing for '386'.
[preprocess][L3] 386: Tokenized into 90 tokens.
[preprocess][L3] 386: Removed stop words; 59 tokens remain.
[preprocess][L3] 386: Stemming complete.
[indexDocument][L3] Preprocessing complete for '386'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '386'.
[indexDocument][L3] Recorded doc length 59 for '386'.
[indexDocument][L3] Tokens added to the inverted index for '386'.
[indexDocument][L2] Indexing document '1254'.
[indexDocument][L3] Opened file '1254'.
[preprocess][L2] Starting preprocessing for '1254'.
[preprocess][L3] 1254: Tokenized into 131 tokens.
[preprocess][L3] 1254: Removed stop words; 78 tokens remain.
[preprocess][L3] 1254: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1254'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1254'.
[indexDocument][L3] Recorded doc length 78 for '1254'.
[indexDocument][L3] Tokens added to the inverted index for '1254'.
[indexDocument][L2] Indexing document '729'.
[indexDocument][L3] Opened file '729'.
[preprocess][L2] Starting preprocessing for '729'.
[preprocess][L3] 729: Tokenized into 152 tokens.
[preprocess][L3] 729: Removed stop words; 103 tokens remain.
[preprocess][L3] 729: Stemming complete.
[indexDocument][L3] Preprocessing complete for '729'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '729'.
[indexDocument][L3] Recorded doc length 103 for '729'.
[indexDocument][L3] Tokens added to the inverted index for '729'.
[indexDocument][L2] Indexing document '943'.
[indexDocument][L3] Opened file '943'.
[preprocess][L2] Starting preprocessing for '943'.
[preprocess][L3] 943: Tokenized into 159 tokens.
[preprocess][L3] 943: Removed stop words; 100 tokens remain.
[preprocess][L3] 943: Stemming complete.
[indexDocument][L3] Preprocessing complete for '943'; 100 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '943'.
[indexDocument][L3] Recorded doc length 100 for '943'.
[indexDocument][L3] Tokens added to the inverted index for '943'.
[indexDocument][L2] Indexing document '1059'.
[indexDocument][L3] Opened file '1059'.
[preprocess][L2] Starting preprocessing for '1059'.
[preprocess][L3] 1059: Tokenized into 218 tokens.
[preprocess][L3] 1059: Removed stop words; 132 tokens remain.
[preprocess][L3] 1059: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1059'; 132 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1059'.
[indexDocument][L3] Recorded doc length 132 for '1059'.
[indexDocument][L3] Tokens added to the inverted index for '1059'.
[indexDocument][L2] Indexing document '917'.
[indexDocument][L3] Opened file '917'.
[preprocess][L2] Starting preprocessing for '917'.
[preprocess][L3] 917: Tokenized into 331 tokens.
[preprocess][L3] 917: Removed stop words; 184 tokens remain.
[preprocess][L3] 917: Stemming complete.
[indexDocument][L3] Preprocessing complete for '917'; 184 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '917'.
[indexDocument][L3] Recorded doc length 184 for '917'.
[indexDocument][L3] Tokens added to the inverted index for '917'.
[indexDocument][L2] Indexing document '319'.
[indexDocument][L3] Opened file '319'.
[preprocess][L2] Starting preprocessing for '319'.
[preprocess][L3] 319: Tokenized into 127 tokens.
[preprocess][L3] 319: Removed stop words; 77 tokens remain.
[preprocess][L3] 319: Stemming complete.
[indexDocument][L3] Preprocessing complete for '319'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '319'.
[indexDocument][L3] Recorded doc length 77 for '319'.
[indexDocument][L3] Tokens added to the inverted index for '319'.
[indexDocument][L2] Indexing document '789'.
[indexDocument][L3] Opened file '789'.
[preprocess][L2] Starting preprocessing for '789'.
[preprocess][L3] 789: Tokenized into 106 tokens.
[preprocess][L3] 789: Removed stop words; 67 tokens remain.
[preprocess][L3] 789: Stemming complete.
[indexDocument][L3] Preprocessing complete for '789'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '789'.
[indexDocument][L3] Recorded doc length 67 for '789'.
[indexDocument][L3] Tokens added to the inverted index for '789'.
[indexDocument][L2] Indexing document '326'.
[indexDocument][L3] Opened file '326'.
[preprocess][L2] Starting preprocessing for '326'.
[preprocess][L3] 326: Tokenized into 55 tokens.
[preprocess][L3] 326: Removed stop words; 38 tokens remain.
[preprocess][L3] 326: Stemming complete.
[indexDocument][L3] Preprocessing complete for '326'; 38 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '326'.
[indexDocument][L3] Recorded doc length 38 for '326'.
[indexDocument][L3] Tokens added to the inverted index for '326'.
[indexDocument][L2] Indexing document '114'.
[indexDocument][L3] Opened file '114'.
[preprocess][L2] Starting preprocessing for '114'.
[preprocess][L3] 114: Tokenized into 198 tokens.
[preprocess][L3] 114: Removed stop words; 108 tokens remain.
[preprocess][L3] 114: Stemming complete.
[indexDocument][L3] Preprocessing complete for '114'; 108 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '114'.
[indexDocument][L3] Recorded doc length 108 for '114'.
[indexDocument][L3] Tokens added to the inverted index for '114'.
[indexDocument][L2] Indexing document '928'.
[indexDocument][L3] Opened file '928'.
[preprocess][L2] Starting preprocessing for '928'.
[preprocess][L3] 928: Tokenized into 434 tokens.
[preprocess][L3] 928: Removed stop words; 237 tokens remain.
[preprocess][L3] 928: Stemming complete.
[indexDocument][L3] Preprocessing complete for '928'; 237 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '928'.
[indexDocument][L3] Recorded doc length 237 for '928'.
[indexDocument][L3] Tokens added to the inverted index for '928'.
[indexDocument][L2] Indexing document '570'.
[indexDocument][L3] Opened file '570'.
[preprocess][L2] Starting preprocessing for '570'.
[preprocess][L3] 570: Tokenized into 94 tokens.
[preprocess][L3] 570: Removed stop words; 60 tokens remain.
[preprocess][L3] 570: Stemming complete.
[indexDocument][L3] Preprocessing complete for '570'; 60 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '570'.
[indexDocument][L3] Recorded doc length 60 for '570'.
[indexDocument][L3] Tokens added to the inverted index for '570'.
[indexDocument][L2] Indexing document '742'.
[indexDocument][L3] Opened file '742'.
[preprocess][L2] Starting preprocessing for '742'.
[preprocess][L3] 742: Tokenized into 110 tokens.
[preprocess][L3] 742: Removed stop words; 71 tokens remain.
[preprocess][L3] 742: Stemming complete.
[indexDocument][L3] Preprocessing complete for '742'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '742'.
[indexDocument][L3] Recorded doc length 71 for '742'.
[indexDocument][L3] Tokens added to the inverted index for '742'.
[indexDocument][L2] Indexing document '584'.
[indexDocument][L3] Opened file '584'.
[preprocess][L2] Starting preprocessing for '584'.
[preprocess][L3] 584: Tokenized into 142 tokens.
[preprocess][L3] 584: Removed stop words; 79 tokens remain.
[preprocess][L3] 584: Stemming complete.
[indexDocument][L3] Preprocessing complete for '584'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '584'.
[indexDocument][L3] Recorded doc length 79 for '584'.
[indexDocument][L3] Tokens added to the inverted index for '584'.
[indexDocument][L2] Indexing document '1200'.
[indexDocument][L3] Opened file '1200'.
[preprocess][L2] Starting preprocessing for '1200'.
[preprocess][L3] 1200: Tokenized into 165 tokens.
[preprocess][L3] 1200: Removed stop words; 101 tokens remain.
[preprocess][L3] 1200: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1200'; 101 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1200'.
[indexDocument][L3] Recorded doc length 101 for '1200'.
[indexDocument][L3] Tokens added to the inverted index for '1200'.
[indexDocument][L2] Indexing document '1032'.
[indexDocument][L3] Opened file '1032'.
[preprocess][L2] Starting preprocessing for '1032'.
[preprocess][L3] 1032: Tokenized into 117 tokens.
[preprocess][L3] 1032: Removed stop words; 73 tokens remain.
[preprocess][L3] 1032: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1032'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1032'.
[indexDocument][L3] Recorded doc length 73 for '1032'.
[indexDocument][L3] Tokens added to the inverted index for '1032'.
[indexDocument][L2] Indexing document '1238'.
[indexDocument][L3] Opened file '1238'.
[preprocess][L2] Starting preprocessing for '1238'.
[preprocess][L3] 1238: Tokenized into 172 tokens.
[preprocess][L3] 1238: Removed stop words; 96 tokens remain.
[preprocess][L3] 1238: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1238'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1238'.
[indexDocument][L3] Recorded doc length 96 for '1238'.
[indexDocument][L3] Tokens added to the inverted index for '1238'.
[indexDocument][L2] Indexing document '910'.
[indexDocument][L3] Opened file '910'.
[preprocess][L2] Starting preprocessing for '910'.
[preprocess][L3] 910: Tokenized into 56 tokens.
[preprocess][L3] 910: Removed stop words; 39 tokens remain.
[preprocess][L3] 910: Stemming complete.
[indexDocument][L3] Preprocessing complete for '910'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '910'.
[indexDocument][L3] Recorded doc length 39 for '910'.
[indexDocument][L3] Tokens added to the inverted index for '910'.
[indexDocument][L2] Indexing document '548'.
[indexDocument][L3] Opened file '548'.
[preprocess][L2] Starting preprocessing for '548'.
[preprocess][L3] 548: Tokenized into 329 tokens.
[preprocess][L3] 548: Removed stop words; 187 tokens remain.
[preprocess][L3] 548: Stemming complete.
[indexDocument][L3] Preprocessing complete for '548'; 187 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '548'.
[indexDocument][L3] Recorded doc length 187 for '548'.
[indexDocument][L3] Tokens added to the inverted index for '548'.
[indexDocument][L2] Indexing document '1035'.
[indexDocument][L3] Opened file '1035'.
[preprocess][L2] Starting preprocessing for '1035'.
[preprocess][L3] 1035: Tokenized into 273 tokens.
[preprocess][L3] 1035: Removed stop words; 133 tokens remain.
[preprocess][L3] 1035: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1035'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1035'.
[indexDocument][L3] Recorded doc length 133 for '1035'.
[indexDocument][L3] Tokens added to the inverted index for '1035'.
[indexDocument][L2] Indexing document '1207'.
[indexDocument][L3] Opened file '1207'.
[preprocess][L2] Starting preprocessing for '1207'.
[preprocess][L3] 1207: Tokenized into 180 tokens.
[preprocess][L3] 1207: Removed stop words; 107 tokens remain.
[preprocess][L3] 1207: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1207'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1207'.
[indexDocument][L3] Recorded doc length 107 for '1207'.
[indexDocument][L3] Tokens added to the inverted index for '1207'.
[indexDocument][L2] Indexing document '583'.
[indexDocument][L3] Opened file '583'.
[preprocess][L2] Starting preprocessing for '583'.
[preprocess][L3] 583: Tokenized into 147 tokens.
[preprocess][L3] 583: Removed stop words; 89 tokens remain.
[preprocess][L3] 583: Stemming complete.
[indexDocument][L3] Preprocessing complete for '583'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '583'.
[indexDocument][L3] Recorded doc length 89 for '583'.
[indexDocument][L3] Tokens added to the inverted index for '583'.
[indexDocument][L2] Indexing document '745'.
[indexDocument][L3] Opened file '745'.
[preprocess][L2] Starting preprocessing for '745'.
[preprocess][L3] 745: Tokenized into 73 tokens.
[preprocess][L3] 745: Removed stop words; 39 tokens remain.
[preprocess][L3] 745: Stemming complete.
[indexDocument][L3] Preprocessing complete for '745'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '745'.
[indexDocument][L3] Recorded doc length 39 for '745'.
[indexDocument][L3] Tokens added to the inverted index for '745'.
[indexDocument][L2] Indexing document '577'.
[indexDocument][L3] Opened file '577'.
[preprocess][L2] Starting preprocessing for '577'.
[preprocess][L3] 577: Tokenized into 91 tokens.
[preprocess][L3] 577: Removed stop words; 58 tokens remain.
[preprocess][L3] 577: Stemming complete.
[indexDocument][L3] Preprocessing complete for '577'; 58 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '577'.
[indexDocument][L3] Recorded doc length 58 for '577'.
[indexDocument][L3] Tokens added to the inverted index for '577'.
[indexDocument][L2] Indexing document '113'.
[indexDocument][L3] Opened file '113'.
[preprocess][L2] Starting preprocessing for '113'.
[preprocess][L3] 113: Tokenized into 119 tokens.
[preprocess][L3] 113: Removed stop words; 74 tokens remain.
[preprocess][L3] 113: Stemming complete.
[indexDocument][L3] Preprocessing complete for '113'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '113'.
[indexDocument][L3] Recorded doc length 74 for '113'.
[indexDocument][L3] Tokens added to the inverted index for '113'.
[indexDocument][L2] Indexing document '321'.
[indexDocument][L3] Opened file '321'.
[preprocess][L2] Starting preprocessing for '321'.
[preprocess][L3] 321: Tokenized into 104 tokens.
[preprocess][L3] 321: Removed stop words; 71 tokens remain.
[preprocess][L3] 321: Stemming complete.
[indexDocument][L3] Preprocessing complete for '321'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '321'.
[indexDocument][L3] Recorded doc length 71 for '321'.
[indexDocument][L3] Tokens added to the inverted index for '321'.
[indexDocument][L2] Indexing document '328'.
[indexDocument][L3] Opened file '328'.
[preprocess][L2] Starting preprocessing for '328'.
[preprocess][L3] 328: Tokenized into 276 tokens.
[preprocess][L3] 328: Removed stop words; 179 tokens remain.
[preprocess][L3] 328: Stemming complete.
[indexDocument][L3] Preprocessing complete for '328'; 179 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '328'.
[indexDocument][L3] Recorded doc length 179 for '328'.
[indexDocument][L3] Tokens added to the inverted index for '328'.
[indexDocument][L2] Indexing document '926'.
[indexDocument][L3] Opened file '926'.
[preprocess][L2] Starting preprocessing for '926'.
[preprocess][L3] 926: Tokenized into 135 tokens.
[preprocess][L3] 926: Removed stop words; 87 tokens remain.
[preprocess][L3] 926: Stemming complete.
[indexDocument][L3] Preprocessing complete for '926'; 87 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '926'.
[indexDocument][L3] Recorded doc length 87 for '926'.
[indexDocument][L3] Tokens added to the inverted index for '926'.
[indexDocument][L2] Indexing document '787'.
[indexDocument][L3] Opened file '787'.
[preprocess][L2] Starting preprocessing for '787'.
[preprocess][L3] 787: Tokenized into 185 tokens.
[preprocess][L3] 787: Removed stop words; 104 tokens remain.
[preprocess][L3] 787: Stemming complete.
[indexDocument][L3] Preprocessing complete for '787'; 104 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '787'.
[indexDocument][L3] Recorded doc length 104 for '787'.
[indexDocument][L3] Tokens added to the inverted index for '787'.
[indexDocument][L2] Indexing document '1003'.
[indexDocument][L3] Opened file '1003'.
[preprocess][L2] Starting preprocessing for '1003'.
[preprocess][L3] 1003: Tokenized into 152 tokens.
[preprocess][L3] 1003: Removed stop words; 89 tokens remain.
[preprocess][L3] 1003: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1003'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1003'.
[indexDocument][L3] Recorded doc length 89 for '1003'.
[indexDocument][L3] Tokens added to the inverted index for '1003'.
[indexDocument][L2] Indexing document '1231'.
[indexDocument][L3] Opened file '1231'.
[preprocess][L2] Starting preprocessing for '1231'.
[preprocess][L3] 1231: Tokenized into 184 tokens.
[preprocess][L3] 1231: Removed stop words; 111 tokens remain.
[preprocess][L3] 1231: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1231'; 111 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1231'.
[indexDocument][L3] Recorded doc length 111 for '1231'.
[indexDocument][L3] Tokens added to the inverted index for '1231'.
[indexDocument][L2] Indexing document '125'.
[indexDocument][L3] Opened file '125'.
[preprocess][L2] Starting preprocessing for '125'.
[preprocess][L3] 125: Tokenized into 224 tokens.
[preprocess][L3] 125: Removed stop words; 138 tokens remain.
[preprocess][L3] 125: Stemming complete.
[indexDocument][L3] Preprocessing complete for '125'; 138 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '125'.
[indexDocument][L3] Recorded doc length 138 for '125'.
[indexDocument][L3] Tokens added to the inverted index for '125'.
[indexDocument][L2] Indexing document '919'.
[indexDocument][L3] Opened file '919'.
[preprocess][L2] Starting preprocessing for '919'.
[preprocess][L3] 919: Tokenized into 155 tokens.
[preprocess][L3] 919: Removed stop words; 107 tokens remain.
[preprocess][L3] 919: Stemming complete.
[indexDocument][L3] Preprocessing complete for '919'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '919'.
[indexDocument][L3] Recorded doc length 107 for '919'.
[indexDocument][L3] Tokens added to the inverted index for '919'.
[indexDocument][L2] Indexing document '317'.
[indexDocument][L3] Opened file '317'.
[preprocess][L2] Starting preprocessing for '317'.
[preprocess][L3] 317: Tokenized into 228 tokens.
[preprocess][L3] 317: Removed stop words; 135 tokens remain.
[preprocess][L3] 317: Stemming complete.
[indexDocument][L3] Preprocessing complete for '317'; 135 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '317'.
[indexDocument][L3] Recorded doc length 135 for '317'.
[indexDocument][L3] Tokens added to the inverted index for '317'.
[indexDocument][L2] Indexing document '773'.
[indexDocument][L3] Opened file '773'.
[preprocess][L2] Starting preprocessing for '773'.
[preprocess][L3] 773: Tokenized into 282 tokens.
[preprocess][L3] 773: Removed stop words; 183 tokens remain.
[preprocess][L3] 773: Stemming complete.
[indexDocument][L3] Preprocessing complete for '773'; 183 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '773'.
[indexDocument][L3] Recorded doc length 183 for '773'.
[indexDocument][L3] Tokens added to the inverted index for '773'.
[indexDocument][L2] Indexing document '541'.
[indexDocument][L3] Opened file '541'.
[preprocess][L2] Starting preprocessing for '541'.
[preprocess][L3] 541: Tokenized into 197 tokens.
[preprocess][L3] 541: Removed stop words; 130 tokens remain.
[preprocess][L3] 541: Stemming complete.
[indexDocument][L3] Preprocessing complete for '541'; 130 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '541'.
[indexDocument][L3] Recorded doc length 130 for '541'.
[indexDocument][L3] Tokens added to the inverted index for '541'.
[indexDocument][L2] Indexing document '921'.
[indexDocument][L3] Opened file '921'.
[preprocess][L2] Starting preprocessing for '921'.
[preprocess][L3] 921: Tokenized into 191 tokens.
[preprocess][L3] 921: Removed stop words; 115 tokens remain.
[preprocess][L3] 921: Stemming complete.
[indexDocument][L3] Preprocessing complete for '921'; 115 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '921'.
[indexDocument][L3] Recorded doc length 115 for '921'.
[indexDocument][L3] Tokens added to the inverted index for '921'.
[indexDocument][L2] Indexing document '579'.
[indexDocument][L3] Opened file '579'.
[preprocess][L2] Starting preprocessing for '579'.
[preprocess][L3] 579: Tokenized into 196 tokens.
[preprocess][L3] 579: Removed stop words; 112 tokens remain.
[preprocess][L3] 579: Stemming complete.
[indexDocument][L3] Preprocessing complete for '579'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '579'.
[indexDocument][L3] Recorded doc length 112 for '579'.
[indexDocument][L3] Tokens added to the inverted index for '579'.
[indexDocument][L2] Indexing document '1209'.
[indexDocument][L3] Opened file '1209'.
[preprocess][L2] Starting preprocessing for '1209'.
[preprocess][L3] 1209: Tokenized into 276 tokens.
[preprocess][L3] 1209: Removed stop words; 156 tokens remain.
[preprocess][L3] 1209: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1209'; 156 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1209'.
[indexDocument][L3] Recorded doc length 156 for '1209'.
[indexDocument][L3] Tokens added to the inverted index for '1209'.
[indexDocument][L2] Indexing document '546'.
[indexDocument][L3] Opened file '546'.
[preprocess][L2] Starting preprocessing for '546'.
[preprocess][L3] 546: Tokenized into 140 tokens.
[preprocess][L3] 546: Removed stop words; 90 tokens remain.
[preprocess][L3] 546: Stemming complete.
[indexDocument][L3] Preprocessing complete for '546'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '546'.
[indexDocument][L3] Recorded doc length 90 for '546'.
[indexDocument][L3] Tokens added to the inverted index for '546'.
[indexDocument][L2] Indexing document '774'.
[indexDocument][L3] Opened file '774'.
[preprocess][L2] Starting preprocessing for '774'.
[preprocess][L3] 774: Tokenized into 63 tokens.
[preprocess][L3] 774: Removed stop words; 37 tokens remain.
[preprocess][L3] 774: Stemming complete.
[indexDocument][L3] Preprocessing complete for '774'; 37 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '774'.
[indexDocument][L3] Recorded doc length 37 for '774'.
[indexDocument][L3] Tokens added to the inverted index for '774'.
[indexDocument][L2] Indexing document '310'.
[indexDocument][L3] Opened file '310'.
[preprocess][L2] Starting preprocessing for '310'.
[preprocess][L3] 310: Tokenized into 167 tokens.
[preprocess][L3] 310: Removed stop words; 113 tokens remain.
[preprocess][L3] 310: Stemming complete.
[indexDocument][L3] Preprocessing complete for '310'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '310'.
[indexDocument][L3] Recorded doc length 113 for '310'.
[indexDocument][L3] Tokens added to the inverted index for '310'.
[indexDocument][L2] Indexing document '122'.
[indexDocument][L3] Opened file '122'.
[preprocess][L2] Starting preprocessing for '122'.
[preprocess][L3] 122: Tokenized into 229 tokens.
[preprocess][L3] 122: Removed stop words; 137 tokens remain.
[preprocess][L3] 122: Stemming complete.
[indexDocument][L3] Preprocessing complete for '122'; 137 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '122'.
[indexDocument][L3] Recorded doc length 137 for '122'.
[indexDocument][L3] Tokens added to the inverted index for '122'.
[indexDocument][L2] Indexing document '1236'.
[indexDocument][L3] Opened file '1236'.
[preprocess][L2] Starting preprocessing for '1236'.
[preprocess][L3] 1236: Tokenized into 94 tokens.
[preprocess][L3] 1236: Removed stop words; 57 tokens remain.
[preprocess][L3] 1236: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1236'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1236'.
[indexDocument][L3] Recorded doc length 57 for '1236'.
[indexDocument][L3] Tokens added to the inverted index for '1236'.
[indexDocument][L2] Indexing document '1004'.
[indexDocument][L3] Opened file '1004'.
[preprocess][L2] Starting preprocessing for '1004'.
[preprocess][L3] 1004: Tokenized into 135 tokens.
[preprocess][L3] 1004: Removed stop words; 82 tokens remain.
[preprocess][L3] 1004: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1004'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1004'.
[indexDocument][L3] Recorded doc length 82 for '1004'.
[indexDocument][L3] Tokens added to the inverted index for '1004'.
[indexDocument][L2] Indexing document '780'.
[indexDocument][L3] Opened file '780'.
[preprocess][L2] Starting preprocessing for '780'.
[preprocess][L3] 780: Tokenized into 120 tokens.
[preprocess][L3] 780: Removed stop words; 77 tokens remain.
[preprocess][L3] 780: Stemming complete.
[indexDocument][L3] Preprocessing complete for '780'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '780'.
[indexDocument][L3] Recorded doc length 77 for '780'.
[indexDocument][L3] Tokens added to the inverted index for '780'.
[indexDocument][L2] Indexing document '387'.
[indexDocument][L3] Opened file '387'.
[preprocess][L2] Starting preprocessing for '387'.
[preprocess][L3] 387: Tokenized into 68 tokens.
[preprocess][L3] 387: Removed stop words; 46 tokens remain.
[preprocess][L3] 387: Stemming complete.
[indexDocument][L3] Preprocessing complete for '387'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '387'.
[indexDocument][L3] Recorded doc length 46 for '387'.
[indexDocument][L3] Tokens added to the inverted index for '387'.
[indexDocument][L2] Indexing document '1255'.
[indexDocument][L3] Opened file '1255'.
[preprocess][L2] Starting preprocessing for '1255'.
[preprocess][L3] 1255: Tokenized into 179 tokens.
[preprocess][L3] 1255: Removed stop words; 104 tokens remain.
[preprocess][L3] 1255: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1255'; 104 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1255'.
[indexDocument][L3] Recorded doc length 104 for '1255'.
[indexDocument][L3] Tokens added to the inverted index for '1255'.
[indexDocument][L2] Indexing document '1067'.
[indexDocument][L3] Opened file '1067'.
[preprocess][L2] Starting preprocessing for '1067'.
[preprocess][L3] 1067: Tokenized into 64 tokens.
[preprocess][L3] 1067: Removed stop words; 49 tokens remain.
[preprocess][L3] 1067: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1067'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1067'.
[indexDocument][L3] Recorded doc length 49 for '1067'.
[indexDocument][L3] Tokens added to the inverted index for '1067'.
[indexDocument][L2] Indexing document '989'.
[indexDocument][L3] Opened file '989'.
[preprocess][L2] Starting preprocessing for '989'.
[preprocess][L3] 989: Tokenized into 205 tokens.
[preprocess][L3] 989: Removed stop words; 120 tokens remain.
[preprocess][L3] 989: Stemming complete.
[indexDocument][L3] Preprocessing complete for '989'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '989'.
[indexDocument][L3] Recorded doc length 120 for '989'.
[indexDocument][L3] Tokens added to the inverted index for '989'.
[indexDocument][L2] Indexing document '373'.
[indexDocument][L3] Opened file '373'.
[preprocess][L2] Starting preprocessing for '373'.
[preprocess][L3] 373: Tokenized into 333 tokens.
[preprocess][L3] 373: Removed stop words; 189 tokens remain.
[preprocess][L3] 373: Stemming complete.
[indexDocument][L3] Preprocessing complete for '373'; 189 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '373'.
[indexDocument][L3] Recorded doc length 189 for '373'.
[indexDocument][L3] Tokens added to the inverted index for '373'.
[indexDocument][L2] Indexing document '141'.
[indexDocument][L3] Opened file '141'.
[preprocess][L2] Starting preprocessing for '141'.
[preprocess][L3] 141: Tokenized into 93 tokens.
[preprocess][L3] 141: Removed stop words; 68 tokens remain.
[preprocess][L3] 141: Stemming complete.
[indexDocument][L3] Preprocessing complete for '141'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '141'.
[indexDocument][L3] Recorded doc length 68 for '141'.
[indexDocument][L3] Tokens added to the inverted index for '141'.
[indexDocument][L2] Indexing document '1093'.
[indexDocument][L3] Opened file '1093'.
[preprocess][L2] Starting preprocessing for '1093'.
[preprocess][L3] 1093: Tokenized into 105 tokens.
[preprocess][L3] 1093: Removed stop words; 65 tokens remain.
[preprocess][L3] 1093: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1093'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1093'.
[indexDocument][L3] Recorded doc length 65 for '1093'.
[indexDocument][L3] Tokens added to the inverted index for '1093'.
[indexDocument][L2] Indexing document '525'.
[indexDocument][L3] Opened file '525'.
[preprocess][L2] Starting preprocessing for '525'.
[preprocess][L3] 525: Tokenized into 118 tokens.
[preprocess][L3] 525: Removed stop words; 72 tokens remain.
[preprocess][L3] 525: Stemming complete.
[indexDocument][L3] Preprocessing complete for '525'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '525'.
[indexDocument][L3] Recorded doc length 72 for '525'.
[indexDocument][L3] Tokens added to the inverted index for '525'.
[indexDocument][L2] Indexing document '717'.
[indexDocument][L3] Opened file '717'.
[preprocess][L2] Starting preprocessing for '717'.
[preprocess][L3] 717: Tokenized into 339 tokens.
[preprocess][L3] 717: Removed stop words; 206 tokens remain.
[preprocess][L3] 717: Stemming complete.
[indexDocument][L3] Preprocessing complete for '717'; 206 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '717'.
[indexDocument][L3] Recorded doc length 206 for '717'.
[indexDocument][L3] Tokens added to the inverted index for '717'.
[indexDocument][L2] Indexing document '1058'.
[indexDocument][L3] Opened file '1058'.
[preprocess][L2] Starting preprocessing for '1058'.
[preprocess][L3] 1058: Tokenized into 97 tokens.
[preprocess][L3] 1058: Removed stop words; 63 tokens remain.
[preprocess][L3] 1058: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1058'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1058'.
[indexDocument][L3] Recorded doc length 63 for '1058'.
[indexDocument][L3] Tokens added to the inverted index for '1058'.
[indexDocument][L2] Indexing document '728'.
[indexDocument][L3] Opened file '728'.
[preprocess][L2] Starting preprocessing for '728'.
[preprocess][L3] 728: Tokenized into 66 tokens.
[preprocess][L3] 728: Removed stop words; 46 tokens remain.
[preprocess][L3] 728: Stemming complete.
[indexDocument][L3] Preprocessing complete for '728'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '728'.
[indexDocument][L3] Recorded doc length 46 for '728'.
[indexDocument][L3] Tokens added to the inverted index for '728'.
[indexDocument][L2] Indexing document '942'.
[indexDocument][L3] Opened file '942'.
[preprocess][L2] Starting preprocessing for '942'.
[preprocess][L3] 942: Tokenized into 142 tokens.
[preprocess][L3] 942: Removed stop words; 89 tokens remain.
[preprocess][L3] 942: Stemming complete.
[indexDocument][L3] Preprocessing complete for '942'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '942'.
[indexDocument][L3] Recorded doc length 89 for '942'.
[indexDocument][L3] Tokens added to the inverted index for '942'.
[indexDocument][L2] Indexing document '710'.
[indexDocument][L3] Opened file '710'.
[preprocess][L2] Starting preprocessing for '710'.
[preprocess][L3] 710: Tokenized into 269 tokens.
[preprocess][L3] 710: Removed stop words; 166 tokens remain.
[preprocess][L3] 710: Stemming complete.
[indexDocument][L3] Preprocessing complete for '710'; 166 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '710'.
[indexDocument][L3] Recorded doc length 166 for '710'.
[indexDocument][L3] Tokens added to the inverted index for '710'.
[indexDocument][L2] Indexing document '522'.
[indexDocument][L3] Opened file '522'.
[preprocess][L2] Starting preprocessing for '522'.
[preprocess][L3] 522: Tokenized into 376 tokens.
[preprocess][L3] 522: Removed stop words; 227 tokens remain.
[preprocess][L3] 522: Stemming complete.
[indexDocument][L3] Preprocessing complete for '522'; 227 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '522'.
[indexDocument][L3] Recorded doc length 227 for '522'.
[indexDocument][L3] Tokens added to the inverted index for '522'.
[indexDocument][L2] Indexing document '146'.
[indexDocument][L3] Opened file '146'.
[preprocess][L2] Starting preprocessing for '146'.
[preprocess][L3] 146: Tokenized into 242 tokens.
[preprocess][L3] 146: Removed stop words; 136 tokens remain.
[preprocess][L3] 146: Stemming complete.
[indexDocument][L3] Preprocessing complete for '146'; 136 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '146'.
[indexDocument][L3] Recorded doc length 136 for '146'.
[indexDocument][L3] Tokens added to the inverted index for '146'.
[indexDocument][L2] Indexing document '1094'.
[indexDocument][L3] Opened file '1094'.
[preprocess][L2] Starting preprocessing for '1094'.
[preprocess][L3] 1094: Tokenized into 174 tokens.
[preprocess][L3] 1094: Removed stop words; 100 tokens remain.
[preprocess][L3] 1094: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1094'; 100 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1094'.
[indexDocument][L3] Recorded doc length 100 for '1094'.
[indexDocument][L3] Tokens added to the inverted index for '1094'.
[indexDocument][L2] Indexing document '374'.
[indexDocument][L3] Opened file '374'.
[preprocess][L2] Starting preprocessing for '374'.
[preprocess][L3] 374: Tokenized into 113 tokens.
[preprocess][L3] 374: Removed stop words; 71 tokens remain.
[preprocess][L3] 374: Stemming complete.
[indexDocument][L3] Preprocessing complete for '374'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '374'.
[indexDocument][L3] Recorded doc length 71 for '374'.
[indexDocument][L3] Tokens added to the inverted index for '374'.
[indexDocument][L2] Indexing document '1060'.
[indexDocument][L3] Opened file '1060'.
[preprocess][L2] Starting preprocessing for '1060'.
[preprocess][L3] 1060: Tokenized into 66 tokens.
[preprocess][L3] 1060: Removed stop words; 49 tokens remain.
[preprocess][L3] 1060: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1060'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1060'.
[indexDocument][L3] Recorded doc length 49 for '1060'.
[indexDocument][L3] Tokens added to the inverted index for '1060'.
[indexDocument][L2] Indexing document '380'.
[indexDocument][L3] Opened file '380'.
[preprocess][L2] Starting preprocessing for '380'.
[preprocess][L3] 380: Tokenized into 120 tokens.
[preprocess][L3] 380: Removed stop words; 72 tokens remain.
[preprocess][L3] 380: Stemming complete.
[indexDocument][L3] Preprocessing complete for '380'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '380'.
[indexDocument][L3] Recorded doc length 72 for '380'.
[indexDocument][L3] Tokens added to the inverted index for '380'.
[indexDocument][L2] Indexing document '1252'.
[indexDocument][L3] Opened file '1252'.
[preprocess][L2] Starting preprocessing for '1252'.
[preprocess][L3] 1252: Tokenized into 224 tokens.
[preprocess][L3] 1252: Removed stop words; 121 tokens remain.
[preprocess][L3] 1252: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1252'; 121 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1252'.
[indexDocument][L3] Recorded doc length 121 for '1252'.
[indexDocument][L3] Tokens added to the inverted index for '1252'.
[indexDocument][L2] Indexing document '1299'.
[indexDocument][L3] Opened file '1299'.
[preprocess][L2] Starting preprocessing for '1299'.
[preprocess][L3] 1299: Tokenized into 69 tokens.
[preprocess][L3] 1299: Removed stop words; 47 tokens remain.
[preprocess][L3] 1299: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1299'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1299'.
[indexDocument][L3] Recorded doc length 47 for '1299'.
[indexDocument][L3] Tokens added to the inverted index for '1299'.
[indexDocument][L2] Indexing document '179'.
[indexDocument][L3] Opened file '179'.
[preprocess][L2] Starting preprocessing for '179'.
[preprocess][L3] 179: Tokenized into 262 tokens.
[preprocess][L3] 179: Removed stop words; 150 tokens remain.
[preprocess][L3] 179: Stemming complete.
[indexDocument][L3] Preprocessing complete for '179'; 150 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '179'.
[indexDocument][L3] Recorded doc length 150 for '179'.
[indexDocument][L3] Tokens added to the inverted index for '179'.
[indexDocument][L2] Indexing document '945'.
[indexDocument][L3] Opened file '945'.
[preprocess][L2] Starting preprocessing for '945'.
[preprocess][L3] 945: Tokenized into 103 tokens.
[preprocess][L3] 945: Removed stop words; 67 tokens remain.
[preprocess][L3] 945: Stemming complete.
[indexDocument][L3] Preprocessing complete for '945'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '945'.
[indexDocument][L3] Recorded doc length 67 for '945'.
[indexDocument][L3] Tokens added to the inverted index for '945'.
[indexDocument][L2] Indexing document '170'.
[indexDocument][L3] Opened file '170'.
[preprocess][L2] Starting preprocessing for '170'.
[preprocess][L3] 170: Tokenized into 258 tokens.
[preprocess][L3] 170: Removed stop words; 140 tokens remain.
[preprocess][L3] 170: Stemming complete.
[indexDocument][L3] Preprocessing complete for '170'; 140 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '170'.
[indexDocument][L3] Recorded doc length 140 for '170'.
[indexDocument][L3] Tokens added to the inverted index for '170'.
[indexDocument][L2] Indexing document '1290'.
[indexDocument][L3] Opened file '1290'.
[preprocess][L2] Starting preprocessing for '1290'.
[preprocess][L3] 1290: Tokenized into 211 tokens.
[preprocess][L3] 1290: Removed stop words; 131 tokens remain.
[preprocess][L3] 1290: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1290'; 131 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1290'.
[indexDocument][L3] Recorded doc length 131 for '1290'.
[indexDocument][L3] Tokens added to the inverted index for '1290'.
[indexDocument][L2] Indexing document '342'.
[indexDocument][L3] Opened file '342'.
[preprocess][L2] Starting preprocessing for '342'.
[preprocess][L3] 342: Tokenized into 236 tokens.
[preprocess][L3] 342: Removed stop words; 135 tokens remain.
[preprocess][L3] 342: Stemming complete.
[indexDocument][L3] Preprocessing complete for '342'; 135 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '342'.
[indexDocument][L3] Recorded doc length 135 for '342'.
[indexDocument][L3] Tokens added to the inverted index for '342'.
[indexDocument][L2] Indexing document '726'.
[indexDocument][L3] Opened file '726'.
[preprocess][L2] Starting preprocessing for '726'.
[preprocess][L3] 726: Tokenized into 124 tokens.
[preprocess][L3] 726: Removed stop words; 73 tokens remain.
[preprocess][L3] 726: Stemming complete.
[indexDocument][L3] Preprocessing complete for '726'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '726'.
[indexDocument][L3] Recorded doc length 73 for '726'.
[indexDocument][L3] Tokens added to the inverted index for '726'.
[indexDocument][L2] Indexing document '514'.
[indexDocument][L3] Opened file '514'.
[preprocess][L2] Starting preprocessing for '514'.
[preprocess][L3] 514: Tokenized into 107 tokens.
[preprocess][L3] 514: Removed stop words; 65 tokens remain.
[preprocess][L3] 514: Stemming complete.
[indexDocument][L3] Preprocessing complete for '514'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '514'.
[indexDocument][L3] Recorded doc length 65 for '514'.
[indexDocument][L3] Tokens added to the inverted index for '514'.
[indexDocument][L2] Indexing document '1056'.
[indexDocument][L3] Opened file '1056'.
[preprocess][L2] Starting preprocessing for '1056'.
[preprocess][L3] 1056: Tokenized into 235 tokens.
[preprocess][L3] 1056: Removed stop words; 152 tokens remain.
[preprocess][L3] 1056: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1056'; 152 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1056'.
[indexDocument][L3] Recorded doc length 152 for '1056'.
[indexDocument][L3] Tokens added to the inverted index for '1056'.
[indexDocument][L2] Indexing document '184'.
[indexDocument][L3] Opened file '184'.
[preprocess][L2] Starting preprocessing for '184'.
[preprocess][L3] 184: Tokenized into 149 tokens.
[preprocess][L3] 184: Removed stop words; 88 tokens remain.
[preprocess][L3] 184: Stemming complete.
[indexDocument][L3] Preprocessing complete for '184'; 88 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '184'.
[indexDocument][L3] Recorded doc length 88 for '184'.
[indexDocument][L3] Tokens added to the inverted index for '184'.
[indexDocument][L2] Indexing document '1264'.
[indexDocument][L3] Opened file '1264'.
[preprocess][L2] Starting preprocessing for '1264'.
[preprocess][L3] 1264: Tokenized into 219 tokens.
[preprocess][L3] 1264: Removed stop words; 139 tokens remain.
[preprocess][L3] 1264: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1264'; 139 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1264'.
[indexDocument][L3] Recorded doc length 139 for '1264'.
[indexDocument][L3] Tokens added to the inverted index for '1264'.
[indexDocument][L2] Indexing document '719'.
[indexDocument][L3] Opened file '719'.
[preprocess][L2] Starting preprocessing for '719'.
[preprocess][L3] 719: Tokenized into 168 tokens.
[preprocess][L3] 719: Removed stop words; 108 tokens remain.
[preprocess][L3] 719: Stemming complete.
[indexDocument][L3] Preprocessing complete for '719'; 108 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '719'.
[indexDocument][L3] Recorded doc length 108 for '719'.
[indexDocument][L3] Tokens added to the inverted index for '719'.
[indexDocument][L2] Indexing document '973'.
[indexDocument][L3] Opened file '973'.
[preprocess][L2] Starting preprocessing for '973'.
[preprocess][L3] 973: Tokenized into 102 tokens.
[preprocess][L3] 973: Removed stop words; 71 tokens remain.
[preprocess][L3] 973: Stemming complete.
[indexDocument][L3] Preprocessing complete for '973'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '973'.
[indexDocument][L3] Recorded doc length 71 for '973'.
[indexDocument][L3] Tokens added to the inverted index for '973'.
[indexDocument][L2] Indexing document '389'.
[indexDocument][L3] Opened file '389'.
[preprocess][L2] Starting preprocessing for '389'.
[preprocess][L3] 389: Tokenized into 72 tokens.
[preprocess][L3] 389: Removed stop words; 43 tokens remain.
[preprocess][L3] 389: Stemming complete.
[indexDocument][L3] Preprocessing complete for '389'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '389'.
[indexDocument][L3] Recorded doc length 43 for '389'.
[indexDocument][L3] Tokens added to the inverted index for '389'.
[indexDocument][L2] Indexing document '1069'.
[indexDocument][L3] Opened file '1069'.
[preprocess][L2] Starting preprocessing for '1069'.
[preprocess][L3] 1069: Tokenized into 68 tokens.
[preprocess][L3] 1069: Removed stop words; 47 tokens remain.
[preprocess][L3] 1069: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1069'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1069'.
[indexDocument][L3] Recorded doc length 47 for '1069'.
[indexDocument][L3] Tokens added to the inverted index for '1069'.
[indexDocument][L2] Indexing document '987'.
[indexDocument][L3] Opened file '987'.
[preprocess][L2] Starting preprocessing for '987'.
[preprocess][L3] 987: Tokenized into 254 tokens.
[preprocess][L3] 987: Removed stop words; 145 tokens remain.
[preprocess][L3] 987: Stemming complete.
[indexDocument][L3] Preprocessing complete for '987'; 145 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '987'.
[indexDocument][L3] Recorded doc length 145 for '987'.
[indexDocument][L3] Tokens added to the inverted index for '987'.
[indexDocument][L2] Indexing document '1263'.
[indexDocument][L3] Opened file '1263'.
[preprocess][L2] Starting preprocessing for '1263'.
[preprocess][L3] 1263: Tokenized into 293 tokens.
[preprocess][L3] 1263: Removed stop words; 177 tokens remain.
[preprocess][L3] 1263: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1263'; 177 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1263'.
[indexDocument][L3] Recorded doc length 177 for '1263'.
[indexDocument][L3] Tokens added to the inverted index for '1263'.
[indexDocument][L2] Indexing document '1051'.
[indexDocument][L3] Opened file '1051'.
[preprocess][L2] Starting preprocessing for '1051'.
[preprocess][L3] 1051: Tokenized into 206 tokens.
[preprocess][L3] 1051: Removed stop words; 133 tokens remain.
[preprocess][L3] 1051: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1051'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1051'.
[indexDocument][L3] Recorded doc length 133 for '1051'.
[indexDocument][L3] Tokens added to the inverted index for '1051'.
[indexDocument][L2] Indexing document '183'.
[indexDocument][L3] Opened file '183'.
[preprocess][L2] Starting preprocessing for '183'.
[preprocess][L3] 183: Tokenized into 156 tokens.
[preprocess][L3] 183: Removed stop words; 92 tokens remain.
[preprocess][L3] 183: Stemming complete.
[indexDocument][L3] Preprocessing complete for '183'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '183'.
[indexDocument][L3] Recorded doc length 92 for '183'.
[indexDocument][L3] Tokens added to the inverted index for '183'.
[indexDocument][L2] Indexing document '513'.
[indexDocument][L3] Opened file '513'.
[preprocess][L2] Starting preprocessing for '513'.
[preprocess][L3] 513: Tokenized into 109 tokens.
[preprocess][L3] 513: Removed stop words; 73 tokens remain.
[preprocess][L3] 513: Stemming complete.
[indexDocument][L3] Preprocessing complete for '513'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '513'.
[indexDocument][L3] Recorded doc length 73 for '513'.
[indexDocument][L3] Tokens added to the inverted index for '513'.
[indexDocument][L2] Indexing document '721'.
[indexDocument][L3] Opened file '721'.
[preprocess][L2] Starting preprocessing for '721'.
[preprocess][L3] 721: Tokenized into 505 tokens.
[preprocess][L3] 721: Removed stop words; 302 tokens remain.
[preprocess][L3] 721: Stemming complete.
[indexDocument][L3] Preprocessing complete for '721'; 302 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '721'.
[indexDocument][L3] Recorded doc length 302 for '721'.
[indexDocument][L3] Tokens added to the inverted index for '721'.
[indexDocument][L2] Indexing document '1297'.
[indexDocument][L3] Opened file '1297'.
[preprocess][L2] Starting preprocessing for '1297'.
[preprocess][L3] 1297: Tokenized into 133 tokens.
[preprocess][L3] 1297: Removed stop words; 91 tokens remain.
[preprocess][L3] 1297: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1297'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1297'.
[indexDocument][L3] Recorded doc length 91 for '1297'.
[indexDocument][L3] Tokens added to the inverted index for '1297'.
[indexDocument][L2] Indexing document '345'.
[indexDocument][L3] Opened file '345'.
[preprocess][L2] Starting preprocessing for '345'.
[preprocess][L3] 345: Tokenized into 151 tokens.
[preprocess][L3] 345: Removed stop words; 92 tokens remain.
[preprocess][L3] 345: Stemming complete.
[indexDocument][L3] Preprocessing complete for '345'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '345'.
[indexDocument][L3] Recorded doc length 92 for '345'.
[indexDocument][L3] Tokens added to the inverted index for '345'.
[indexDocument][L2] Indexing document '177'.
[indexDocument][L3] Opened file '177'.
[preprocess][L2] Starting preprocessing for '177'.
[preprocess][L3] 177: Tokenized into 244 tokens.
[preprocess][L3] 177: Removed stop words; 134 tokens remain.
[preprocess][L3] 177: Stemming complete.
[indexDocument][L3] Preprocessing complete for '177'; 134 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '177'.
[indexDocument][L3] Recorded doc length 134 for '177'.
[indexDocument][L3] Tokens added to the inverted index for '177'.
[indexDocument][L2] Indexing document '980'.
[indexDocument][L3] Opened file '980'.
[preprocess][L2] Starting preprocessing for '980'.
[preprocess][L3] 980: Tokenized into 206 tokens.
[preprocess][L3] 980: Removed stop words; 121 tokens remain.
[preprocess][L3] 980: Stemming complete.
[indexDocument][L3] Preprocessing complete for '980'; 121 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '980'.
[indexDocument][L3] Recorded doc length 121 for '980'.
[indexDocument][L3] Tokens added to the inverted index for '980'.
[indexDocument][L2] Indexing document '148'.
[indexDocument][L3] Opened file '148'.
[preprocess][L2] Starting preprocessing for '148'.
[preprocess][L3] 148: Tokenized into 149 tokens.
[preprocess][L3] 148: Removed stop words; 88 tokens remain.
[preprocess][L3] 148: Stemming complete.
[indexDocument][L3] Preprocessing complete for '148'; 88 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '148'.
[indexDocument][L3] Recorded doc length 88 for '148'.
[indexDocument][L3] Tokens added to the inverted index for '148'.
[indexDocument][L2] Indexing document '974'.
[indexDocument][L3] Opened file '974'.
[preprocess][L2] Starting preprocessing for '974'.
[preprocess][L3] 974: Tokenized into 212 tokens.
[preprocess][L3] 974: Removed stop words; 124 tokens remain.
[preprocess][L3] 974: Stemming complete.
[indexDocument][L3] Preprocessing complete for '974'; 124 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '974'.
[indexDocument][L3] Recorded doc length 124 for '974'.
[indexDocument][L3] Tokens added to the inverted index for '974'.
[indexDocument][L2] Indexing document '1208'.
[indexDocument][L3] Opened file '1208'.
[preprocess][L2] Starting preprocessing for '1208'.
[preprocess][L3] 1208: Tokenized into 122 tokens.
[preprocess][L3] 1208: Removed stop words; 72 tokens remain.
[preprocess][L3] 1208: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1208'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1208'.
[indexDocument][L3] Recorded doc length 72 for '1208'.
[indexDocument][L3] Tokens added to the inverted index for '1208'.
[indexDocument][L2] Indexing document '920'.
[indexDocument][L3] Opened file '920'.
[preprocess][L2] Starting preprocessing for '920'.
[preprocess][L3] 920: Tokenized into 54 tokens.
[preprocess][L3] 920: Removed stop words; 38 tokens remain.
[preprocess][L3] 920: Stemming complete.
[indexDocument][L3] Preprocessing complete for '920'; 38 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '920'.
[indexDocument][L3] Recorded doc length 38 for '920'.
[indexDocument][L3] Tokens added to the inverted index for '920'.
[indexDocument][L2] Indexing document '578'.
[indexDocument][L3] Opened file '578'.
[preprocess][L2] Starting preprocessing for '578'.
[preprocess][L3] 578: Tokenized into 58 tokens.
[preprocess][L3] 578: Removed stop words; 45 tokens remain.
[preprocess][L3] 578: Stemming complete.
[indexDocument][L3] Preprocessing complete for '578'; 45 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '578'.
[indexDocument][L3] Recorded doc length 45 for '578'.
[indexDocument][L3] Tokens added to the inverted index for '578'.
[indexDocument][L2] Indexing document '1005'.
[indexDocument][L3] Opened file '1005'.
[preprocess][L2] Starting preprocessing for '1005'.
[preprocess][L3] 1005: Tokenized into 115 tokens.
[preprocess][L3] 1005: Removed stop words; 74 tokens remain.
[preprocess][L3] 1005: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1005'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1005'.
[indexDocument][L3] Recorded doc length 74 for '1005'.
[indexDocument][L3] Tokens added to the inverted index for '1005'.
[indexDocument][L2] Indexing document '1237'.
[indexDocument][L3] Opened file '1237'.
[preprocess][L2] Starting preprocessing for '1237'.
[preprocess][L3] 1237: Tokenized into 121 tokens.
[preprocess][L3] 1237: Removed stop words; 75 tokens remain.
[preprocess][L3] 1237: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1237'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1237'.
[indexDocument][L3] Recorded doc length 75 for '1237'.
[indexDocument][L3] Tokens added to the inverted index for '1237'.
[indexDocument][L2] Indexing document '781'.
[indexDocument][L3] Opened file '781'.
[preprocess][L2] Starting preprocessing for '781'.
[preprocess][L3] 781: Tokenized into 225 tokens.
[preprocess][L3] 781: Removed stop words; 137 tokens remain.
[preprocess][L3] 781: Stemming complete.
[indexDocument][L3] Preprocessing complete for '781'; 137 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '781'.
[indexDocument][L3] Recorded doc length 137 for '781'.
[indexDocument][L3] Tokens added to the inverted index for '781'.
[indexDocument][L2] Indexing document '775'.
[indexDocument][L3] Opened file '775'.
[preprocess][L2] Starting preprocessing for '775'.
[preprocess][L3] 775: Tokenized into 93 tokens.
[preprocess][L3] 775: Removed stop words; 47 tokens remain.
[preprocess][L3] 775: Stemming complete.
[indexDocument][L3] Preprocessing complete for '775'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '775'.
[indexDocument][L3] Recorded doc length 47 for '775'.
[indexDocument][L3] Tokens added to the inverted index for '775'.
[indexDocument][L2] Indexing document '547'.
[indexDocument][L3] Opened file '547'.
[preprocess][L2] Starting preprocessing for '547'.
[preprocess][L3] 547: Tokenized into 130 tokens.
[preprocess][L3] 547: Removed stop words; 78 tokens remain.
[preprocess][L3] 547: Stemming complete.
[indexDocument][L3] Preprocessing complete for '547'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '547'.
[indexDocument][L3] Recorded doc length 78 for '547'.
[indexDocument][L3] Tokens added to the inverted index for '547'.
[indexDocument][L2] Indexing document '123'.
[indexDocument][L3] Opened file '123'.
[preprocess][L2] Starting preprocessing for '123'.
[preprocess][L3] 123: Tokenized into 243 tokens.
[preprocess][L3] 123: Removed stop words; 143 tokens remain.
[preprocess][L3] 123: Stemming complete.
[indexDocument][L3] Preprocessing complete for '123'; 143 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '123'.
[indexDocument][L3] Recorded doc length 143 for '123'.
[indexDocument][L3] Tokens added to the inverted index for '123'.
[indexDocument][L2] Indexing document '311'.
[indexDocument][L3] Opened file '311'.
[preprocess][L2] Starting preprocessing for '311'.
[preprocess][L3] 311: Tokenized into 194 tokens.
[preprocess][L3] 311: Removed stop words; 115 tokens remain.
[preprocess][L3] 311: Stemming complete.
[indexDocument][L3] Preprocessing complete for '311'; 115 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '311'.
[indexDocument][L3] Recorded doc length 115 for '311'.
[indexDocument][L3] Tokens added to the inverted index for '311'.
[indexDocument][L2] Indexing document '927'.
[indexDocument][L3] Opened file '927'.
[preprocess][L2] Starting preprocessing for '927'.
[preprocess][L3] 927: Tokenized into 435 tokens.
[preprocess][L3] 927: Removed stop words; 252 tokens remain.
[preprocess][L3] 927: Stemming complete.
[indexDocument][L3] Preprocessing complete for '927'; 252 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '927'.
[indexDocument][L3] Recorded doc length 252 for '927'.
[indexDocument][L3] Tokens added to the inverted index for '927'.
[indexDocument][L2] Indexing document '329'.
[indexDocument][L3] Opened file '329'.
[preprocess][L2] Starting preprocessing for '329'.
[preprocess][L3] 329: Tokenized into 634 tokens.
[preprocess][L3] 329: Removed stop words; 376 tokens remain.
[preprocess][L3] 329: Stemming complete.
[indexDocument][L3] Preprocessing complete for '329'; 376 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '329'.
[indexDocument][L3] Recorded doc length 376 for '329'.
[indexDocument][L3] Tokens added to the inverted index for '329'.
[indexDocument][L2] Indexing document '316'.
[indexDocument][L3] Opened file '316'.
[preprocess][L2] Starting preprocessing for '316'.
[preprocess][L3] 316: Tokenized into 113 tokens.
[preprocess][L3] 316: Removed stop words; 63 tokens remain.
[preprocess][L3] 316: Stemming complete.
[indexDocument][L3] Preprocessing complete for '316'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '316'.
[indexDocument][L3] Recorded doc length 63 for '316'.
[indexDocument][L3] Tokens added to the inverted index for '316'.
[indexDocument][L2] Indexing document '918'.
[indexDocument][L3] Opened file '918'.
[preprocess][L2] Starting preprocessing for '918'.
[preprocess][L3] 918: Tokenized into 111 tokens.
[preprocess][L3] 918: Removed stop words; 67 tokens remain.
[preprocess][L3] 918: Stemming complete.
[indexDocument][L3] Preprocessing complete for '918'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '918'.
[indexDocument][L3] Recorded doc length 67 for '918'.
[indexDocument][L3] Tokens added to the inverted index for '918'.
[indexDocument][L2] Indexing document '124'.
[indexDocument][L3] Opened file '124'.
[preprocess][L2] Starting preprocessing for '124'.
[preprocess][L3] 124: Tokenized into 213 tokens.
[preprocess][L3] 124: Removed stop words; 127 tokens remain.
[preprocess][L3] 124: Stemming complete.
[indexDocument][L3] Preprocessing complete for '124'; 127 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '124'.
[indexDocument][L3] Recorded doc length 127 for '124'.
[indexDocument][L3] Tokens added to the inverted index for '124'.
[indexDocument][L2] Indexing document '540'.
[indexDocument][L3] Opened file '540'.
[preprocess][L2] Starting preprocessing for '540'.
[preprocess][L3] 540: Tokenized into 207 tokens.
[preprocess][L3] 540: Removed stop words; 123 tokens remain.
[preprocess][L3] 540: Stemming complete.
[indexDocument][L3] Preprocessing complete for '540'; 123 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '540'.
[indexDocument][L3] Recorded doc length 123 for '540'.
[indexDocument][L3] Tokens added to the inverted index for '540'.
[indexDocument][L2] Indexing document '772'.
[indexDocument][L3] Opened file '772'.
[preprocess][L2] Starting preprocessing for '772'.
[preprocess][L3] 772: Tokenized into 115 tokens.
[preprocess][L3] 772: Removed stop words; 74 tokens remain.
[preprocess][L3] 772: Stemming complete.
[indexDocument][L3] Preprocessing complete for '772'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '772'.
[indexDocument][L3] Recorded doc length 74 for '772'.
[indexDocument][L3] Tokens added to the inverted index for '772'.
[indexDocument][L2] Indexing document '786'.
[indexDocument][L3] Opened file '786'.
[preprocess][L2] Starting preprocessing for '786'.
[preprocess][L3] 786: Tokenized into 237 tokens.
[preprocess][L3] 786: Removed stop words; 123 tokens remain.
[preprocess][L3] 786: Stemming complete.
[indexDocument][L3] Preprocessing complete for '786'; 123 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '786'.
[indexDocument][L3] Recorded doc length 123 for '786'.
[indexDocument][L3] Tokens added to the inverted index for '786'.
[indexDocument][L2] Indexing document '1230'.
[indexDocument][L3] Opened file '1230'.
[preprocess][L2] Starting preprocessing for '1230'.
[preprocess][L3] 1230: Tokenized into 188 tokens.
[preprocess][L3] 1230: Removed stop words; 113 tokens remain.
[preprocess][L3] 1230: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1230'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1230'.
[indexDocument][L3] Recorded doc length 113 for '1230'.
[indexDocument][L3] Tokens added to the inverted index for '1230'.
[indexDocument][L2] Indexing document '1002'.
[indexDocument][L3] Opened file '1002'.
[preprocess][L2] Starting preprocessing for '1002'.
[preprocess][L3] 1002: Tokenized into 181 tokens.
[preprocess][L3] 1002: Removed stop words; 106 tokens remain.
[preprocess][L3] 1002: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1002'; 106 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1002'.
[indexDocument][L3] Recorded doc length 106 for '1002'.
[indexDocument][L3] Tokens added to the inverted index for '1002'.
[indexDocument][L2] Indexing document '911'.
[indexDocument][L3] Opened file '911'.
[preprocess][L2] Starting preprocessing for '911'.
[preprocess][L3] 911: Tokenized into 118 tokens.
[preprocess][L3] 911: Removed stop words; 71 tokens remain.
[preprocess][L3] 911: Stemming complete.
[indexDocument][L3] Preprocessing complete for '911'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '911'.
[indexDocument][L3] Recorded doc length 71 for '911'.
[indexDocument][L3] Tokens added to the inverted index for '911'.
[indexDocument][L2] Indexing document '549'.
[indexDocument][L3] Opened file '549'.
[preprocess][L2] Starting preprocessing for '549'.
[preprocess][L3] 549: Tokenized into 82 tokens.
[preprocess][L3] 549: Removed stop words; 48 tokens remain.
[preprocess][L3] 549: Stemming complete.
[indexDocument][L3] Preprocessing complete for '549'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '549'.
[indexDocument][L3] Recorded doc length 48 for '549'.
[indexDocument][L3] Tokens added to the inverted index for '549'.
[indexDocument][L2] Indexing document '1239'.
[indexDocument][L3] Opened file '1239'.
[preprocess][L2] Starting preprocessing for '1239'.
[preprocess][L3] 1239: Tokenized into 429 tokens.
[preprocess][L3] 1239: Removed stop words; 234 tokens remain.
[preprocess][L3] 1239: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1239'; 234 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1239'.
[indexDocument][L3] Recorded doc length 234 for '1239'.
[indexDocument][L3] Tokens added to the inverted index for '1239'.
[indexDocument][L2] Indexing document '576'.
[indexDocument][L3] Opened file '576'.
[preprocess][L2] Starting preprocessing for '576'.
[preprocess][L3] 576: Tokenized into 423 tokens.
[preprocess][L3] 576: Removed stop words; 263 tokens remain.
[preprocess][L3] 576: Stemming complete.
[indexDocument][L3] Preprocessing complete for '576'; 263 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '576'.
[indexDocument][L3] Recorded doc length 263 for '576'.
[indexDocument][L3] Tokens added to the inverted index for '576'.
[indexDocument][L2] Indexing document '744'.
[indexDocument][L3] Opened file '744'.
[preprocess][L2] Starting preprocessing for '744'.
[preprocess][L3] 744: Tokenized into 133 tokens.
[preprocess][L3] 744: Removed stop words; 80 tokens remain.
[preprocess][L3] 744: Stemming complete.
[indexDocument][L3] Preprocessing complete for '744'; 80 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '744'.
[indexDocument][L3] Recorded doc length 80 for '744'.
[indexDocument][L3] Tokens added to the inverted index for '744'.
[indexDocument][L2] Indexing document '320'.
[indexDocument][L3] Opened file '320'.
[preprocess][L2] Starting preprocessing for '320'.
[preprocess][L3] 320: Tokenized into 35 tokens.
[preprocess][L3] 320: Removed stop words; 26 tokens remain.
[preprocess][L3] 320: Stemming complete.
[indexDocument][L3] Preprocessing complete for '320'; 26 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '320'.
[indexDocument][L3] Recorded doc length 26 for '320'.
[indexDocument][L3] Tokens added to the inverted index for '320'.
[indexDocument][L2] Indexing document '112'.
[indexDocument][L3] Opened file '112'.
[preprocess][L2] Starting preprocessing for '112'.
[preprocess][L3] 112: Tokenized into 151 tokens.
[preprocess][L3] 112: Removed stop words; 93 tokens remain.
[preprocess][L3] 112: Stemming complete.
[indexDocument][L3] Preprocessing complete for '112'; 93 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '112'.
[indexDocument][L3] Recorded doc length 93 for '112'.
[indexDocument][L3] Tokens added to the inverted index for '112'.
[indexDocument][L2] Indexing document '1206'.
[indexDocument][L3] Opened file '1206'.
[preprocess][L2] Starting preprocessing for '1206'.
[preprocess][L3] 1206: Tokenized into 131 tokens.
[preprocess][L3] 1206: Removed stop words; 81 tokens remain.
[preprocess][L3] 1206: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1206'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1206'.
[indexDocument][L3] Recorded doc length 81 for '1206'.
[indexDocument][L3] Tokens added to the inverted index for '1206'.
[indexDocument][L2] Indexing document '1034'.
[indexDocument][L3] Opened file '1034'.
[preprocess][L2] Starting preprocessing for '1034'.
[preprocess][L3] 1034: Tokenized into 192 tokens.
[preprocess][L3] 1034: Removed stop words; 117 tokens remain.
[preprocess][L3] 1034: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1034'; 117 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1034'.
[indexDocument][L3] Recorded doc length 117 for '1034'.
[indexDocument][L3] Tokens added to the inverted index for '1034'.
[indexDocument][L2] Indexing document '582'.
[indexDocument][L3] Opened file '582'.
[preprocess][L2] Starting preprocessing for '582'.
[preprocess][L3] 582: Tokenized into 100 tokens.
[preprocess][L3] 582: Removed stop words; 59 tokens remain.
[preprocess][L3] 582: Stemming complete.
[indexDocument][L3] Preprocessing complete for '582'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '582'.
[indexDocument][L3] Recorded doc length 59 for '582'.
[indexDocument][L3] Tokens added to the inverted index for '582'.
[indexDocument][L2] Indexing document '788'.
[indexDocument][L3] Opened file '788'.
[preprocess][L2] Starting preprocessing for '788'.
[preprocess][L3] 788: Tokenized into 228 tokens.
[preprocess][L3] 788: Removed stop words; 136 tokens remain.
[preprocess][L3] 788: Stemming complete.
[indexDocument][L3] Preprocessing complete for '788'; 136 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '788'.
[indexDocument][L3] Recorded doc length 136 for '788'.
[indexDocument][L3] Tokens added to the inverted index for '788'.
[indexDocument][L2] Indexing document '318'.
[indexDocument][L3] Opened file '318'.
[preprocess][L2] Starting preprocessing for '318'.
[preprocess][L3] 318: Tokenized into 146 tokens.
[preprocess][L3] 318: Removed stop words; 92 tokens remain.
[preprocess][L3] 318: Stemming complete.
[indexDocument][L3] Preprocessing complete for '318'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '318'.
[indexDocument][L3] Recorded doc length 92 for '318'.
[indexDocument][L3] Tokens added to the inverted index for '318'.
[indexDocument][L2] Indexing document '916'.
[indexDocument][L3] Opened file '916'.
[preprocess][L2] Starting preprocessing for '916'.
[preprocess][L3] 916: Tokenized into 174 tokens.
[preprocess][L3] 916: Removed stop words; 106 tokens remain.
[preprocess][L3] 916: Stemming complete.
[indexDocument][L3] Preprocessing complete for '916'; 106 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '916'.
[indexDocument][L3] Recorded doc length 106 for '916'.
[indexDocument][L3] Tokens added to the inverted index for '916'.
[indexDocument][L2] Indexing document '585'.
[indexDocument][L3] Opened file '585'.
[preprocess][L2] Starting preprocessing for '585'.
[preprocess][L3] 585: Tokenized into 117 tokens.
[preprocess][L3] 585: Removed stop words; 72 tokens remain.
[preprocess][L3] 585: Stemming complete.
[indexDocument][L3] Preprocessing complete for '585'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '585'.
[indexDocument][L3] Recorded doc length 72 for '585'.
[indexDocument][L3] Tokens added to the inverted index for '585'.
[indexDocument][L2] Indexing document '1033'.
[indexDocument][L3] Opened file '1033'.
[preprocess][L2] Starting preprocessing for '1033'.
[preprocess][L3] 1033: Tokenized into 105 tokens.
[preprocess][L3] 1033: Removed stop words; 59 tokens remain.
[preprocess][L3] 1033: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1033'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1033'.
[indexDocument][L3] Recorded doc length 59 for '1033'.
[indexDocument][L3] Tokens added to the inverted index for '1033'.
[indexDocument][L2] Indexing document '1201'.
[indexDocument][L3] Opened file '1201'.
[preprocess][L2] Starting preprocessing for '1201'.
[preprocess][L3] 1201: Tokenized into 588 tokens.
[preprocess][L3] 1201: Removed stop words; 296 tokens remain.
[preprocess][L3] 1201: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1201'; 296 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1201'.
[indexDocument][L3] Recorded doc length 296 for '1201'.
[indexDocument][L3] Tokens added to the inverted index for '1201'.
[indexDocument][L2] Indexing document '929'.
[indexDocument][L3] Opened file '929'.
[preprocess][L2] Starting preprocessing for '929'.
[preprocess][L3] 929: Tokenized into 170 tokens.
[preprocess][L3] 929: Removed stop words; 98 tokens remain.
[preprocess][L3] 929: Stemming complete.
[indexDocument][L3] Preprocessing complete for '929'; 98 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '929'.
[indexDocument][L3] Recorded doc length 98 for '929'.
[indexDocument][L3] Tokens added to the inverted index for '929'.
[indexDocument][L2] Indexing document '115'.
[indexDocument][L3] Opened file '115'.
[preprocess][L2] Starting preprocessing for '115'.
[preprocess][L3] 115: Tokenized into 197 tokens.
[preprocess][L3] 115: Removed stop words; 115 tokens remain.
[preprocess][L3] 115: Stemming complete.
[indexDocument][L3] Preprocessing complete for '115'; 115 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '115'.
[indexDocument][L3] Recorded doc length 115 for '115'.
[indexDocument][L3] Tokens added to the inverted index for '115'.
[indexDocument][L2] Indexing document '327'.
[indexDocument][L3] Opened file '327'.
[preprocess][L2] Starting preprocessing for '327'.
[preprocess][L3] 327: Tokenized into 90 tokens.
[preprocess][L3] 327: Removed stop words; 60 tokens remain.
[preprocess][L3] 327: Stemming complete.
[indexDocument][L3] Preprocessing complete for '327'; 60 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '327'.
[indexDocument][L3] Recorded doc length 60 for '327'.
[indexDocument][L3] Tokens added to the inverted index for '327'.
[indexDocument][L2] Indexing document '743'.
[indexDocument][L3] Opened file '743'.
[preprocess][L2] Starting preprocessing for '743'.
[preprocess][L3] 743: Tokenized into 135 tokens.
[preprocess][L3] 743: Removed stop words; 89 tokens remain.
[preprocess][L3] 743: Stemming complete.
[indexDocument][L3] Preprocessing complete for '743'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '743'.
[indexDocument][L3] Recorded doc length 89 for '743'.
[indexDocument][L3] Tokens added to the inverted index for '743'.
[indexDocument][L2] Indexing document '571'.
[indexDocument][L3] Opened file '571'.
[preprocess][L2] Starting preprocessing for '571'.
[preprocess][L3] 571: Tokenized into 226 tokens.
[preprocess][L3] 571: Removed stop words; 149 tokens remain.
[preprocess][L3] 571: Stemming complete.
[indexDocument][L3] Preprocessing complete for '571'; 149 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '571'.
[indexDocument][L3] Recorded doc length 149 for '571'.
[indexDocument][L3] Tokens added to the inverted index for '571'.
[indexDocument][L2] Indexing document '952'.
[indexDocument][L3] Opened file '952'.
[preprocess][L2] Starting preprocessing for '952'.
[preprocess][L3] 952: Tokenized into 121 tokens.
[preprocess][L3] 952: Removed stop words; 76 tokens remain.
[preprocess][L3] 952: Stemming complete.
[indexDocument][L3] Preprocessing complete for '952'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '952'.
[indexDocument][L3] Recorded doc length 76 for '952'.
[indexDocument][L3] Tokens added to the inverted index for '952'.
[indexDocument][L2] Indexing document '738'.
[indexDocument][L3] Opened file '738'.
[preprocess][L2] Starting preprocessing for '738'.
[preprocess][L3] 738: Tokenized into 148 tokens.
[preprocess][L3] 738: Removed stop words; 93 tokens remain.
[preprocess][L3] 738: Stemming complete.
[indexDocument][L3] Preprocessing complete for '738'; 93 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '738'.
[indexDocument][L3] Recorded doc length 93 for '738'.
[indexDocument][L3] Tokens added to the inverted index for '738'.
[indexDocument][L2] Indexing document '1048'.
[indexDocument][L3] Opened file '1048'.
[preprocess][L2] Starting preprocessing for '1048'.
[preprocess][L3] 1048: Tokenized into 56 tokens.
[preprocess][L3] 1048: Removed stop words; 39 tokens remain.
[preprocess][L3] 1048: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1048'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1048'.
[indexDocument][L3] Recorded doc length 39 for '1048'.
[indexDocument][L3] Tokens added to the inverted index for '1048'.
[indexDocument][L2] Indexing document '707'.
[indexDocument][L3] Opened file '707'.
[preprocess][L2] Starting preprocessing for '707'.
[preprocess][L3] 707: Tokenized into 227 tokens.
[preprocess][L3] 707: Removed stop words; 133 tokens remain.
[preprocess][L3] 707: Stemming complete.
[indexDocument][L3] Preprocessing complete for '707'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '707'.
[indexDocument][L3] Recorded doc length 133 for '707'.
[indexDocument][L3] Tokens added to the inverted index for '707'.
[indexDocument][L2] Indexing document '535'.
[indexDocument][L3] Opened file '535'.
[preprocess][L2] Starting preprocessing for '535'.
[preprocess][L3] 535: Tokenized into 126 tokens.
[preprocess][L3] 535: Removed stop words; 77 tokens remain.
[preprocess][L3] 535: Stemming complete.
[indexDocument][L3] Preprocessing complete for '535'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '535'.
[indexDocument][L3] Recorded doc length 77 for '535'.
[indexDocument][L3] Tokens added to the inverted index for '535'.
[indexDocument][L2] Indexing document '1083'.
[indexDocument][L3] Opened file '1083'.
[preprocess][L2] Starting preprocessing for '1083'.
[preprocess][L3] 1083: Tokenized into 80 tokens.
[preprocess][L3] 1083: Removed stop words; 51 tokens remain.
[preprocess][L3] 1083: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1083'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1083'.
[indexDocument][L3] Recorded doc length 51 for '1083'.
[indexDocument][L3] Tokens added to the inverted index for '1083'.
[indexDocument][L2] Indexing document '151'.
[indexDocument][L3] Opened file '151'.
[preprocess][L2] Starting preprocessing for '151'.
[preprocess][L3] 151: Tokenized into 305 tokens.
[preprocess][L3] 151: Removed stop words; 160 tokens remain.
[preprocess][L3] 151: Stemming complete.
[indexDocument][L3] Preprocessing complete for '151'; 160 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '151'.
[indexDocument][L3] Recorded doc length 160 for '151'.
[indexDocument][L3] Tokens added to the inverted index for '151'.
[indexDocument][L2] Indexing document '363'.
[indexDocument][L3] Opened file '363'.
[preprocess][L2] Starting preprocessing for '363'.
[preprocess][L3] 363: Tokenized into 250 tokens.
[preprocess][L3] 363: Removed stop words; 144 tokens remain.
[preprocess][L3] 363: Stemming complete.
[indexDocument][L3] Preprocessing complete for '363'; 144 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '363'.
[indexDocument][L3] Recorded doc length 144 for '363'.
[indexDocument][L3] Tokens added to the inverted index for '363'.
[indexDocument][L2] Indexing document '1077'.
[indexDocument][L3] Opened file '1077'.
[preprocess][L2] Starting preprocessing for '1077'.
[preprocess][L3] 1077: Tokenized into 143 tokens.
[preprocess][L3] 1077: Removed stop words; 91 tokens remain.
[preprocess][L3] 1077: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1077'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1077'.
[indexDocument][L3] Recorded doc length 91 for '1077'.
[indexDocument][L3] Tokens added to the inverted index for '1077'.
[indexDocument][L2] Indexing document '999'.
[indexDocument][L3] Opened file '999'.
[preprocess][L2] Starting preprocessing for '999'.
[preprocess][L3] 999: Tokenized into 222 tokens.
[preprocess][L3] 999: Removed stop words; 144 tokens remain.
[preprocess][L3] 999: Stemming complete.
[indexDocument][L3] Preprocessing complete for '999'; 144 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '999'.
[indexDocument][L3] Recorded doc length 144 for '999'.
[indexDocument][L3] Tokens added to the inverted index for '999'.
[indexDocument][L2] Indexing document '1245'.
[indexDocument][L3] Opened file '1245'.
[preprocess][L2] Starting preprocessing for '1245'.
[preprocess][L3] 1245: Tokenized into 170 tokens.
[preprocess][L3] 1245: Removed stop words; 102 tokens remain.
[preprocess][L3] 1245: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1245'; 102 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1245'.
[indexDocument][L3] Recorded doc length 102 for '1245'.
[indexDocument][L3] Tokens added to the inverted index for '1245'.
[indexDocument][L2] Indexing document '397'.
[indexDocument][L3] Opened file '397'.
[preprocess][L2] Starting preprocessing for '397'.
[preprocess][L3] 397: Tokenized into 122 tokens.
[preprocess][L3] 397: Removed stop words; 73 tokens remain.
[preprocess][L3] 397: Stemming complete.
[indexDocument][L3] Preprocessing complete for '397'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '397'.
[indexDocument][L3] Recorded doc length 73 for '397'.
[indexDocument][L3] Tokens added to the inverted index for '397'.
[indexDocument][L2] Indexing document '169'.
[indexDocument][L3] Opened file '169'.
[preprocess][L2] Starting preprocessing for '169'.
[preprocess][L3] 169: Tokenized into 137 tokens.
[preprocess][L3] 169: Removed stop words; 82 tokens remain.
[preprocess][L3] 169: Stemming complete.
[indexDocument][L3] Preprocessing complete for '169'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '169'.
[indexDocument][L3] Recorded doc length 82 for '169'.
[indexDocument][L3] Tokens added to the inverted index for '169'.
[indexDocument][L2] Indexing document '955'.
[indexDocument][L3] Opened file '955'.
[preprocess][L2] Starting preprocessing for '955'.
[preprocess][L3] 955: Tokenized into 93 tokens.
[preprocess][L3] 955: Removed stop words; 62 tokens remain.
[preprocess][L3] 955: Stemming complete.
[indexDocument][L3] Preprocessing complete for '955'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '955'.
[indexDocument][L3] Recorded doc length 62 for '955'.
[indexDocument][L3] Tokens added to the inverted index for '955'.
[indexDocument][L2] Indexing document '1289'.
[indexDocument][L3] Opened file '1289'.
[preprocess][L2] Starting preprocessing for '1289'.
[preprocess][L3] 1289: Tokenized into 245 tokens.
[preprocess][L3] 1289: Removed stop words; 160 tokens remain.
[preprocess][L3] 1289: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1289'; 160 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1289'.
[indexDocument][L3] Recorded doc length 160 for '1289'.
[indexDocument][L3] Tokens added to the inverted index for '1289'.
[indexDocument][L2] Indexing document '1242'.
[indexDocument][L3] Opened file '1242'.
[preprocess][L2] Starting preprocessing for '1242'.
[preprocess][L3] 1242: Tokenized into 242 tokens.
[preprocess][L3] 1242: Removed stop words; 132 tokens remain.
[preprocess][L3] 1242: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1242'; 132 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1242'.
[indexDocument][L3] Recorded doc length 132 for '1242'.
[indexDocument][L3] Tokens added to the inverted index for '1242'.
[indexDocument][L2] Indexing document '390'.
[indexDocument][L3] Opened file '390'.
[preprocess][L2] Starting preprocessing for '390'.
[preprocess][L3] 390: Tokenized into 123 tokens.
[preprocess][L3] 390: Removed stop words; 81 tokens remain.
[preprocess][L3] 390: Stemming complete.
[indexDocument][L3] Preprocessing complete for '390'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '390'.
[indexDocument][L3] Recorded doc length 81 for '390'.
[indexDocument][L3] Tokens added to the inverted index for '390'.
[indexDocument][L2] Indexing document '1070'.
[indexDocument][L3] Opened file '1070'.
[preprocess][L2] Starting preprocessing for '1070'.
[preprocess][L3] 1070: Tokenized into 100 tokens.
[preprocess][L3] 1070: Removed stop words; 73 tokens remain.
[preprocess][L3] 1070: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1070'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1070'.
[indexDocument][L3] Recorded doc length 73 for '1070'.
[indexDocument][L3] Tokens added to the inverted index for '1070'.
[indexDocument][L2] Indexing document '364'.
[indexDocument][L3] Opened file '364'.
[preprocess][L2] Starting preprocessing for '364'.
[preprocess][L3] 364: Tokenized into 368 tokens.
[preprocess][L3] 364: Removed stop words; 217 tokens remain.
[preprocess][L3] 364: Stemming complete.
[indexDocument][L3] Preprocessing complete for '364'; 217 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '364'.
[indexDocument][L3] Recorded doc length 217 for '364'.
[indexDocument][L3] Tokens added to the inverted index for '364'.
[indexDocument][L2] Indexing document '1084'.
[indexDocument][L3] Opened file '1084'.
[preprocess][L2] Starting preprocessing for '1084'.
[preprocess][L3] 1084: Tokenized into 64 tokens.
[preprocess][L3] 1084: Removed stop words; 40 tokens remain.
[preprocess][L3] 1084: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1084'; 40 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1084'.
[indexDocument][L3] Recorded doc length 40 for '1084'.
[indexDocument][L3] Tokens added to the inverted index for '1084'.
[indexDocument][L2] Indexing document '156'.
[indexDocument][L3] Opened file '156'.
[preprocess][L2] Starting preprocessing for '156'.
[preprocess][L3] 156: Tokenized into 274 tokens.
[preprocess][L3] 156: Removed stop words; 143 tokens remain.
[preprocess][L3] 156: Stemming complete.
[indexDocument][L3] Preprocessing complete for '156'; 143 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '156'.
[indexDocument][L3] Recorded doc length 143 for '156'.
[indexDocument][L3] Tokens added to the inverted index for '156'.
[indexDocument][L2] Indexing document '532'.
[indexDocument][L3] Opened file '532'.
[preprocess][L2] Starting preprocessing for '532'.
[preprocess][L3] 532: Tokenized into 58 tokens.
[preprocess][L3] 532: Removed stop words; 40 tokens remain.
[preprocess][L3] 532: Stemming complete.
[indexDocument][L3] Preprocessing complete for '532'; 40 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '532'.
[indexDocument][L3] Recorded doc length 40 for '532'.
[indexDocument][L3] Tokens added to the inverted index for '532'.
[indexDocument][L2] Indexing document '700'.
[indexDocument][L3] Opened file '700'.
[preprocess][L2] Starting preprocessing for '700'.
[preprocess][L3] 700: Tokenized into 113 tokens.
[preprocess][L3] 700: Removed stop words; 73 tokens remain.
[preprocess][L3] 700: Stemming complete.
[indexDocument][L3] Preprocessing complete for '700'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '700'.
[indexDocument][L3] Recorded doc length 73 for '700'.
[indexDocument][L3] Tokens added to the inverted index for '700'.
[indexDocument][L2] Indexing document '1079'.
[indexDocument][L3] Opened file '1079'.
[preprocess][L2] Starting preprocessing for '1079'.
[preprocess][L3] 1079: Tokenized into 97 tokens.
[preprocess][L3] 1079: Removed stop words; 56 tokens remain.
[preprocess][L3] 1079: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1079'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1079'.
[indexDocument][L3] Recorded doc length 56 for '1079'.
[indexDocument][L3] Tokens added to the inverted index for '1079'.
[indexDocument][L2] Indexing document '997'.
[indexDocument][L3] Opened file '997'.
[preprocess][L2] Starting preprocessing for '997'.
[preprocess][L3] 997: Tokenized into 193 tokens.
[preprocess][L3] 997: Removed stop words; 125 tokens remain.
[preprocess][L3] 997: Stemming complete.
[indexDocument][L3] Preprocessing complete for '997'; 125 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '997'.
[indexDocument][L3] Recorded doc length 125 for '997'.
[indexDocument][L3] Tokens added to the inverted index for '997'.
[indexDocument][L2] Indexing document '399'.
[indexDocument][L3] Opened file '399'.
[preprocess][L2] Starting preprocessing for '399'.
[preprocess][L3] 399: Tokenized into 66 tokens.
[preprocess][L3] 399: Removed stop words; 45 tokens remain.
[preprocess][L3] 399: Stemming complete.
[indexDocument][L3] Preprocessing complete for '399'; 45 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '399'.
[indexDocument][L3] Recorded doc length 45 for '399'.
[indexDocument][L3] Tokens added to the inverted index for '399'.
[indexDocument][L2] Indexing document '963'.
[indexDocument][L3] Opened file '963'.
[preprocess][L2] Starting preprocessing for '963'.
[preprocess][L3] 963: Tokenized into 60 tokens.
[preprocess][L3] 963: Removed stop words; 43 tokens remain.
[preprocess][L3] 963: Stemming complete.
[indexDocument][L3] Preprocessing complete for '963'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '963'.
[indexDocument][L3] Recorded doc length 43 for '963'.
[indexDocument][L3] Tokens added to the inverted index for '963'.
[indexDocument][L2] Indexing document '709'.
[indexDocument][L3] Opened file '709'.
[preprocess][L2] Starting preprocessing for '709'.
[preprocess][L3] 709: Tokenized into 213 tokens.
[preprocess][L3] 709: Removed stop words; 128 tokens remain.
[preprocess][L3] 709: Stemming complete.
[indexDocument][L3] Preprocessing complete for '709'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '709'.
[indexDocument][L3] Recorded doc length 128 for '709'.
[indexDocument][L3] Tokens added to the inverted index for '709'.
[indexDocument][L2] Indexing document '1274'.
[indexDocument][L3] Opened file '1274'.
[preprocess][L2] Starting preprocessing for '1274'.
[preprocess][L3] 1274: Tokenized into 246 tokens.
[preprocess][L3] 1274: Removed stop words; 152 tokens remain.
[preprocess][L3] 1274: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1274'; 152 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1274'.
[indexDocument][L3] Recorded doc length 152 for '1274'.
[indexDocument][L3] Tokens added to the inverted index for '1274'.
[indexDocument][L2] Indexing document '194'.
[indexDocument][L3] Opened file '194'.
[preprocess][L2] Starting preprocessing for '194'.
[preprocess][L3] 194: Tokenized into 70 tokens.
[preprocess][L3] 194: Removed stop words; 43 tokens remain.
[preprocess][L3] 194: Stemming complete.
[indexDocument][L3] Preprocessing complete for '194'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '194'.
[indexDocument][L3] Recorded doc length 43 for '194'.
[indexDocument][L3] Tokens added to the inverted index for '194'.
[indexDocument][L2] Indexing document '1046'.
[indexDocument][L3] Opened file '1046'.
[preprocess][L2] Starting preprocessing for '1046'.
[preprocess][L3] 1046: Tokenized into 82 tokens.
[preprocess][L3] 1046: Removed stop words; 47 tokens remain.
[preprocess][L3] 1046: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1046'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1046'.
[indexDocument][L3] Recorded doc length 47 for '1046'.
[indexDocument][L3] Tokens added to the inverted index for '1046'.
[indexDocument][L2] Indexing document '504'.
[indexDocument][L3] Opened file '504'.
[preprocess][L2] Starting preprocessing for '504'.
[preprocess][L3] 504: Tokenized into 205 tokens.
[preprocess][L3] 504: Removed stop words; 126 tokens remain.
[preprocess][L3] 504: Stemming complete.
[indexDocument][L3] Preprocessing complete for '504'; 126 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '504'.
[indexDocument][L3] Recorded doc length 126 for '504'.
[indexDocument][L3] Tokens added to the inverted index for '504'.
[indexDocument][L2] Indexing document '736'.
[indexDocument][L3] Opened file '736'.
[preprocess][L2] Starting preprocessing for '736'.
[preprocess][L3] 736: Tokenized into 100 tokens.
[preprocess][L3] 736: Removed stop words; 57 tokens remain.
[preprocess][L3] 736: Stemming complete.
[indexDocument][L3] Preprocessing complete for '736'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '736'.
[indexDocument][L3] Recorded doc length 57 for '736'.
[indexDocument][L3] Tokens added to the inverted index for '736'.
[indexDocument][L2] Indexing document '352'.
[indexDocument][L3] Opened file '352'.
[preprocess][L2] Starting preprocessing for '352'.
[preprocess][L3] 352: Tokenized into 210 tokens.
[preprocess][L3] 352: Removed stop words; 124 tokens remain.
[preprocess][L3] 352: Stemming complete.
[indexDocument][L3] Preprocessing complete for '352'; 124 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '352'.
[indexDocument][L3] Recorded doc length 124 for '352'.
[indexDocument][L3] Tokens added to the inverted index for '352'.
[indexDocument][L2] Indexing document '1280'.
[indexDocument][L3] Opened file '1280'.
[preprocess][L2] Starting preprocessing for '1280'.
[preprocess][L3] 1280: Tokenized into 259 tokens.
[preprocess][L3] 1280: Removed stop words; 152 tokens remain.
[preprocess][L3] 1280: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1280'; 152 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1280'.
[indexDocument][L3] Recorded doc length 152 for '1280'.
[indexDocument][L3] Tokens added to the inverted index for '1280'.
[indexDocument][L2] Indexing document '160'.
[indexDocument][L3] Opened file '160'.
[preprocess][L2] Starting preprocessing for '160'.
[preprocess][L3] 160: Tokenized into 371 tokens.
[preprocess][L3] 160: Removed stop words; 214 tokens remain.
[preprocess][L3] 160: Stemming complete.
[indexDocument][L3] Preprocessing complete for '160'; 214 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '160'.
[indexDocument][L3] Recorded doc length 214 for '160'.
[indexDocument][L3] Tokens added to the inverted index for '160'.
[indexDocument][L2] Indexing document '158'.
[indexDocument][L3] Opened file '158'.
[preprocess][L2] Starting preprocessing for '158'.
[preprocess][L3] 158: Tokenized into 144 tokens.
[preprocess][L3] 158: Removed stop words; 91 tokens remain.
[preprocess][L3] 158: Stemming complete.
[indexDocument][L3] Preprocessing complete for '158'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '158'.
[indexDocument][L3] Recorded doc length 91 for '158'.
[indexDocument][L3] Tokens added to the inverted index for '158'.
[indexDocument][L2] Indexing document '964'.
[indexDocument][L3] Opened file '964'.
[preprocess][L2] Starting preprocessing for '964'.
[preprocess][L3] 964: Tokenized into 151 tokens.
[preprocess][L3] 964: Removed stop words; 95 tokens remain.
[preprocess][L3] 964: Stemming complete.
[indexDocument][L3] Preprocessing complete for '964'; 95 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '964'.
[indexDocument][L3] Recorded doc length 95 for '964'.
[indexDocument][L3] Tokens added to the inverted index for '964'.
[indexDocument][L2] Indexing document '990'.
[indexDocument][L3] Opened file '990'.
[preprocess][L2] Starting preprocessing for '990'.
[preprocess][L3] 990: Tokenized into 180 tokens.
[preprocess][L3] 990: Removed stop words; 110 tokens remain.
[preprocess][L3] 990: Stemming complete.
[indexDocument][L3] Preprocessing complete for '990'; 110 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '990'.
[indexDocument][L3] Recorded doc length 110 for '990'.
[indexDocument][L3] Tokens added to the inverted index for '990'.
[indexDocument][L2] Indexing document '167'.
[indexDocument][L3] Opened file '167'.
[preprocess][L2] Starting preprocessing for '167'.
[preprocess][L3] 167: Tokenized into 132 tokens.
[preprocess][L3] 167: Removed stop words; 82 tokens remain.
[preprocess][L3] 167: Stemming complete.
[indexDocument][L3] Preprocessing complete for '167'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '167'.
[indexDocument][L3] Recorded doc length 82 for '167'.
[indexDocument][L3] Tokens added to the inverted index for '167'.
[indexDocument][L2] Indexing document '355'.
[indexDocument][L3] Opened file '355'.
[preprocess][L2] Starting preprocessing for '355'.
[preprocess][L3] 355: Tokenized into 104 tokens.
[preprocess][L3] 355: Removed stop words; 68 tokens remain.
[preprocess][L3] 355: Stemming complete.
[indexDocument][L3] Preprocessing complete for '355'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '355'.
[indexDocument][L3] Recorded doc length 68 for '355'.
[indexDocument][L3] Tokens added to the inverted index for '355'.
[indexDocument][L2] Indexing document '1287'.
[indexDocument][L3] Opened file '1287'.
[preprocess][L2] Starting preprocessing for '1287'.
[preprocess][L3] 1287: Tokenized into 115 tokens.
[preprocess][L3] 1287: Removed stop words; 61 tokens remain.
[preprocess][L3] 1287: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1287'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1287'.
[indexDocument][L3] Recorded doc length 61 for '1287'.
[indexDocument][L3] Tokens added to the inverted index for '1287'.
[indexDocument][L2] Indexing document '731'.
[indexDocument][L3] Opened file '731'.
[preprocess][L2] Starting preprocessing for '731'.
[preprocess][L3] 731: Tokenized into 280 tokens.
[preprocess][L3] 731: Removed stop words; 154 tokens remain.
[preprocess][L3] 731: Stemming complete.
[indexDocument][L3] Preprocessing complete for '731'; 154 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '731'.
[indexDocument][L3] Recorded doc length 154 for '731'.
[indexDocument][L3] Tokens added to the inverted index for '731'.
[indexDocument][L2] Indexing document '503'.
[indexDocument][L3] Opened file '503'.
[preprocess][L2] Starting preprocessing for '503'.
[preprocess][L3] 503: Tokenized into 110 tokens.
[preprocess][L3] 503: Removed stop words; 68 tokens remain.
[preprocess][L3] 503: Stemming complete.
[indexDocument][L3] Preprocessing complete for '503'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '503'.
[indexDocument][L3] Recorded doc length 68 for '503'.
[indexDocument][L3] Tokens added to the inverted index for '503'.
[indexDocument][L2] Indexing document '193'.
[indexDocument][L3] Opened file '193'.
[preprocess][L2] Starting preprocessing for '193'.
[preprocess][L3] 193: Tokenized into 402 tokens.
[preprocess][L3] 193: Removed stop words; 229 tokens remain.
[preprocess][L3] 193: Stemming complete.
[indexDocument][L3] Preprocessing complete for '193'; 229 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '193'.
[indexDocument][L3] Recorded doc length 229 for '193'.
[indexDocument][L3] Tokens added to the inverted index for '193'.
[indexDocument][L2] Indexing document '1041'.
[indexDocument][L3] Opened file '1041'.
[preprocess][L2] Starting preprocessing for '1041'.
[preprocess][L3] 1041: Tokenized into 139 tokens.
[preprocess][L3] 1041: Removed stop words; 92 tokens remain.
[preprocess][L3] 1041: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1041'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1041'.
[indexDocument][L3] Recorded doc length 92 for '1041'.
[indexDocument][L3] Tokens added to the inverted index for '1041'.
[indexDocument][L2] Indexing document '1273'.
[indexDocument][L3] Opened file '1273'.
[preprocess][L2] Starting preprocessing for '1273'.
[preprocess][L3] 1273: Tokenized into 172 tokens.
[preprocess][L3] 1273: Removed stop words; 94 tokens remain.
[preprocess][L3] 1273: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1273'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1273'.
[indexDocument][L3] Recorded doc length 94 for '1273'.
[indexDocument][L3] Tokens added to the inverted index for '1273'.
[indexDocument][L2] Indexing document '301'.
[indexDocument][L3] Opened file '301'.
[preprocess][L2] Starting preprocessing for '301'.
[preprocess][L3] 301: Tokenized into 67 tokens.
[preprocess][L3] 301: Removed stop words; 41 tokens remain.
[preprocess][L3] 301: Stemming complete.
[indexDocument][L3] Preprocessing complete for '301'; 41 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '301'.
[indexDocument][L3] Recorded doc length 41 for '301'.
[indexDocument][L3] Tokens added to the inverted index for '301'.
[indexDocument][L2] Indexing document '133'.
[indexDocument][L3] Opened file '133'.
[preprocess][L2] Starting preprocessing for '133'.
[preprocess][L3] 133: Tokenized into 216 tokens.
[preprocess][L3] 133: Removed stop words; 118 tokens remain.
[preprocess][L3] 133: Stemming complete.
[indexDocument][L3] Preprocessing complete for '133'; 118 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '133'.
[indexDocument][L3] Recorded doc length 118 for '133'.
[indexDocument][L3] Tokens added to the inverted index for '133'.
[indexDocument][L2] Indexing document '557'.
[indexDocument][L3] Opened file '557'.
[preprocess][L2] Starting preprocessing for '557'.
[preprocess][L3] 557: Tokenized into 135 tokens.
[preprocess][L3] 557: Removed stop words; 92 tokens remain.
[preprocess][L3] 557: Stemming complete.
[indexDocument][L3] Preprocessing complete for '557'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '557'.
[indexDocument][L3] Recorded doc length 92 for '557'.
[indexDocument][L3] Tokens added to the inverted index for '557'.
[indexDocument][L2] Indexing document '765'.
[indexDocument][L3] Opened file '765'.
[preprocess][L2] Starting preprocessing for '765'.
[preprocess][L3] 765: Tokenized into 148 tokens.
[preprocess][L3] 765: Removed stop words; 92 tokens remain.
[preprocess][L3] 765: Stemming complete.
[indexDocument][L3] Preprocessing complete for '765'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '765'.
[indexDocument][L3] Recorded doc length 92 for '765'.
[indexDocument][L3] Tokens added to the inverted index for '765'.
[indexDocument][L2] Indexing document '791'.
[indexDocument][L3] Opened file '791'.
[preprocess][L2] Starting preprocessing for '791'.
[preprocess][L3] 791: Tokenized into 94 tokens.
[preprocess][L3] 791: Removed stop words; 64 tokens remain.
[preprocess][L3] 791: Stemming complete.
[indexDocument][L3] Preprocessing complete for '791'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '791'.
[indexDocument][L3] Recorded doc length 64 for '791'.
[indexDocument][L3] Tokens added to the inverted index for '791'.
[indexDocument][L2] Indexing document '1227'.
[indexDocument][L3] Opened file '1227'.
[preprocess][L2] Starting preprocessing for '1227'.
[preprocess][L3] 1227: Tokenized into 93 tokens.
[preprocess][L3] 1227: Removed stop words; 54 tokens remain.
[preprocess][L3] 1227: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1227'; 54 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1227'.
[indexDocument][L3] Recorded doc length 54 for '1227'.
[indexDocument][L3] Tokens added to the inverted index for '1227'.
[indexDocument][L2] Indexing document '1015'.
[indexDocument][L3] Opened file '1015'.
[preprocess][L2] Starting preprocessing for '1015'.
[preprocess][L3] 1015: Tokenized into 147 tokens.
[preprocess][L3] 1015: Removed stop words; 85 tokens remain.
[preprocess][L3] 1015: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1015'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1015'.
[indexDocument][L3] Recorded doc length 85 for '1015'.
[indexDocument][L3] Tokens added to the inverted index for '1015'.
[indexDocument][L2] Indexing document '568'.
[indexDocument][L3] Opened file '568'.
[preprocess][L2] Starting preprocessing for '568'.
[preprocess][L3] 568: Tokenized into 133 tokens.
[preprocess][L3] 568: Removed stop words; 79 tokens remain.
[preprocess][L3] 568: Stemming complete.
[indexDocument][L3] Preprocessing complete for '568'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '568'.
[indexDocument][L3] Recorded doc length 79 for '568'.
[indexDocument][L3] Tokens added to the inverted index for '568'.
[indexDocument][L2] Indexing document '930'.
[indexDocument][L3] Opened file '930'.
[preprocess][L2] Starting preprocessing for '930'.
[preprocess][L3] 930: Tokenized into 144 tokens.
[preprocess][L3] 930: Removed stop words; 81 tokens remain.
[preprocess][L3] 930: Stemming complete.
[indexDocument][L3] Preprocessing complete for '930'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '930'.
[indexDocument][L3] Recorded doc length 81 for '930'.
[indexDocument][L3] Tokens added to the inverted index for '930'.
[indexDocument][L2] Indexing document '1218'.
[indexDocument][L3] Opened file '1218'.
[preprocess][L2] Starting preprocessing for '1218'.
[preprocess][L3] 1218: Tokenized into 272 tokens.
[preprocess][L3] 1218: Removed stop words; 181 tokens remain.
[preprocess][L3] 1218: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1218'; 181 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1218'.
[indexDocument][L3] Recorded doc length 181 for '1218'.
[indexDocument][L3] Tokens added to the inverted index for '1218'.
[indexDocument][L2] Indexing document '1012'.
[indexDocument][L3] Opened file '1012'.
[preprocess][L2] Starting preprocessing for '1012'.
[preprocess][L3] 1012: Tokenized into 140 tokens.
[preprocess][L3] 1012: Removed stop words; 84 tokens remain.
[preprocess][L3] 1012: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1012'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1012'.
[indexDocument][L3] Recorded doc length 84 for '1012'.
[indexDocument][L3] Tokens added to the inverted index for '1012'.
[indexDocument][L2] Indexing document '1220'.
[indexDocument][L3] Opened file '1220'.
[preprocess][L2] Starting preprocessing for '1220'.
[preprocess][L3] 1220: Tokenized into 225 tokens.
[preprocess][L3] 1220: Removed stop words; 126 tokens remain.
[preprocess][L3] 1220: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1220'; 126 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1220'.
[indexDocument][L3] Recorded doc length 126 for '1220'.
[indexDocument][L3] Tokens added to the inverted index for '1220'.
[indexDocument][L2] Indexing document '796'.
[indexDocument][L3] Opened file '796'.
[preprocess][L2] Starting preprocessing for '796'.
[preprocess][L3] 796: Tokenized into 197 tokens.
[preprocess][L3] 796: Removed stop words; 115 tokens remain.
[preprocess][L3] 796: Stemming complete.
[indexDocument][L3] Preprocessing complete for '796'; 115 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '796'.
[indexDocument][L3] Recorded doc length 115 for '796'.
[indexDocument][L3] Tokens added to the inverted index for '796'.
[indexDocument][L2] Indexing document '762'.
[indexDocument][L3] Opened file '762'.
[preprocess][L2] Starting preprocessing for '762'.
[preprocess][L3] 762: Tokenized into 178 tokens.
[preprocess][L3] 762: Removed stop words; 111 tokens remain.
[preprocess][L3] 762: Stemming complete.
[indexDocument][L3] Preprocessing complete for '762'; 111 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '762'.
[indexDocument][L3] Recorded doc length 111 for '762'.
[indexDocument][L3] Tokens added to the inverted index for '762'.
[indexDocument][L2] Indexing document '550'.
[indexDocument][L3] Opened file '550'.
[preprocess][L2] Starting preprocessing for '550'.
[preprocess][L3] 550: Tokenized into 122 tokens.
[preprocess][L3] 550: Removed stop words; 76 tokens remain.
[preprocess][L3] 550: Stemming complete.
[indexDocument][L3] Preprocessing complete for '550'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '550'.
[indexDocument][L3] Recorded doc length 76 for '550'.
[indexDocument][L3] Tokens added to the inverted index for '550'.
[indexDocument][L2] Indexing document '908'.
[indexDocument][L3] Opened file '908'.
[preprocess][L2] Starting preprocessing for '908'.
[preprocess][L3] 908: Tokenized into 192 tokens.
[preprocess][L3] 908: Removed stop words; 113 tokens remain.
[preprocess][L3] 908: Stemming complete.
[indexDocument][L3] Preprocessing complete for '908'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '908'.
[indexDocument][L3] Recorded doc length 113 for '908'.
[indexDocument][L3] Tokens added to the inverted index for '908'.
[indexDocument][L2] Indexing document '134'.
[indexDocument][L3] Opened file '134'.
[preprocess][L2] Starting preprocessing for '134'.
[preprocess][L3] 134: Tokenized into 218 tokens.
[preprocess][L3] 134: Removed stop words; 122 tokens remain.
[preprocess][L3] 134: Stemming complete.
[indexDocument][L3] Preprocessing complete for '134'; 122 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '134'.
[indexDocument][L3] Recorded doc length 122 for '134'.
[indexDocument][L3] Tokens added to the inverted index for '134'.
[indexDocument][L2] Indexing document '306'.
[indexDocument][L3] Opened file '306'.
[preprocess][L2] Starting preprocessing for '306'.
[preprocess][L3] 306: Tokenized into 155 tokens.
[preprocess][L3] 306: Removed stop words; 91 tokens remain.
[preprocess][L3] 306: Stemming complete.
[indexDocument][L3] Preprocessing complete for '306'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '306'.
[indexDocument][L3] Recorded doc length 91 for '306'.
[indexDocument][L3] Tokens added to the inverted index for '306'.
[indexDocument][L2] Indexing document '339'.
[indexDocument][L3] Opened file '339'.
[preprocess][L2] Starting preprocessing for '339'.
[preprocess][L3] 339: Tokenized into 62 tokens.
[preprocess][L3] 339: Removed stop words; 42 tokens remain.
[preprocess][L3] 339: Stemming complete.
[indexDocument][L3] Preprocessing complete for '339'; 42 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '339'.
[indexDocument][L3] Recorded doc length 42 for '339'.
[indexDocument][L3] Tokens added to the inverted index for '339'.
[indexDocument][L2] Indexing document '937'.
[indexDocument][L3] Opened file '937'.
[preprocess][L2] Starting preprocessing for '937'.
[preprocess][L3] 937: Tokenized into 135 tokens.
[preprocess][L3] 937: Removed stop words; 74 tokens remain.
[preprocess][L3] 937: Stemming complete.
[indexDocument][L3] Preprocessing complete for '937'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '937'.
[indexDocument][L3] Recorded doc length 74 for '937'.
[indexDocument][L3] Tokens added to the inverted index for '937'.
[indexDocument][L2] Indexing document '592'.
[indexDocument][L3] Opened file '592'.
[preprocess][L2] Starting preprocessing for '592'.
[preprocess][L3] 592: Tokenized into 75 tokens.
[preprocess][L3] 592: Removed stop words; 57 tokens remain.
[preprocess][L3] 592: Stemming complete.
[indexDocument][L3] Preprocessing complete for '592'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '592'.
[indexDocument][L3] Recorded doc length 57 for '592'.
[indexDocument][L3] Tokens added to the inverted index for '592'.
[indexDocument][L2] Indexing document '1024'.
[indexDocument][L3] Opened file '1024'.
[preprocess][L2] Starting preprocessing for '1024'.
[preprocess][L3] 1024: Tokenized into 199 tokens.
[preprocess][L3] 1024: Removed stop words; 113 tokens remain.
[preprocess][L3] 1024: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1024'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1024'.
[indexDocument][L3] Recorded doc length 113 for '1024'.
[indexDocument][L3] Tokens added to the inverted index for '1024'.
[indexDocument][L2] Indexing document '1216'.
[indexDocument][L3] Opened file '1216'.
[preprocess][L2] Starting preprocessing for '1216'.
[preprocess][L3] 1216: Tokenized into 207 tokens.
[preprocess][L3] 1216: Removed stop words; 121 tokens remain.
[preprocess][L3] 1216: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1216'; 121 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1216'.
[indexDocument][L3] Recorded doc length 121 for '1216'.
[indexDocument][L3] Tokens added to the inverted index for '1216'.
[indexDocument][L2] Indexing document '102'.
[indexDocument][L3] Opened file '102'.
[preprocess][L2] Starting preprocessing for '102'.
[preprocess][L3] 102: Tokenized into 85 tokens.
[preprocess][L3] 102: Removed stop words; 54 tokens remain.
[preprocess][L3] 102: Stemming complete.
[indexDocument][L3] Preprocessing complete for '102'; 54 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '102'.
[indexDocument][L3] Recorded doc length 54 for '102'.
[indexDocument][L3] Tokens added to the inverted index for '102'.
[indexDocument][L2] Indexing document '330'.
[indexDocument][L3] Opened file '330'.
[preprocess][L2] Starting preprocessing for '330'.
[preprocess][L3] 330: Tokenized into 113 tokens.
[preprocess][L3] 330: Removed stop words; 71 tokens remain.
[preprocess][L3] 330: Stemming complete.
[indexDocument][L3] Preprocessing complete for '330'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '330'.
[indexDocument][L3] Recorded doc length 71 for '330'.
[indexDocument][L3] Tokens added to the inverted index for '330'.
[indexDocument][L2] Indexing document '754'.
[indexDocument][L3] Opened file '754'.
[preprocess][L2] Starting preprocessing for '754'.
[preprocess][L3] 754: Tokenized into 167 tokens.
[preprocess][L3] 754: Removed stop words; 99 tokens remain.
[preprocess][L3] 754: Stemming complete.
[indexDocument][L3] Preprocessing complete for '754'; 99 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '754'.
[indexDocument][L3] Recorded doc length 99 for '754'.
[indexDocument][L3] Tokens added to the inverted index for '754'.
[indexDocument][L2] Indexing document '566'.
[indexDocument][L3] Opened file '566'.
[preprocess][L2] Starting preprocessing for '566'.
[preprocess][L3] 566: Tokenized into 181 tokens.
[preprocess][L3] 566: Removed stop words; 114 tokens remain.
[preprocess][L3] 566: Stemming complete.
[indexDocument][L3] Preprocessing complete for '566'; 114 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '566'.
[indexDocument][L3] Recorded doc length 114 for '566'.
[indexDocument][L3] Tokens added to the inverted index for '566'.
[indexDocument][L2] Indexing document '1229'.
[indexDocument][L3] Opened file '1229'.
[preprocess][L2] Starting preprocessing for '1229'.
[preprocess][L3] 1229: Tokenized into 301 tokens.
[preprocess][L3] 1229: Removed stop words; 183 tokens remain.
[preprocess][L3] 1229: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1229'; 183 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1229'.
[indexDocument][L3] Recorded doc length 183 for '1229'.
[indexDocument][L3] Tokens added to the inverted index for '1229'.
[indexDocument][L2] Indexing document '559'.
[indexDocument][L3] Opened file '559'.
[preprocess][L2] Starting preprocessing for '559'.
[preprocess][L3] 559: Tokenized into 98 tokens.
[preprocess][L3] 559: Removed stop words; 63 tokens remain.
[preprocess][L3] 559: Stemming complete.
[indexDocument][L3] Preprocessing complete for '559'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '559'.
[indexDocument][L3] Recorded doc length 63 for '559'.
[indexDocument][L3] Tokens added to the inverted index for '559'.
[indexDocument][L2] Indexing document '901'.
[indexDocument][L3] Opened file '901'.
[preprocess][L2] Starting preprocessing for '901'.
[preprocess][L3] 901: Tokenized into 138 tokens.
[preprocess][L3] 901: Removed stop words; 85 tokens remain.
[preprocess][L3] 901: Stemming complete.
[indexDocument][L3] Preprocessing complete for '901'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '901'.
[indexDocument][L3] Recorded doc length 85 for '901'.
[indexDocument][L3] Tokens added to the inverted index for '901'.
[indexDocument][L2] Indexing document '561'.
[indexDocument][L3] Opened file '561'.
[preprocess][L2] Starting preprocessing for '561'.
[preprocess][L3] 561: Tokenized into 227 tokens.
[preprocess][L3] 561: Removed stop words; 131 tokens remain.
[preprocess][L3] 561: Stemming complete.
[indexDocument][L3] Preprocessing complete for '561'; 131 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '561'.
[indexDocument][L3] Recorded doc length 131 for '561'.
[indexDocument][L3] Tokens added to the inverted index for '561'.
[indexDocument][L2] Indexing document '753'.
[indexDocument][L3] Opened file '753'.
[preprocess][L2] Starting preprocessing for '753'.
[preprocess][L3] 753: Tokenized into 225 tokens.
[preprocess][L3] 753: Removed stop words; 146 tokens remain.
[preprocess][L3] 753: Stemming complete.
[indexDocument][L3] Preprocessing complete for '753'; 146 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '753'.
[indexDocument][L3] Recorded doc length 146 for '753'.
[indexDocument][L3] Tokens added to the inverted index for '753'.
[indexDocument][L2] Indexing document '337'.
[indexDocument][L3] Opened file '337'.
[preprocess][L2] Starting preprocessing for '337'.
[preprocess][L3] 337: Tokenized into 97 tokens.
[preprocess][L3] 337: Removed stop words; 62 tokens remain.
[preprocess][L3] 337: Stemming complete.
[indexDocument][L3] Preprocessing complete for '337'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '337'.
[indexDocument][L3] Recorded doc length 62 for '337'.
[indexDocument][L3] Tokens added to the inverted index for '337'.
[indexDocument][L2] Indexing document '939'.
[indexDocument][L3] Opened file '939'.
[preprocess][L2] Starting preprocessing for '939'.
[preprocess][L3] 939: Tokenized into 110 tokens.
[preprocess][L3] 939: Removed stop words; 63 tokens remain.
[preprocess][L3] 939: Stemming complete.
[indexDocument][L3] Preprocessing complete for '939'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '939'.
[indexDocument][L3] Recorded doc length 63 for '939'.
[indexDocument][L3] Tokens added to the inverted index for '939'.
[indexDocument][L2] Indexing document '105'.
[indexDocument][L3] Opened file '105'.
[preprocess][L2] Starting preprocessing for '105'.
[preprocess][L3] 105: Tokenized into 178 tokens.
[preprocess][L3] 105: Removed stop words; 93 tokens remain.
[preprocess][L3] 105: Stemming complete.
[indexDocument][L3] Preprocessing complete for '105'; 93 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '105'.
[indexDocument][L3] Recorded doc length 93 for '105'.
[indexDocument][L3] Tokens added to the inverted index for '105'.
[indexDocument][L2] Indexing document '1211'.
[indexDocument][L3] Opened file '1211'.
[preprocess][L2] Starting preprocessing for '1211'.
[preprocess][L3] 1211: Tokenized into 136 tokens.
[preprocess][L3] 1211: Removed stop words; 82 tokens remain.
[preprocess][L3] 1211: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1211'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1211'.
[indexDocument][L3] Recorded doc length 82 for '1211'.
[indexDocument][L3] Tokens added to the inverted index for '1211'.
[indexDocument][L2] Indexing document '1023'.
[indexDocument][L3] Opened file '1023'.
[preprocess][L2] Starting preprocessing for '1023'.
[preprocess][L3] 1023: Tokenized into 66 tokens.
[preprocess][L3] 1023: Removed stop words; 35 tokens remain.
[preprocess][L3] 1023: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1023'; 35 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1023'.
[indexDocument][L3] Recorded doc length 35 for '1023'.
[indexDocument][L3] Tokens added to the inverted index for '1023'.
[indexDocument][L2] Indexing document '595'.
[indexDocument][L3] Opened file '595'.
[preprocess][L2] Starting preprocessing for '595'.
[preprocess][L3] 595: Tokenized into 231 tokens.
[preprocess][L3] 595: Removed stop words; 133 tokens remain.
[preprocess][L3] 595: Stemming complete.
[indexDocument][L3] Preprocessing complete for '595'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '595'.
[indexDocument][L3] Recorded doc length 133 for '595'.
[indexDocument][L3] Tokens added to the inverted index for '595'.
[indexDocument][L2] Indexing document '906'.
[indexDocument][L3] Opened file '906'.
[preprocess][L2] Starting preprocessing for '906'.
[preprocess][L3] 906: Tokenized into 66 tokens.
[preprocess][L3] 906: Removed stop words; 39 tokens remain.
[preprocess][L3] 906: Stemming complete.
[indexDocument][L3] Preprocessing complete for '906'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '906'.
[indexDocument][L3] Recorded doc length 39 for '906'.
[indexDocument][L3] Tokens added to the inverted index for '906'.
[indexDocument][L2] Indexing document '308'.
[indexDocument][L3] Opened file '308'.
[preprocess][L2] Starting preprocessing for '308'.
[preprocess][L3] 308: Tokenized into 181 tokens.
[preprocess][L3] 308: Removed stop words; 96 tokens remain.
[preprocess][L3] 308: Stemming complete.
[indexDocument][L3] Preprocessing complete for '308'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '308'.
[indexDocument][L3] Recorded doc length 96 for '308'.
[indexDocument][L3] Tokens added to the inverted index for '308'.
[indexDocument][L2] Indexing document '798'.
[indexDocument][L3] Opened file '798'.
[preprocess][L2] Starting preprocessing for '798'.
[preprocess][L3] 798: Tokenized into 658 tokens.
[preprocess][L3] 798: Removed stop words; 348 tokens remain.
[preprocess][L3] 798: Stemming complete.
[indexDocument][L3] Preprocessing complete for '798'; 348 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '798'.
[indexDocument][L3] Recorded doc length 348 for '798'.
[indexDocument][L3] Tokens added to the inverted index for '798'.
[indexDocument][L2] Indexing document '991'.
[indexDocument][L3] Opened file '991'.
[preprocess][L2] Starting preprocessing for '991'.
[preprocess][L3] 991: Tokenized into 252 tokens.
[preprocess][L3] 991: Removed stop words; 137 tokens remain.
[preprocess][L3] 991: Stemming complete.
[indexDocument][L3] Preprocessing complete for '991'; 137 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '991'.
[indexDocument][L3] Recorded doc length 137 for '991'.
[indexDocument][L3] Tokens added to the inverted index for '991'.
[indexDocument][L2] Indexing document '965'.
[indexDocument][L3] Opened file '965'.
[preprocess][L2] Starting preprocessing for '965'.
[preprocess][L3] 965: Tokenized into 93 tokens.
[preprocess][L3] 965: Removed stop words; 54 tokens remain.
[preprocess][L3] 965: Stemming complete.
[indexDocument][L3] Preprocessing complete for '965'; 54 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '965'.
[indexDocument][L3] Recorded doc length 54 for '965'.
[indexDocument][L3] Tokens added to the inverted index for '965'.
[indexDocument][L2] Indexing document '159'.
[indexDocument][L3] Opened file '159'.
[preprocess][L2] Starting preprocessing for '159'.
[preprocess][L3] 159: Tokenized into 107 tokens.
[preprocess][L3] 159: Removed stop words; 63 tokens remain.
[preprocess][L3] 159: Stemming complete.
[indexDocument][L3] Preprocessing complete for '159'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '159'.
[indexDocument][L3] Recorded doc length 63 for '159'.
[indexDocument][L3] Tokens added to the inverted index for '159'.
[indexDocument][L2] Indexing document '1272'.
[indexDocument][L3] Opened file '1272'.
[preprocess][L2] Starting preprocessing for '1272'.
[preprocess][L3] 1272: Tokenized into 102 tokens.
[preprocess][L3] 1272: Removed stop words; 67 tokens remain.
[preprocess][L3] 1272: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1272'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1272'.
[indexDocument][L3] Recorded doc length 67 for '1272'.
[indexDocument][L3] Tokens added to the inverted index for '1272'.
[indexDocument][L2] Indexing document '192'.
[indexDocument][L3] Opened file '192'.
[preprocess][L2] Starting preprocessing for '192'.
[preprocess][L3] 192: Tokenized into 210 tokens.
[preprocess][L3] 192: Removed stop words; 116 tokens remain.
[preprocess][L3] 192: Stemming complete.
[indexDocument][L3] Preprocessing complete for '192'; 116 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '192'.
[indexDocument][L3] Recorded doc length 116 for '192'.
[indexDocument][L3] Tokens added to the inverted index for '192'.
[indexDocument][L2] Indexing document '1040'.
[indexDocument][L3] Opened file '1040'.
[preprocess][L2] Starting preprocessing for '1040'.
[preprocess][L3] 1040: Tokenized into 509 tokens.
[preprocess][L3] 1040: Removed stop words; 304 tokens remain.
[preprocess][L3] 1040: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1040'; 304 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1040'.
[indexDocument][L3] Recorded doc length 304 for '1040'.
[indexDocument][L3] Tokens added to the inverted index for '1040'.
[indexDocument][L2] Indexing document '354'.
[indexDocument][L3] Opened file '354'.
[preprocess][L2] Starting preprocessing for '354'.
[preprocess][L3] 354: Tokenized into 136 tokens.
[preprocess][L3] 354: Removed stop words; 81 tokens remain.
[preprocess][L3] 354: Stemming complete.
[indexDocument][L3] Preprocessing complete for '354'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '354'.
[indexDocument][L3] Recorded doc length 81 for '354'.
[indexDocument][L3] Tokens added to the inverted index for '354'.
[indexDocument][L2] Indexing document '1286'.
[indexDocument][L3] Opened file '1286'.
[preprocess][L2] Starting preprocessing for '1286'.
[preprocess][L3] 1286: Tokenized into 102 tokens.
[preprocess][L3] 1286: Removed stop words; 63 tokens remain.
[preprocess][L3] 1286: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1286'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1286'.
[indexDocument][L3] Recorded doc length 63 for '1286'.
[indexDocument][L3] Tokens added to the inverted index for '1286'.
[indexDocument][L2] Indexing document '166'.
[indexDocument][L3] Opened file '166'.
[preprocess][L2] Starting preprocessing for '166'.
[preprocess][L3] 166: Tokenized into 194 tokens.
[preprocess][L3] 166: Removed stop words; 104 tokens remain.
[preprocess][L3] 166: Stemming complete.
[indexDocument][L3] Preprocessing complete for '166'; 104 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '166'.
[indexDocument][L3] Recorded doc length 104 for '166'.
[indexDocument][L3] Tokens added to the inverted index for '166'.
[indexDocument][L2] Indexing document '502'.
[indexDocument][L3] Opened file '502'.
[preprocess][L2] Starting preprocessing for '502'.
[preprocess][L3] 502: Tokenized into 55 tokens.
[preprocess][L3] 502: Removed stop words; 35 tokens remain.
[preprocess][L3] 502: Stemming complete.
[indexDocument][L3] Preprocessing complete for '502'; 35 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '502'.
[indexDocument][L3] Recorded doc length 35 for '502'.
[indexDocument][L3] Tokens added to the inverted index for '502'.
[indexDocument][L2] Indexing document '730'.
[indexDocument][L3] Opened file '730'.
[preprocess][L2] Starting preprocessing for '730'.
[preprocess][L3] 730: Tokenized into 264 tokens.
[preprocess][L3] 730: Removed stop words; 145 tokens remain.
[preprocess][L3] 730: Stemming complete.
[indexDocument][L3] Preprocessing complete for '730'; 145 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '730'.
[indexDocument][L3] Recorded doc length 145 for '730'.
[indexDocument][L3] Tokens added to the inverted index for '730'.
[indexDocument][L2] Indexing document '962'.
[indexDocument][L3] Opened file '962'.
[preprocess][L2] Starting preprocessing for '962'.
[preprocess][L3] 962: Tokenized into 439 tokens.
[preprocess][L3] 962: Removed stop words; 247 tokens remain.
[preprocess][L3] 962: Stemming complete.
[indexDocument][L3] Preprocessing complete for '962'; 247 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '962'.
[indexDocument][L3] Recorded doc length 247 for '962'.
[indexDocument][L3] Tokens added to the inverted index for '962'.
[indexDocument][L2] Indexing document '708'.
[indexDocument][L3] Opened file '708'.
[preprocess][L2] Starting preprocessing for '708'.
[preprocess][L3] 708: Tokenized into 79 tokens.
[preprocess][L3] 708: Removed stop words; 54 tokens remain.
[preprocess][L3] 708: Stemming complete.
[indexDocument][L3] Preprocessing complete for '708'; 54 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '708'.
[indexDocument][L3] Recorded doc length 54 for '708'.
[indexDocument][L3] Tokens added to the inverted index for '708'.
[indexDocument][L2] Indexing document '398'.
[indexDocument][L3] Opened file '398'.
[preprocess][L2] Starting preprocessing for '398'.
[preprocess][L3] 398: Tokenized into 61 tokens.
[preprocess][L3] 398: Removed stop words; 43 tokens remain.
[preprocess][L3] 398: Stemming complete.
[indexDocument][L3] Preprocessing complete for '398'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '398'.
[indexDocument][L3] Recorded doc length 43 for '398'.
[indexDocument][L3] Tokens added to the inverted index for '398'.
[indexDocument][L2] Indexing document '996'.
[indexDocument][L3] Opened file '996'.
[preprocess][L2] Starting preprocessing for '996'.
[preprocess][L3] 996: Tokenized into 295 tokens.
[preprocess][L3] 996: Removed stop words; 184 tokens remain.
[preprocess][L3] 996: Stemming complete.
[indexDocument][L3] Preprocessing complete for '996'; 184 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '996'.
[indexDocument][L3] Recorded doc length 184 for '996'.
[indexDocument][L3] Tokens added to the inverted index for '996'.
[indexDocument][L2] Indexing document '1078'.
[indexDocument][L3] Opened file '1078'.
[preprocess][L2] Starting preprocessing for '1078'.
[preprocess][L3] 1078: Tokenized into 154 tokens.
[preprocess][L3] 1078: Removed stop words; 90 tokens remain.
[preprocess][L3] 1078: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1078'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1078'.
[indexDocument][L3] Recorded doc length 90 for '1078'.
[indexDocument][L3] Tokens added to the inverted index for '1078'.
[indexDocument][L2] Indexing document '737'.
[indexDocument][L3] Opened file '737'.
[preprocess][L2] Starting preprocessing for '737'.
[preprocess][L3] 737: Tokenized into 104 tokens.
[preprocess][L3] 737: Removed stop words; 65 tokens remain.
[preprocess][L3] 737: Stemming complete.
[indexDocument][L3] Preprocessing complete for '737'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '737'.
[indexDocument][L3] Recorded doc length 65 for '737'.
[indexDocument][L3] Tokens added to the inverted index for '737'.
[indexDocument][L2] Indexing document '505'.
[indexDocument][L3] Opened file '505'.
[preprocess][L2] Starting preprocessing for '505'.
[preprocess][L3] 505: Tokenized into 95 tokens.
[preprocess][L3] 505: Removed stop words; 61 tokens remain.
[preprocess][L3] 505: Stemming complete.
[indexDocument][L3] Preprocessing complete for '505'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '505'.
[indexDocument][L3] Recorded doc length 61 for '505'.
[indexDocument][L3] Tokens added to the inverted index for '505'.
[indexDocument][L2] Indexing document '161'.
[indexDocument][L3] Opened file '161'.
[preprocess][L2] Starting preprocessing for '161'.
[preprocess][L3] 161: Tokenized into 69 tokens.
[preprocess][L3] 161: Removed stop words; 47 tokens remain.
[preprocess][L3] 161: Stemming complete.
[indexDocument][L3] Preprocessing complete for '161'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '161'.
[indexDocument][L3] Recorded doc length 47 for '161'.
[indexDocument][L3] Tokens added to the inverted index for '161'.
[indexDocument][L2] Indexing document '353'.
[indexDocument][L3] Opened file '353'.
[preprocess][L2] Starting preprocessing for '353'.
[preprocess][L3] 353: Tokenized into 171 tokens.
[preprocess][L3] 353: Removed stop words; 97 tokens remain.
[preprocess][L3] 353: Stemming complete.
[indexDocument][L3] Preprocessing complete for '353'; 97 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '353'.
[indexDocument][L3] Recorded doc length 97 for '353'.
[indexDocument][L3] Tokens added to the inverted index for '353'.
[indexDocument][L2] Indexing document '1281'.
[indexDocument][L3] Opened file '1281'.
[preprocess][L2] Starting preprocessing for '1281'.
[preprocess][L3] 1281: Tokenized into 217 tokens.
[preprocess][L3] 1281: Removed stop words; 128 tokens remain.
[preprocess][L3] 1281: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1281'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1281'.
[indexDocument][L3] Recorded doc length 128 for '1281'.
[indexDocument][L3] Tokens added to the inverted index for '1281'.
[indexDocument][L2] Indexing document '195'.
[indexDocument][L3] Opened file '195'.
[preprocess][L2] Starting preprocessing for '195'.
[preprocess][L3] 195: Tokenized into 174 tokens.
[preprocess][L3] 195: Removed stop words; 105 tokens remain.
[preprocess][L3] 195: Stemming complete.
[indexDocument][L3] Preprocessing complete for '195'; 105 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '195'.
[indexDocument][L3] Recorded doc length 105 for '195'.
[indexDocument][L3] Tokens added to the inverted index for '195'.
[indexDocument][L2] Indexing document '1047'.
[indexDocument][L3] Opened file '1047'.
[preprocess][L2] Starting preprocessing for '1047'.
[preprocess][L3] 1047: Tokenized into 409 tokens.
[preprocess][L3] 1047: Removed stop words; 221 tokens remain.
[preprocess][L3] 1047: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1047'; 221 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1047'.
[indexDocument][L3] Recorded doc length 221 for '1047'.
[indexDocument][L3] Tokens added to the inverted index for '1047'.
[indexDocument][L2] Indexing document '1275'.
[indexDocument][L3] Opened file '1275'.
[preprocess][L2] Starting preprocessing for '1275'.
[preprocess][L3] 1275: Tokenized into 108 tokens.
[preprocess][L3] 1275: Removed stop words; 66 tokens remain.
[preprocess][L3] 1275: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1275'; 66 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1275'.
[indexDocument][L3] Recorded doc length 66 for '1275'.
[indexDocument][L3] Tokens added to the inverted index for '1275'.
[indexDocument][L2] Indexing document '1288'.
[indexDocument][L3] Opened file '1288'.
[preprocess][L2] Starting preprocessing for '1288'.
[preprocess][L3] 1288: Tokenized into 121 tokens.
[preprocess][L3] 1288: Removed stop words; 65 tokens remain.
[preprocess][L3] 1288: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1288'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1288'.
[indexDocument][L3] Recorded doc length 65 for '1288'.
[indexDocument][L3] Tokens added to the inverted index for '1288'.
[indexDocument][L2] Indexing document '954'.
[indexDocument][L3] Opened file '954'.
[preprocess][L2] Starting preprocessing for '954'.
[preprocess][L3] 954: Tokenized into 93 tokens.
[preprocess][L3] 954: Removed stop words; 58 tokens remain.
[preprocess][L3] 954: Stemming complete.
[indexDocument][L3] Preprocessing complete for '954'; 58 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '954'.
[indexDocument][L3] Recorded doc length 58 for '954'.
[indexDocument][L3] Tokens added to the inverted index for '954'.
[indexDocument][L2] Indexing document '168'.
[indexDocument][L3] Opened file '168'.
[preprocess][L2] Starting preprocessing for '168'.
[preprocess][L3] 168: Tokenized into 252 tokens.
[preprocess][L3] 168: Removed stop words; 143 tokens remain.
[preprocess][L3] 168: Stemming complete.
[indexDocument][L3] Preprocessing complete for '168'; 143 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '168'.
[indexDocument][L3] Recorded doc length 143 for '168'.
[indexDocument][L3] Tokens added to the inverted index for '168'.
[indexDocument][L2] Indexing document '1085'.
[indexDocument][L3] Opened file '1085'.
[preprocess][L2] Starting preprocessing for '1085'.
[preprocess][L3] 1085: Tokenized into 86 tokens.
[preprocess][L3] 1085: Removed stop words; 51 tokens remain.
[preprocess][L3] 1085: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1085'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1085'.
[indexDocument][L3] Recorded doc length 51 for '1085'.
[indexDocument][L3] Tokens added to the inverted index for '1085'.
[indexDocument][L2] Indexing document '157'.
[indexDocument][L3] Opened file '157'.
[preprocess][L2] Starting preprocessing for '157'.
[preprocess][L3] 157: Tokenized into 251 tokens.
[preprocess][L3] 157: Removed stop words; 134 tokens remain.
[preprocess][L3] 157: Stemming complete.
[indexDocument][L3] Preprocessing complete for '157'; 134 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '157'.
[indexDocument][L3] Recorded doc length 134 for '157'.
[indexDocument][L3] Tokens added to the inverted index for '157'.
[indexDocument][L2] Indexing document '365'.
[indexDocument][L3] Opened file '365'.
[preprocess][L2] Starting preprocessing for '365'.
[preprocess][L3] 365: Tokenized into 184 tokens.
[preprocess][L3] 365: Removed stop words; 110 tokens remain.
[preprocess][L3] 365: Stemming complete.
[indexDocument][L3] Preprocessing complete for '365'; 110 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '365'.
[indexDocument][L3] Recorded doc length 110 for '365'.
[indexDocument][L3] Tokens added to the inverted index for '365'.
[indexDocument][L2] Indexing document '701'.
[indexDocument][L3] Opened file '701'.
[preprocess][L2] Starting preprocessing for '701'.
[preprocess][L3] 701: Tokenized into 257 tokens.
[preprocess][L3] 701: Removed stop words; 162 tokens remain.
[preprocess][L3] 701: Stemming complete.
[indexDocument][L3] Preprocessing complete for '701'; 162 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '701'.
[indexDocument][L3] Recorded doc length 162 for '701'.
[indexDocument][L3] Tokens added to the inverted index for '701'.
[indexDocument][L2] Indexing document '533'.
[indexDocument][L3] Opened file '533'.
[preprocess][L2] Starting preprocessing for '533'.
[preprocess][L3] 533: Tokenized into 47 tokens.
[preprocess][L3] 533: Removed stop words; 31 tokens remain.
[preprocess][L3] 533: Stemming complete.
[indexDocument][L3] Preprocessing complete for '533'; 31 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '533'.
[indexDocument][L3] Recorded doc length 31 for '533'.
[indexDocument][L3] Tokens added to the inverted index for '533'.
[indexDocument][L2] Indexing document '1071'.
[indexDocument][L3] Opened file '1071'.
[preprocess][L2] Starting preprocessing for '1071'.
[preprocess][L3] 1071: Tokenized into 108 tokens.
[preprocess][L3] 1071: Removed stop words; 68 tokens remain.
[preprocess][L3] 1071: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1071'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1071'.
[indexDocument][L3] Recorded doc length 68 for '1071'.
[indexDocument][L3] Tokens added to the inverted index for '1071'.
[indexDocument][L2] Indexing document '1243'.
[indexDocument][L3] Opened file '1243'.
[preprocess][L2] Starting preprocessing for '1243'.
[preprocess][L3] 1243: Tokenized into 114 tokens.
[preprocess][L3] 1243: Removed stop words; 69 tokens remain.
[preprocess][L3] 1243: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1243'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1243'.
[indexDocument][L3] Recorded doc length 69 for '1243'.
[indexDocument][L3] Tokens added to the inverted index for '1243'.
[indexDocument][L2] Indexing document '391'.
[indexDocument][L3] Opened file '391'.
[preprocess][L2] Starting preprocessing for '391'.
[preprocess][L3] 391: Tokenized into 121 tokens.
[preprocess][L3] 391: Removed stop words; 86 tokens remain.
[preprocess][L3] 391: Stemming complete.
[indexDocument][L3] Preprocessing complete for '391'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '391'.
[indexDocument][L3] Recorded doc length 86 for '391'.
[indexDocument][L3] Tokens added to the inverted index for '391'.
[indexDocument][L2] Indexing document '1049'.
[indexDocument][L3] Opened file '1049'.
[preprocess][L2] Starting preprocessing for '1049'.
[preprocess][L3] 1049: Tokenized into 83 tokens.
[preprocess][L3] 1049: Removed stop words; 53 tokens remain.
[preprocess][L3] 1049: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1049'; 53 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1049'.
[indexDocument][L3] Recorded doc length 53 for '1049'.
[indexDocument][L3] Tokens added to the inverted index for '1049'.
[indexDocument][L2] Indexing document '953'.
[indexDocument][L3] Opened file '953'.
[preprocess][L2] Starting preprocessing for '953'.
[preprocess][L3] 953: Tokenized into 67 tokens.
[preprocess][L3] 953: Removed stop words; 46 tokens remain.
[preprocess][L3] 953: Stemming complete.
[indexDocument][L3] Preprocessing complete for '953'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '953'.
[indexDocument][L3] Recorded doc length 46 for '953'.
[indexDocument][L3] Tokens added to the inverted index for '953'.
[indexDocument][L2] Indexing document '739'.
[indexDocument][L3] Opened file '739'.
[preprocess][L2] Starting preprocessing for '739'.
[preprocess][L3] 739: Tokenized into 335 tokens.
[preprocess][L3] 739: Removed stop words; 180 tokens remain.
[preprocess][L3] 739: Stemming complete.
[indexDocument][L3] Preprocessing complete for '739'; 180 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '739'.
[indexDocument][L3] Recorded doc length 180 for '739'.
[indexDocument][L3] Tokens added to the inverted index for '739'.
[indexDocument][L2] Indexing document '1244'.
[indexDocument][L3] Opened file '1244'.
[preprocess][L2] Starting preprocessing for '1244'.
[preprocess][L3] 1244: Tokenized into 414 tokens.
[preprocess][L3] 1244: Removed stop words; 252 tokens remain.
[preprocess][L3] 1244: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1244'; 252 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1244'.
[indexDocument][L3] Recorded doc length 252 for '1244'.
[indexDocument][L3] Tokens added to the inverted index for '1244'.
[indexDocument][L2] Indexing document '396'.
[indexDocument][L3] Opened file '396'.
[preprocess][L2] Starting preprocessing for '396'.
[preprocess][L3] 396: Tokenized into 78 tokens.
[preprocess][L3] 396: Removed stop words; 47 tokens remain.
[preprocess][L3] 396: Stemming complete.
[indexDocument][L3] Preprocessing complete for '396'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '396'.
[indexDocument][L3] Recorded doc length 47 for '396'.
[indexDocument][L3] Tokens added to the inverted index for '396'.
[indexDocument][L2] Indexing document '998'.
[indexDocument][L3] Opened file '998'.
[preprocess][L2] Starting preprocessing for '998'.
[preprocess][L3] 998: Tokenized into 137 tokens.
[preprocess][L3] 998: Removed stop words; 88 tokens remain.
[preprocess][L3] 998: Stemming complete.
[indexDocument][L3] Preprocessing complete for '998'; 88 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '998'.
[indexDocument][L3] Recorded doc length 88 for '998'.
[indexDocument][L3] Tokens added to the inverted index for '998'.
[indexDocument][L2] Indexing document '1076'.
[indexDocument][L3] Opened file '1076'.
[preprocess][L2] Starting preprocessing for '1076'.
[preprocess][L3] 1076: Tokenized into 104 tokens.
[preprocess][L3] 1076: Removed stop words; 64 tokens remain.
[preprocess][L3] 1076: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1076'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1076'.
[indexDocument][L3] Recorded doc length 64 for '1076'.
[indexDocument][L3] Tokens added to the inverted index for '1076'.
[indexDocument][L2] Indexing document '534'.
[indexDocument][L3] Opened file '534'.
[preprocess][L2] Starting preprocessing for '534'.
[preprocess][L3] 534: Tokenized into 88 tokens.
[preprocess][L3] 534: Removed stop words; 52 tokens remain.
[preprocess][L3] 534: Stemming complete.
[indexDocument][L3] Preprocessing complete for '534'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '534'.
[indexDocument][L3] Recorded doc length 52 for '534'.
[indexDocument][L3] Tokens added to the inverted index for '534'.
[indexDocument][L2] Indexing document '706'.
[indexDocument][L3] Opened file '706'.
[preprocess][L2] Starting preprocessing for '706'.
[preprocess][L3] 706: Tokenized into 102 tokens.
[preprocess][L3] 706: Removed stop words; 64 tokens remain.
[preprocess][L3] 706: Stemming complete.
[indexDocument][L3] Preprocessing complete for '706'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '706'.
[indexDocument][L3] Recorded doc length 64 for '706'.
[indexDocument][L3] Tokens added to the inverted index for '706'.
[indexDocument][L2] Indexing document '362'.
[indexDocument][L3] Opened file '362'.
[preprocess][L2] Starting preprocessing for '362'.
[preprocess][L3] 362: Tokenized into 81 tokens.
[preprocess][L3] 362: Removed stop words; 57 tokens remain.
[preprocess][L3] 362: Stemming complete.
[indexDocument][L3] Preprocessing complete for '362'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '362'.
[indexDocument][L3] Recorded doc length 57 for '362'.
[indexDocument][L3] Tokens added to the inverted index for '362'.
[indexDocument][L2] Indexing document '1082'.
[indexDocument][L3] Opened file '1082'.
[preprocess][L2] Starting preprocessing for '1082'.
[preprocess][L3] 1082: Tokenized into 298 tokens.
[preprocess][L3] 1082: Removed stop words; 162 tokens remain.
[preprocess][L3] 1082: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1082'; 162 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1082'.
[indexDocument][L3] Recorded doc length 162 for '1082'.
[indexDocument][L3] Tokens added to the inverted index for '1082'.
[indexDocument][L2] Indexing document '150'.
[indexDocument][L3] Opened file '150'.
[preprocess][L2] Starting preprocessing for '150'.
[preprocess][L3] 150: Tokenized into 181 tokens.
[preprocess][L3] 150: Removed stop words; 106 tokens remain.
[preprocess][L3] 150: Stemming complete.
[indexDocument][L3] Preprocessing complete for '150'; 106 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '150'.
[indexDocument][L3] Recorded doc length 106 for '150'.
[indexDocument][L3] Tokens added to the inverted index for '150'.
[indexDocument][L2] Indexing document '1022'.
[indexDocument][L3] Opened file '1022'.
[preprocess][L2] Starting preprocessing for '1022'.
[preprocess][L3] 1022: Tokenized into 65 tokens.
[preprocess][L3] 1022: Removed stop words; 39 tokens remain.
[preprocess][L3] 1022: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1022'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1022'.
[indexDocument][L3] Recorded doc length 39 for '1022'.
[indexDocument][L3] Tokens added to the inverted index for '1022'.
[indexDocument][L2] Indexing document '1210'.
[indexDocument][L3] Opened file '1210'.
[preprocess][L2] Starting preprocessing for '1210'.
[preprocess][L3] 1210: Tokenized into 127 tokens.
[preprocess][L3] 1210: Removed stop words; 76 tokens remain.
[preprocess][L3] 1210: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1210'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1210'.
[indexDocument][L3] Recorded doc length 76 for '1210'.
[indexDocument][L3] Tokens added to the inverted index for '1210'.
[indexDocument][L2] Indexing document '594'.
[indexDocument][L3] Opened file '594'.
[preprocess][L2] Starting preprocessing for '594'.
[preprocess][L3] 594: Tokenized into 67 tokens.
[preprocess][L3] 594: Removed stop words; 43 tokens remain.
[preprocess][L3] 594: Stemming complete.
[indexDocument][L3] Preprocessing complete for '594'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '594'.
[indexDocument][L3] Recorded doc length 43 for '594'.
[indexDocument][L3] Tokens added to the inverted index for '594'.
[indexDocument][L2] Indexing document '752'.
[indexDocument][L3] Opened file '752'.
[preprocess][L2] Starting preprocessing for '752'.
[preprocess][L3] 752: Tokenized into 111 tokens.
[preprocess][L3] 752: Removed stop words; 61 tokens remain.
[preprocess][L3] 752: Stemming complete.
[indexDocument][L3] Preprocessing complete for '752'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '752'.
[indexDocument][L3] Recorded doc length 61 for '752'.
[indexDocument][L3] Tokens added to the inverted index for '752'.
[indexDocument][L2] Indexing document '560'.
[indexDocument][L3] Opened file '560'.
[preprocess][L2] Starting preprocessing for '560'.
[preprocess][L3] 560: Tokenized into 246 tokens.
[preprocess][L3] 560: Removed stop words; 134 tokens remain.
[preprocess][L3] 560: Stemming complete.
[indexDocument][L3] Preprocessing complete for '560'; 134 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '560'.
[indexDocument][L3] Recorded doc length 134 for '560'.
[indexDocument][L3] Tokens added to the inverted index for '560'.
[indexDocument][L2] Indexing document '104'.
[indexDocument][L3] Opened file '104'.
[preprocess][L2] Starting preprocessing for '104'.
[preprocess][L3] 104: Tokenized into 129 tokens.
[preprocess][L3] 104: Removed stop words; 76 tokens remain.
[preprocess][L3] 104: Stemming complete.
[indexDocument][L3] Preprocessing complete for '104'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '104'.
[indexDocument][L3] Recorded doc length 76 for '104'.
[indexDocument][L3] Tokens added to the inverted index for '104'.
[indexDocument][L2] Indexing document '938'.
[indexDocument][L3] Opened file '938'.
[preprocess][L2] Starting preprocessing for '938'.
[preprocess][L3] 938: Tokenized into 72 tokens.
[preprocess][L3] 938: Removed stop words; 47 tokens remain.
[preprocess][L3] 938: Stemming complete.
[indexDocument][L3] Preprocessing complete for '938'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '938'.
[indexDocument][L3] Recorded doc length 47 for '938'.
[indexDocument][L3] Tokens added to the inverted index for '938'.
[indexDocument][L2] Indexing document '336'.
[indexDocument][L3] Opened file '336'.
[preprocess][L2] Starting preprocessing for '336'.
[preprocess][L3] 336: Tokenized into 85 tokens.
[preprocess][L3] 336: Removed stop words; 56 tokens remain.
[preprocess][L3] 336: Stemming complete.
[indexDocument][L3] Preprocessing complete for '336'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '336'.
[indexDocument][L3] Recorded doc length 56 for '336'.
[indexDocument][L3] Tokens added to the inverted index for '336'.
[indexDocument][L2] Indexing document '799'.
[indexDocument][L3] Opened file '799'.
[preprocess][L2] Starting preprocessing for '799'.
[preprocess][L3] 799: Tokenized into 245 tokens.
[preprocess][L3] 799: Removed stop words; 149 tokens remain.
[preprocess][L3] 799: Stemming complete.
[indexDocument][L3] Preprocessing complete for '799'; 149 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '799'.
[indexDocument][L3] Recorded doc length 149 for '799'.
[indexDocument][L3] Tokens added to the inverted index for '799'.
[indexDocument][L2] Indexing document '309'.
[indexDocument][L3] Opened file '309'.
[preprocess][L2] Starting preprocessing for '309'.
[preprocess][L3] 309: Tokenized into 193 tokens.
[preprocess][L3] 309: Removed stop words; 105 tokens remain.
[preprocess][L3] 309: Stemming complete.
[indexDocument][L3] Preprocessing complete for '309'; 105 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '309'.
[indexDocument][L3] Recorded doc length 105 for '309'.
[indexDocument][L3] Tokens added to the inverted index for '309'.
[indexDocument][L2] Indexing document '907'.
[indexDocument][L3] Opened file '907'.
[preprocess][L2] Starting preprocessing for '907'.
[preprocess][L3] 907: Tokenized into 221 tokens.
[preprocess][L3] 907: Removed stop words; 138 tokens remain.
[preprocess][L3] 907: Stemming complete.
[indexDocument][L3] Preprocessing complete for '907'; 138 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '907'.
[indexDocument][L3] Recorded doc length 138 for '907'.
[indexDocument][L3] Tokens added to the inverted index for '907'.
[indexDocument][L2] Indexing document '331'.
[indexDocument][L3] Opened file '331'.
[preprocess][L2] Starting preprocessing for '331'.
[preprocess][L3] 331: Tokenized into 94 tokens.
[preprocess][L3] 331: Removed stop words; 57 tokens remain.
[preprocess][L3] 331: Stemming complete.
[indexDocument][L3] Preprocessing complete for '331'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '331'.
[indexDocument][L3] Recorded doc length 57 for '331'.
[indexDocument][L3] Tokens added to the inverted index for '331'.
[indexDocument][L2] Indexing document '103'.
[indexDocument][L3] Opened file '103'.
[preprocess][L2] Starting preprocessing for '103'.
[preprocess][L3] 103: Tokenized into 133 tokens.
[preprocess][L3] 103: Removed stop words; 78 tokens remain.
[preprocess][L3] 103: Stemming complete.
[indexDocument][L3] Preprocessing complete for '103'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '103'.
[indexDocument][L3] Recorded doc length 78 for '103'.
[indexDocument][L3] Tokens added to the inverted index for '103'.
[indexDocument][L2] Indexing document '567'.
[indexDocument][L3] Opened file '567'.
[preprocess][L2] Starting preprocessing for '567'.
[preprocess][L3] 567: Tokenized into 178 tokens.
[preprocess][L3] 567: Removed stop words; 105 tokens remain.
[preprocess][L3] 567: Stemming complete.
[indexDocument][L3] Preprocessing complete for '567'; 105 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '567'.
[indexDocument][L3] Recorded doc length 105 for '567'.
[indexDocument][L3] Tokens added to the inverted index for '567'.
[indexDocument][L2] Indexing document '755'.
[indexDocument][L3] Opened file '755'.
[preprocess][L2] Starting preprocessing for '755'.
[preprocess][L3] 755: Tokenized into 136 tokens.
[preprocess][L3] 755: Removed stop words; 84 tokens remain.
[preprocess][L3] 755: Stemming complete.
[indexDocument][L3] Preprocessing complete for '755'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '755'.
[indexDocument][L3] Recorded doc length 84 for '755'.
[indexDocument][L3] Tokens added to the inverted index for '755'.
[indexDocument][L2] Indexing document '593'.
[indexDocument][L3] Opened file '593'.
[preprocess][L2] Starting preprocessing for '593'.
[preprocess][L3] 593: Tokenized into 191 tokens.
[preprocess][L3] 593: Removed stop words; 123 tokens remain.
[preprocess][L3] 593: Stemming complete.
[indexDocument][L3] Preprocessing complete for '593'; 123 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '593'.
[indexDocument][L3] Recorded doc length 123 for '593'.
[indexDocument][L3] Tokens added to the inverted index for '593'.
[indexDocument][L2] Indexing document '1217'.
[indexDocument][L3] Opened file '1217'.
[preprocess][L2] Starting preprocessing for '1217'.
[preprocess][L3] 1217: Tokenized into 124 tokens.
[preprocess][L3] 1217: Removed stop words; 76 tokens remain.
[preprocess][L3] 1217: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1217'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1217'.
[indexDocument][L3] Recorded doc length 76 for '1217'.
[indexDocument][L3] Tokens added to the inverted index for '1217'.
[indexDocument][L2] Indexing document '1025'.
[indexDocument][L3] Opened file '1025'.
[preprocess][L2] Starting preprocessing for '1025'.
[preprocess][L3] 1025: Tokenized into 190 tokens.
[preprocess][L3] 1025: Removed stop words; 107 tokens remain.
[preprocess][L3] 1025: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1025'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1025'.
[indexDocument][L3] Recorded doc length 107 for '1025'.
[indexDocument][L3] Tokens added to the inverted index for '1025'.
[indexDocument][L2] Indexing document '558'.
[indexDocument][L3] Opened file '558'.
[preprocess][L2] Starting preprocessing for '558'.
[preprocess][L3] 558: Tokenized into 129 tokens.
[preprocess][L3] 558: Removed stop words; 81 tokens remain.
[preprocess][L3] 558: Stemming complete.
[indexDocument][L3] Preprocessing complete for '558'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '558'.
[indexDocument][L3] Recorded doc length 81 for '558'.
[indexDocument][L3] Tokens added to the inverted index for '558'.
[indexDocument][L2] Indexing document '900'.
[indexDocument][L3] Opened file '900'.
[preprocess][L2] Starting preprocessing for '900'.
[preprocess][L3] 900: Tokenized into 95 tokens.
[preprocess][L3] 900: Removed stop words; 56 tokens remain.
[preprocess][L3] 900: Stemming complete.
[indexDocument][L3] Preprocessing complete for '900'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '900'.
[indexDocument][L3] Recorded doc length 56 for '900'.
[indexDocument][L3] Tokens added to the inverted index for '900'.
[indexDocument][L2] Indexing document '1228'.
[indexDocument][L3] Opened file '1228'.
[preprocess][L2] Starting preprocessing for '1228'.
[preprocess][L3] 1228: Tokenized into 118 tokens.
[preprocess][L3] 1228: Removed stop words; 77 tokens remain.
[preprocess][L3] 1228: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1228'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1228'.
[indexDocument][L3] Recorded doc length 77 for '1228'.
[indexDocument][L3] Tokens added to the inverted index for '1228'.
[indexDocument][L2] Indexing document '551'.
[indexDocument][L3] Opened file '551'.
[preprocess][L2] Starting preprocessing for '551'.
[preprocess][L3] 551: Tokenized into 55 tokens.
[preprocess][L3] 551: Removed stop words; 40 tokens remain.
[preprocess][L3] 551: Stemming complete.
[indexDocument][L3] Preprocessing complete for '551'; 40 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '551'.
[indexDocument][L3] Recorded doc length 40 for '551'.
[indexDocument][L3] Tokens added to the inverted index for '551'.
[indexDocument][L2] Indexing document '763'.
[indexDocument][L3] Opened file '763'.
[preprocess][L2] Starting preprocessing for '763'.
[preprocess][L3] 763: Tokenized into 95 tokens.
[preprocess][L3] 763: Removed stop words; 56 tokens remain.
[preprocess][L3] 763: Stemming complete.
[indexDocument][L3] Preprocessing complete for '763'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '763'.
[indexDocument][L3] Recorded doc length 56 for '763'.
[indexDocument][L3] Tokens added to the inverted index for '763'.
[indexDocument][L2] Indexing document '307'.
[indexDocument][L3] Opened file '307'.
[preprocess][L2] Starting preprocessing for '307'.
[preprocess][L3] 307: Tokenized into 150 tokens.
[preprocess][L3] 307: Removed stop words; 91 tokens remain.
[preprocess][L3] 307: Stemming complete.
[indexDocument][L3] Preprocessing complete for '307'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '307'.
[indexDocument][L3] Recorded doc length 91 for '307'.
[indexDocument][L3] Tokens added to the inverted index for '307'.
[indexDocument][L2] Indexing document '135'.
[indexDocument][L3] Opened file '135'.
[preprocess][L2] Starting preprocessing for '135'.
[preprocess][L3] 135: Tokenized into 199 tokens.
[preprocess][L3] 135: Removed stop words; 111 tokens remain.
[preprocess][L3] 135: Stemming complete.
[indexDocument][L3] Preprocessing complete for '135'; 111 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '135'.
[indexDocument][L3] Recorded doc length 111 for '135'.
[indexDocument][L3] Tokens added to the inverted index for '135'.
[indexDocument][L2] Indexing document '909'.
[indexDocument][L3] Opened file '909'.
[preprocess][L2] Starting preprocessing for '909'.
[preprocess][L3] 909: Tokenized into 77 tokens.
[preprocess][L3] 909: Removed stop words; 44 tokens remain.
[preprocess][L3] 909: Stemming complete.
[indexDocument][L3] Preprocessing complete for '909'; 44 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '909'.
[indexDocument][L3] Recorded doc length 44 for '909'.
[indexDocument][L3] Tokens added to the inverted index for '909'.
[indexDocument][L2] Indexing document '1221'.
[indexDocument][L3] Opened file '1221'.
[preprocess][L2] Starting preprocessing for '1221'.
[preprocess][L3] 1221: Tokenized into 109 tokens.
[preprocess][L3] 1221: Removed stop words; 67 tokens remain.
[preprocess][L3] 1221: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1221'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1221'.
[indexDocument][L3] Recorded doc length 67 for '1221'.
[indexDocument][L3] Tokens added to the inverted index for '1221'.
[indexDocument][L2] Indexing document '1013'.
[indexDocument][L3] Opened file '1013'.
[preprocess][L2] Starting preprocessing for '1013'.
[preprocess][L3] 1013: Tokenized into 121 tokens.
[preprocess][L3] 1013: Removed stop words; 75 tokens remain.
[preprocess][L3] 1013: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1013'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1013'.
[indexDocument][L3] Recorded doc length 75 for '1013'.
[indexDocument][L3] Tokens added to the inverted index for '1013'.
[indexDocument][L2] Indexing document '797'.
[indexDocument][L3] Opened file '797'.
[preprocess][L2] Starting preprocessing for '797'.
[preprocess][L3] 797: Tokenized into 339 tokens.
[preprocess][L3] 797: Removed stop words; 201 tokens remain.
[preprocess][L3] 797: Stemming complete.
[indexDocument][L3] Preprocessing complete for '797'; 201 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '797'.
[indexDocument][L3] Recorded doc length 201 for '797'.
[indexDocument][L3] Tokens added to the inverted index for '797'.
[indexDocument][L2] Indexing document '936'.
[indexDocument][L3] Opened file '936'.
[preprocess][L2] Starting preprocessing for '936'.
[preprocess][L3] 936: Tokenized into 155 tokens.
[preprocess][L3] 936: Removed stop words; 98 tokens remain.
[preprocess][L3] 936: Stemming complete.
[indexDocument][L3] Preprocessing complete for '936'; 98 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '936'.
[indexDocument][L3] Recorded doc length 98 for '936'.
[indexDocument][L3] Tokens added to the inverted index for '936'.
[indexDocument][L2] Indexing document '338'.
[indexDocument][L3] Opened file '338'.
[preprocess][L2] Starting preprocessing for '338'.
[preprocess][L3] 338: Tokenized into 154 tokens.
[preprocess][L3] 338: Removed stop words; 102 tokens remain.
[preprocess][L3] 338: Stemming complete.
[indexDocument][L3] Preprocessing complete for '338'; 102 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '338'.
[indexDocument][L3] Recorded doc length 102 for '338'.
[indexDocument][L3] Tokens added to the inverted index for '338'.
[indexDocument][L2] Indexing document '790'.
[indexDocument][L3] Opened file '790'.
[preprocess][L2] Starting preprocessing for '790'.
[preprocess][L3] 790: Tokenized into 203 tokens.
[preprocess][L3] 790: Removed stop words; 126 tokens remain.
[preprocess][L3] 790: Stemming complete.
[indexDocument][L3] Preprocessing complete for '790'; 126 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '790'.
[indexDocument][L3] Recorded doc length 126 for '790'.
[indexDocument][L3] Tokens added to the inverted index for '790'.
[indexDocument][L2] Indexing document '1014'.
[indexDocument][L3] Opened file '1014'.
[preprocess][L2] Starting preprocessing for '1014'.
[preprocess][L3] 1014: Tokenized into 57 tokens.
[preprocess][L3] 1014: Removed stop words; 34 tokens remain.
[preprocess][L3] 1014: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1014'; 34 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1014'.
[indexDocument][L3] Recorded doc length 34 for '1014'.
[indexDocument][L3] Tokens added to the inverted index for '1014'.
[indexDocument][L2] Indexing document '1226'.
[indexDocument][L3] Opened file '1226'.
[preprocess][L2] Starting preprocessing for '1226'.
[preprocess][L3] 1226: Tokenized into 274 tokens.
[preprocess][L3] 1226: Removed stop words; 168 tokens remain.
[preprocess][L3] 1226: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1226'; 168 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1226'.
[indexDocument][L3] Recorded doc length 168 for '1226'.
[indexDocument][L3] Tokens added to the inverted index for '1226'.
[indexDocument][L2] Indexing document '132'.
[indexDocument][L3] Opened file '132'.
[preprocess][L2] Starting preprocessing for '132'.
[preprocess][L3] 132: Tokenized into 348 tokens.
[preprocess][L3] 132: Removed stop words; 186 tokens remain.
[preprocess][L3] 132: Stemming complete.
[indexDocument][L3] Preprocessing complete for '132'; 186 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '132'.
[indexDocument][L3] Recorded doc length 186 for '132'.
[indexDocument][L3] Tokens added to the inverted index for '132'.
[indexDocument][L2] Indexing document '300'.
[indexDocument][L3] Opened file '300'.
[preprocess][L2] Starting preprocessing for '300'.
[preprocess][L3] 300: Tokenized into 260 tokens.
[preprocess][L3] 300: Removed stop words; 160 tokens remain.
[preprocess][L3] 300: Stemming complete.
[indexDocument][L3] Preprocessing complete for '300'; 160 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '300'.
[indexDocument][L3] Recorded doc length 160 for '300'.
[indexDocument][L3] Tokens added to the inverted index for '300'.
[indexDocument][L2] Indexing document '764'.
[indexDocument][L3] Opened file '764'.
[preprocess][L2] Starting preprocessing for '764'.
[preprocess][L3] 764: Tokenized into 172 tokens.
[preprocess][L3] 764: Removed stop words; 105 tokens remain.
[preprocess][L3] 764: Stemming complete.
[indexDocument][L3] Preprocessing complete for '764'; 105 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '764'.
[indexDocument][L3] Recorded doc length 105 for '764'.
[indexDocument][L3] Tokens added to the inverted index for '764'.
[indexDocument][L2] Indexing document '556'.
[indexDocument][L3] Opened file '556'.
[preprocess][L2] Starting preprocessing for '556'.
[preprocess][L3] 556: Tokenized into 175 tokens.
[preprocess][L3] 556: Removed stop words; 103 tokens remain.
[preprocess][L3] 556: Stemming complete.
[indexDocument][L3] Preprocessing complete for '556'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '556'.
[indexDocument][L3] Recorded doc length 103 for '556'.
[indexDocument][L3] Tokens added to the inverted index for '556'.
[indexDocument][L2] Indexing document '1219'.
[indexDocument][L3] Opened file '1219'.
[preprocess][L2] Starting preprocessing for '1219'.
[preprocess][L3] 1219: Tokenized into 132 tokens.
[preprocess][L3] 1219: Removed stop words; 84 tokens remain.
[preprocess][L3] 1219: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1219'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1219'.
[indexDocument][L3] Recorded doc length 84 for '1219'.
[indexDocument][L3] Tokens added to the inverted index for '1219'.
[indexDocument][L2] Indexing document '569'.
[indexDocument][L3] Opened file '569'.
[preprocess][L2] Starting preprocessing for '569'.
[preprocess][L3] 569: Tokenized into 267 tokens.
[preprocess][L3] 569: Removed stop words; 174 tokens remain.
[preprocess][L3] 569: Stemming complete.
[indexDocument][L3] Preprocessing complete for '569'; 174 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '569'.
[indexDocument][L3] Recorded doc length 174 for '569'.
[indexDocument][L3] Tokens added to the inverted index for '569'.
[indexDocument][L2] Indexing document '931'.
[indexDocument][L3] Opened file '931'.
[preprocess][L2] Starting preprocessing for '931'.
[preprocess][L3] 931: Tokenized into 62 tokens.
[preprocess][L3] 931: Removed stop words; 47 tokens remain.
[preprocess][L3] 931: Stemming complete.
[indexDocument][L3] Preprocessing complete for '931'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '931'.
[indexDocument][L3] Recorded doc length 47 for '931'.
[indexDocument][L3] Tokens added to the inverted index for '931'.
[indexDocument][L2] Indexing document '843'.
[indexDocument][L3] Opened file '843'.
[preprocess][L2] Starting preprocessing for '843'.
[preprocess][L3] 843: Tokenized into 176 tokens.
[preprocess][L3] 843: Removed stop words; 101 tokens remain.
[preprocess][L3] 843: Stemming complete.
[indexDocument][L3] Preprocessing complete for '843'; 101 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '843'.
[indexDocument][L3] Recorded doc length 101 for '843'.
[indexDocument][L3] Tokens added to the inverted index for '843'.
[indexDocument][L2] Indexing document '629'.
[indexDocument][L3] Opened file '629'.
[preprocess][L2] Starting preprocessing for '629'.
[preprocess][L3] 629: Tokenized into 121 tokens.
[preprocess][L3] 629: Removed stop words; 74 tokens remain.
[preprocess][L3] 629: Stemming complete.
[indexDocument][L3] Preprocessing complete for '629'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '629'.
[indexDocument][L3] Recorded doc length 74 for '629'.
[indexDocument][L3] Tokens added to the inverted index for '629'.
[indexDocument][L2] Indexing document '1159'.
[indexDocument][L3] Opened file '1159'.
[preprocess][L2] Starting preprocessing for '1159'.
[preprocess][L3] 1159: Tokenized into 118 tokens.
[preprocess][L3] 1159: Removed stop words; 76 tokens remain.
[preprocess][L3] 1159: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1159'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1159'.
[indexDocument][L3] Recorded doc length 76 for '1159'.
[indexDocument][L3] Tokens added to the inverted index for '1159'.
[indexDocument][L2] Indexing document '616'.
[indexDocument][L3] Opened file '616'.
[preprocess][L2] Starting preprocessing for '616'.
[preprocess][L3] 616: Tokenized into 153 tokens.
[preprocess][L3] 616: Removed stop words; 90 tokens remain.
[preprocess][L3] 616: Stemming complete.
[indexDocument][L3] Preprocessing complete for '616'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '616'.
[indexDocument][L3] Recorded doc length 90 for '616'.
[indexDocument][L3] Tokens added to the inverted index for '616'.
[indexDocument][L2] Indexing document '424'.
[indexDocument][L3] Opened file '424'.
[preprocess][L2] Starting preprocessing for '424'.
[preprocess][L3] 424: Tokenized into 97 tokens.
[preprocess][L3] 424: Removed stop words; 57 tokens remain.
[preprocess][L3] 424: Stemming complete.
[indexDocument][L3] Preprocessing complete for '424'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '424'.
[indexDocument][L3] Recorded doc length 57 for '424'.
[indexDocument][L3] Tokens added to the inverted index for '424'.
[indexDocument][L2] Indexing document '40'.
[indexDocument][L3] Opened file '40'.
[preprocess][L2] Starting preprocessing for '40'.
[preprocess][L3] 40: Tokenized into 169 tokens.
[preprocess][L3] 40: Removed stop words; 119 tokens remain.
[preprocess][L3] 40: Stemming complete.
[indexDocument][L3] Preprocessing complete for '40'; 119 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '40'.
[indexDocument][L3] Recorded doc length 119 for '40'.
[indexDocument][L3] Tokens added to the inverted index for '40'.
[indexDocument][L2] Indexing document '1192'.
[indexDocument][L3] Opened file '1192'.
[preprocess][L2] Starting preprocessing for '1192'.
[preprocess][L3] 1192: Tokenized into 134 tokens.
[preprocess][L3] 1192: Removed stop words; 83 tokens remain.
[preprocess][L3] 1192: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1192'; 83 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1192'.
[indexDocument][L3] Recorded doc length 83 for '1192'.
[indexDocument][L3] Tokens added to the inverted index for '1192'.
[indexDocument][L2] Indexing document '272'.
[indexDocument][L3] Opened file '272'.
[preprocess][L2] Starting preprocessing for '272'.
[preprocess][L3] 272: Tokenized into 449 tokens.
[preprocess][L3] 272: Removed stop words; 275 tokens remain.
[preprocess][L3] 272: Stemming complete.
[indexDocument][L3] Preprocessing complete for '272'; 275 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '272'.
[indexDocument][L3] Recorded doc length 275 for '272'.
[indexDocument][L3] Tokens added to the inverted index for '272'.
[indexDocument][L2] Indexing document '888'.
[indexDocument][L3] Opened file '888'.
[preprocess][L2] Starting preprocessing for '888'.
[preprocess][L3] 888: Tokenized into 113 tokens.
[preprocess][L3] 888: Removed stop words; 70 tokens remain.
[preprocess][L3] 888: Stemming complete.
[indexDocument][L3] Preprocessing complete for '888'; 70 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '888'.
[indexDocument][L3] Recorded doc length 70 for '888'.
[indexDocument][L3] Tokens added to the inverted index for '888'.
[indexDocument][L2] Indexing document '1166'.
[indexDocument][L3] Opened file '1166'.
[preprocess][L2] Starting preprocessing for '1166'.
[preprocess][L3] 1166: Tokenized into 211 tokens.
[preprocess][L3] 1166: Removed stop words; 130 tokens remain.
[preprocess][L3] 1166: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1166'; 130 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1166'.
[indexDocument][L3] Recorded doc length 130 for '1166'.
[indexDocument][L3] Tokens added to the inverted index for '1166'.
[indexDocument][L2] Indexing document '286'.
[indexDocument][L3] Opened file '286'.
[preprocess][L2] Starting preprocessing for '286'.
[preprocess][L3] 286: Tokenized into 42 tokens.
[preprocess][L3] 286: Removed stop words; 31 tokens remain.
[preprocess][L3] 286: Stemming complete.
[indexDocument][L3] Preprocessing complete for '286'; 31 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '286'.
[indexDocument][L3] Recorded doc length 31 for '286'.
[indexDocument][L3] Tokens added to the inverted index for '286'.
[indexDocument][L2] Indexing document '1354'.
[indexDocument][L3] Opened file '1354'.
[preprocess][L2] Starting preprocessing for '1354'.
[preprocess][L3] 1354: Tokenized into 165 tokens.
[preprocess][L3] 1354: Removed stop words; 99 tokens remain.
[preprocess][L3] 1354: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1354'; 99 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1354'.
[indexDocument][L3] Recorded doc length 99 for '1354'.
[indexDocument][L3] Tokens added to the inverted index for '1354'.
[indexDocument][L2] Indexing document '844'.
[indexDocument][L3] Opened file '844'.
[preprocess][L2] Starting preprocessing for '844'.
[preprocess][L3] 844: Tokenized into 229 tokens.
[preprocess][L3] 844: Removed stop words; 128 tokens remain.
[preprocess][L3] 844: Stemming complete.
[indexDocument][L3] Preprocessing complete for '844'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '844'.
[indexDocument][L3] Recorded doc length 128 for '844'.
[indexDocument][L3] Tokens added to the inverted index for '844'.
[indexDocument][L2] Indexing document '78'.
[indexDocument][L3] Opened file '78'.
[preprocess][L2] Starting preprocessing for '78'.
[preprocess][L3] 78: Tokenized into 201 tokens.
[preprocess][L3] 78: Removed stop words; 124 tokens remain.
[preprocess][L3] 78: Stemming complete.
[indexDocument][L3] Preprocessing complete for '78'; 124 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '78'.
[indexDocument][L3] Recorded doc length 124 for '78'.
[indexDocument][L3] Tokens added to the inverted index for '78'.
[indexDocument][L2] Indexing document '1398'.
[indexDocument][L3] Opened file '1398'.
[preprocess][L2] Starting preprocessing for '1398'.
[preprocess][L3] 1398: Tokenized into 214 tokens.
[preprocess][L3] 1398: Removed stop words; 124 tokens remain.
[preprocess][L3] 1398: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1398'; 124 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1398'.
[indexDocument][L3] Recorded doc length 124 for '1398'.
[indexDocument][L3] Tokens added to the inverted index for '1398'.
[indexDocument][L2] Indexing document '281'.
[indexDocument][L3] Opened file '281'.
[preprocess][L2] Starting preprocessing for '281'.
[preprocess][L3] 281: Tokenized into 46 tokens.
[preprocess][L3] 281: Removed stop words; 27 tokens remain.
[preprocess][L3] 281: Stemming complete.
[indexDocument][L3] Preprocessing complete for '281'; 27 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '281'.
[indexDocument][L3] Recorded doc length 27 for '281'.
[indexDocument][L3] Tokens added to the inverted index for '281'.
[indexDocument][L2] Indexing document '1353'.
[indexDocument][L3] Opened file '1353'.
[preprocess][L2] Starting preprocessing for '1353'.
[preprocess][L3] 1353: Tokenized into 142 tokens.
[preprocess][L3] 1353: Removed stop words; 85 tokens remain.
[preprocess][L3] 1353: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1353'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1353'.
[indexDocument][L3] Recorded doc length 85 for '1353'.
[indexDocument][L3] Tokens added to the inverted index for '1353'.
[indexDocument][L2] Indexing document '1161'.
[indexDocument][L3] Opened file '1161'.
[preprocess][L2] Starting preprocessing for '1161'.
[preprocess][L3] 1161: Tokenized into 126 tokens.
[preprocess][L3] 1161: Removed stop words; 77 tokens remain.
[preprocess][L3] 1161: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1161'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1161'.
[indexDocument][L3] Recorded doc length 77 for '1161'.
[indexDocument][L3] Tokens added to the inverted index for '1161'.
[indexDocument][L2] Indexing document '275'.
[indexDocument][L3] Opened file '275'.
[preprocess][L2] Starting preprocessing for '275'.
[preprocess][L3] 275: Tokenized into 126 tokens.
[preprocess][L3] 275: Removed stop words; 82 tokens remain.
[preprocess][L3] 275: Stemming complete.
[indexDocument][L3] Preprocessing complete for '275'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '275'.
[indexDocument][L3] Recorded doc length 82 for '275'.
[indexDocument][L3] Tokens added to the inverted index for '275'.
[indexDocument][L2] Indexing document '47'.
[indexDocument][L3] Opened file '47'.
[preprocess][L2] Starting preprocessing for '47'.
[preprocess][L3] 47: Tokenized into 203 tokens.
[preprocess][L3] 47: Removed stop words; 125 tokens remain.
[preprocess][L3] 47: Stemming complete.
[indexDocument][L3] Preprocessing complete for '47'; 125 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '47'.
[indexDocument][L3] Recorded doc length 125 for '47'.
[indexDocument][L3] Tokens added to the inverted index for '47'.
[indexDocument][L2] Indexing document '1195'.
[indexDocument][L3] Opened file '1195'.
[preprocess][L2] Starting preprocessing for '1195'.
[preprocess][L3] 1195: Tokenized into 263 tokens.
[preprocess][L3] 1195: Removed stop words; 168 tokens remain.
[preprocess][L3] 1195: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1195'; 168 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1195'.
[indexDocument][L3] Recorded doc length 168 for '1195'.
[indexDocument][L3] Tokens added to the inverted index for '1195'.
[indexDocument][L2] Indexing document '423'.
[indexDocument][L3] Opened file '423'.
[preprocess][L2] Starting preprocessing for '423'.
[preprocess][L3] 423: Tokenized into 323 tokens.
[preprocess][L3] 423: Removed stop words; 195 tokens remain.
[preprocess][L3] 423: Stemming complete.
[indexDocument][L3] Preprocessing complete for '423'; 195 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '423'.
[indexDocument][L3] Recorded doc length 195 for '423'.
[indexDocument][L3] Tokens added to the inverted index for '423'.
[indexDocument][L2] Indexing document '611'.
[indexDocument][L3] Opened file '611'.
[preprocess][L2] Starting preprocessing for '611'.
[preprocess][L3] 611: Tokenized into 170 tokens.
[preprocess][L3] 611: Removed stop words; 103 tokens remain.
[preprocess][L3] 611: Stemming complete.
[indexDocument][L3] Preprocessing complete for '611'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '611'.
[indexDocument][L3] Recorded doc length 103 for '611'.
[indexDocument][L3] Tokens added to the inverted index for '611'.
[indexDocument][L2] Indexing document '886'.
[indexDocument][L3] Opened file '886'.
[preprocess][L2] Starting preprocessing for '886'.
[preprocess][L3] 886: Tokenized into 79 tokens.
[preprocess][L3] 886: Removed stop words; 49 tokens remain.
[preprocess][L3] 886: Stemming complete.
[indexDocument][L3] Preprocessing complete for '886'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '886'.
[indexDocument][L3] Recorded doc length 49 for '886'.
[indexDocument][L3] Tokens added to the inverted index for '886'.
[indexDocument][L2] Indexing document '1168'.
[indexDocument][L3] Opened file '1168'.
[preprocess][L2] Starting preprocessing for '1168'.
[preprocess][L3] 1168: Tokenized into 129 tokens.
[preprocess][L3] 1168: Removed stop words; 78 tokens remain.
[preprocess][L3] 1168: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1168'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1168'.
[indexDocument][L3] Recorded doc length 78 for '1168'.
[indexDocument][L3] Tokens added to the inverted index for '1168'.
[indexDocument][L2] Indexing document '288'.
[indexDocument][L3] Opened file '288'.
[preprocess][L2] Starting preprocessing for '288'.
[preprocess][L3] 288: Tokenized into 197 tokens.
[preprocess][L3] 288: Removed stop words; 113 tokens remain.
[preprocess][L3] 288: Stemming complete.
[indexDocument][L3] Preprocessing complete for '288'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '288'.
[indexDocument][L3] Recorded doc length 113 for '288'.
[indexDocument][L3] Tokens added to the inverted index for '288'.
[indexDocument][L2] Indexing document '872'.
[indexDocument][L3] Opened file '872'.
[preprocess][L2] Starting preprocessing for '872'.
[preprocess][L3] 872: Tokenized into 207 tokens.
[preprocess][L3] 872: Removed stop words; 112 tokens remain.
[preprocess][L3] 872: Stemming complete.
[indexDocument][L3] Preprocessing complete for '872'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '872'.
[indexDocument][L3] Recorded doc length 112 for '872'.
[indexDocument][L3] Tokens added to the inverted index for '872'.
[indexDocument][L2] Indexing document '618'.
[indexDocument][L3] Opened file '618'.
[preprocess][L2] Starting preprocessing for '618'.
[preprocess][L3] 618: Tokenized into 149 tokens.
[preprocess][L3] 618: Removed stop words; 91 tokens remain.
[preprocess][L3] 618: Stemming complete.
[indexDocument][L3] Preprocessing complete for '618'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '618'.
[indexDocument][L3] Recorded doc length 91 for '618'.
[indexDocument][L3] Tokens added to the inverted index for '618'.
[indexDocument][L2] Indexing document '1365'.
[indexDocument][L3] Opened file '1365'.
[preprocess][L2] Starting preprocessing for '1365'.
[preprocess][L3] 1365: Tokenized into 154 tokens.
[preprocess][L3] 1365: Removed stop words; 89 tokens remain.
[preprocess][L3] 1365: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1365'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1365'.
[indexDocument][L3] Recorded doc length 89 for '1365'.
[indexDocument][L3] Tokens added to the inverted index for '1365'.
[indexDocument][L2] Indexing document '1157'.
[indexDocument][L3] Opened file '1157'.
[preprocess][L2] Starting preprocessing for '1157'.
[preprocess][L3] 1157: Tokenized into 211 tokens.
[preprocess][L3] 1157: Removed stop words; 132 tokens remain.
[preprocess][L3] 1157: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1157'; 132 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1157'.
[indexDocument][L3] Recorded doc length 132 for '1157'.
[indexDocument][L3] Tokens added to the inverted index for '1157'.
[indexDocument][L2] Indexing document '85'.
[indexDocument][L3] Opened file '85'.
[preprocess][L2] Starting preprocessing for '85'.
[preprocess][L3] 85: Tokenized into 291 tokens.
[preprocess][L3] 85: Removed stop words; 166 tokens remain.
[preprocess][L3] 85: Stemming complete.
[indexDocument][L3] Preprocessing complete for '85'; 166 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '85'.
[indexDocument][L3] Recorded doc length 166 for '85'.
[indexDocument][L3] Tokens added to the inverted index for '85'.
[indexDocument][L2] Indexing document '415'.
[indexDocument][L3] Opened file '415'.
[preprocess][L2] Starting preprocessing for '415'.
[preprocess][L3] 415: Tokenized into 123 tokens.
[preprocess][L3] 415: Removed stop words; 77 tokens remain.
[preprocess][L3] 415: Stemming complete.
[indexDocument][L3] Preprocessing complete for '415'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '415'.
[indexDocument][L3] Recorded doc length 77 for '415'.
[indexDocument][L3] Tokens added to the inverted index for '415'.
[indexDocument][L2] Indexing document '627'.
[indexDocument][L3] Opened file '627'.
[preprocess][L2] Starting preprocessing for '627'.
[preprocess][L3] 627: Tokenized into 146 tokens.
[preprocess][L3] 627: Removed stop words; 90 tokens remain.
[preprocess][L3] 627: Stemming complete.
[indexDocument][L3] Preprocessing complete for '627'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '627'.
[indexDocument][L3] Recorded doc length 90 for '627'.
[indexDocument][L3] Tokens added to the inverted index for '627'.
[indexDocument][L2] Indexing document '1391'.
[indexDocument][L3] Opened file '1391'.
[preprocess][L2] Starting preprocessing for '1391'.
[preprocess][L3] 1391: Tokenized into 220 tokens.
[preprocess][L3] 1391: Removed stop words; 131 tokens remain.
[preprocess][L3] 1391: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1391'; 131 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1391'.
[indexDocument][L3] Recorded doc length 131 for '1391'.
[indexDocument][L3] Tokens added to the inverted index for '1391'.
[indexDocument][L2] Indexing document '243'.
[indexDocument][L3] Opened file '243'.
[preprocess][L2] Starting preprocessing for '243'.
[preprocess][L3] 243: Tokenized into 119 tokens.
[preprocess][L3] 243: Removed stop words; 62 tokens remain.
[preprocess][L3] 243: Stemming complete.
[indexDocument][L3] Preprocessing complete for '243'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '243'.
[indexDocument][L3] Recorded doc length 62 for '243'.
[indexDocument][L3] Tokens added to the inverted index for '243'.
[indexDocument][L2] Indexing document '71'.
[indexDocument][L3] Opened file '71'.
[preprocess][L2] Starting preprocessing for '71'.
[preprocess][L3] 71: Tokenized into 84 tokens.
[preprocess][L3] 71: Removed stop words; 56 tokens remain.
[preprocess][L3] 71: Stemming complete.
[indexDocument][L3] Preprocessing complete for '71'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '71'.
[indexDocument][L3] Recorded doc length 56 for '71'.
[indexDocument][L3] Tokens added to the inverted index for '71'.
[indexDocument][L2] Indexing document '875'.
[indexDocument][L3] Opened file '875'.
[preprocess][L2] Starting preprocessing for '875'.
[preprocess][L3] 875: Tokenized into 46 tokens.
[preprocess][L3] 875: Removed stop words; 33 tokens remain.
[preprocess][L3] 875: Stemming complete.
[indexDocument][L3] Preprocessing complete for '875'; 33 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '875'.
[indexDocument][L3] Recorded doc length 33 for '875'.
[indexDocument][L3] Tokens added to the inverted index for '875'.
[indexDocument][L2] Indexing document '49'.
[indexDocument][L3] Opened file '49'.
[preprocess][L2] Starting preprocessing for '49'.
[preprocess][L3] 49: Tokenized into 400 tokens.
[preprocess][L3] 49: Removed stop words; 230 tokens remain.
[preprocess][L3] 49: Stemming complete.
[indexDocument][L3] Preprocessing complete for '49'; 230 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '49'.
[indexDocument][L3] Recorded doc length 230 for '49'.
[indexDocument][L3] Tokens added to the inverted index for '49'.
[indexDocument][L2] Indexing document '881'.
[indexDocument][L3] Opened file '881'.
[preprocess][L2] Starting preprocessing for '881'.
[preprocess][L3] 881: Tokenized into 155 tokens.
[preprocess][L3] 881: Removed stop words; 95 tokens remain.
[preprocess][L3] 881: Stemming complete.
[indexDocument][L3] Preprocessing complete for '881'; 95 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '881'.
[indexDocument][L3] Recorded doc length 95 for '881'.
[indexDocument][L3] Tokens added to the inverted index for '881'.
[indexDocument][L2] Indexing document '76'.
[indexDocument][L3] Opened file '76'.
[preprocess][L2] Starting preprocessing for '76'.
[preprocess][L3] 76: Tokenized into 198 tokens.
[preprocess][L3] 76: Removed stop words; 116 tokens remain.
[preprocess][L3] 76: Stemming complete.
[indexDocument][L3] Preprocessing complete for '76'; 116 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '76'.
[indexDocument][L3] Recorded doc length 116 for '76'.
[indexDocument][L3] Tokens added to the inverted index for '76'.
[indexDocument][L2] Indexing document '1396'.
[indexDocument][L3] Opened file '1396'.
[preprocess][L2] Starting preprocessing for '1396'.
[preprocess][L3] 1396: Tokenized into 118 tokens.
[preprocess][L3] 1396: Removed stop words; 69 tokens remain.
[preprocess][L3] 1396: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1396'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1396'.
[indexDocument][L3] Recorded doc length 69 for '1396'.
[indexDocument][L3] Tokens added to the inverted index for '1396'.
[indexDocument][L2] Indexing document '244'.
[indexDocument][L3] Opened file '244'.
[preprocess][L2] Starting preprocessing for '244'.
[preprocess][L3] 244: Tokenized into 489 tokens.
[preprocess][L3] 244: Removed stop words; 269 tokens remain.
[preprocess][L3] 244: Stemming complete.
[indexDocument][L3] Preprocessing complete for '244'; 269 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '244'.
[indexDocument][L3] Recorded doc length 269 for '244'.
[indexDocument][L3] Tokens added to the inverted index for '244'.
[indexDocument][L2] Indexing document '620'.
[indexDocument][L3] Opened file '620'.
[preprocess][L2] Starting preprocessing for '620'.
[preprocess][L3] 620: Tokenized into 251 tokens.
[preprocess][L3] 620: Removed stop words; 157 tokens remain.
[preprocess][L3] 620: Stemming complete.
[indexDocument][L3] Preprocessing complete for '620'; 157 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '620'.
[indexDocument][L3] Recorded doc length 157 for '620'.
[indexDocument][L3] Tokens added to the inverted index for '620'.
[indexDocument][L2] Indexing document '412'.
[indexDocument][L3] Opened file '412'.
[preprocess][L2] Starting preprocessing for '412'.
[preprocess][L3] 412: Tokenized into 114 tokens.
[preprocess][L3] 412: Removed stop words; 69 tokens remain.
[preprocess][L3] 412: Stemming complete.
[indexDocument][L3] Preprocessing complete for '412'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '412'.
[indexDocument][L3] Recorded doc length 69 for '412'.
[indexDocument][L3] Tokens added to the inverted index for '412'.
[indexDocument][L2] Indexing document '1150'.
[indexDocument][L3] Opened file '1150'.
[preprocess][L2] Starting preprocessing for '1150'.
[preprocess][L3] 1150: Tokenized into 104 tokens.
[preprocess][L3] 1150: Removed stop words; 67 tokens remain.
[preprocess][L3] 1150: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1150'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1150'.
[indexDocument][L3] Recorded doc length 67 for '1150'.
[indexDocument][L3] Tokens added to the inverted index for '1150'.
[indexDocument][L2] Indexing document '82'.
[indexDocument][L3] Opened file '82'.
[preprocess][L2] Starting preprocessing for '82'.
[preprocess][L3] 82: Tokenized into 318 tokens.
[preprocess][L3] 82: Removed stop words; 200 tokens remain.
[preprocess][L3] 82: Stemming complete.
[indexDocument][L3] Preprocessing complete for '82'; 200 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '82'.
[indexDocument][L3] Recorded doc length 200 for '82'.
[indexDocument][L3] Tokens added to the inverted index for '82'.
[indexDocument][L2] Indexing document '1362'.
[indexDocument][L3] Opened file '1362'.
[preprocess][L2] Starting preprocessing for '1362'.
[preprocess][L3] 1362: Tokenized into 151 tokens.
[preprocess][L3] 1362: Removed stop words; 94 tokens remain.
[preprocess][L3] 1362: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1362'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1362'.
[indexDocument][L3] Recorded doc length 94 for '1362'.
[indexDocument][L3] Tokens added to the inverted index for '1362'.
[indexDocument][L2] Indexing document '210'.
[indexDocument][L3] Opened file '210'.
[preprocess][L2] Starting preprocessing for '210'.
[preprocess][L3] 210: Tokenized into 335 tokens.
[preprocess][L3] 210: Removed stop words; 183 tokens remain.
[preprocess][L3] 210: Stemming complete.
[indexDocument][L3] Preprocessing complete for '210'; 183 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '210'.
[indexDocument][L3] Recorded doc length 183 for '210'.
[indexDocument][L3] Tokens added to the inverted index for '210'.
[indexDocument][L2] Indexing document '22'.
[indexDocument][L3] Opened file '22'.
[preprocess][L2] Starting preprocessing for '22'.
[preprocess][L3] 22: Tokenized into 99 tokens.
[preprocess][L3] 22: Removed stop words; 69 tokens remain.
[preprocess][L3] 22: Stemming complete.
[indexDocument][L3] Preprocessing complete for '22'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '22'.
[indexDocument][L3] Recorded doc length 69 for '22'.
[indexDocument][L3] Tokens added to the inverted index for '22'.
[indexDocument][L2] Indexing document '446'.
[indexDocument][L3] Opened file '446'.
[preprocess][L2] Starting preprocessing for '446'.
[preprocess][L3] 446: Tokenized into 141 tokens.
[preprocess][L3] 446: Removed stop words; 76 tokens remain.
[preprocess][L3] 446: Stemming complete.
[indexDocument][L3] Preprocessing complete for '446'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '446'.
[indexDocument][L3] Recorded doc length 76 for '446'.
[indexDocument][L3] Tokens added to the inverted index for '446'.
[indexDocument][L2] Indexing document '674'.
[indexDocument][L3] Opened file '674'.
[preprocess][L2] Starting preprocessing for '674'.
[preprocess][L3] 674: Tokenized into 131 tokens.
[preprocess][L3] 674: Removed stop words; 82 tokens remain.
[preprocess][L3] 674: Stemming complete.
[indexDocument][L3] Preprocessing complete for '674'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '674'.
[indexDocument][L3] Recorded doc length 82 for '674'.
[indexDocument][L3] Tokens added to the inverted index for '674'.
[indexDocument][L2] Indexing document '680'.
[indexDocument][L3] Opened file '680'.
[preprocess][L2] Starting preprocessing for '680'.
[preprocess][L3] 680: Tokenized into 143 tokens.
[preprocess][L3] 680: Removed stop words; 85 tokens remain.
[preprocess][L3] 680: Stemming complete.
[indexDocument][L3] Preprocessing complete for '680'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '680'.
[indexDocument][L3] Recorded doc length 85 for '680'.
[indexDocument][L3] Tokens added to the inverted index for '680'.
[indexDocument][L2] Indexing document '1336'.
[indexDocument][L3] Opened file '1336'.
[preprocess][L2] Starting preprocessing for '1336'.
[preprocess][L3] 1336: Tokenized into 201 tokens.
[preprocess][L3] 1336: Removed stop words; 122 tokens remain.
[preprocess][L3] 1336: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1336'; 122 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1336'.
[indexDocument][L3] Recorded doc length 122 for '1336'.
[indexDocument][L3] Tokens added to the inverted index for '1336'.
[indexDocument][L2] Indexing document '1104'.
[indexDocument][L3] Opened file '1104'.
[preprocess][L2] Starting preprocessing for '1104'.
[preprocess][L3] 1104: Tokenized into 313 tokens.
[preprocess][L3] 1104: Removed stop words; 192 tokens remain.
[preprocess][L3] 1104: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1104'; 192 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1104'.
[indexDocument][L3] Recorded doc length 192 for '1104'.
[indexDocument][L3] Tokens added to the inverted index for '1104'.
[indexDocument][L2] Indexing document '479'.
[indexDocument][L3] Opened file '479'.
[preprocess][L2] Starting preprocessing for '479'.
[preprocess][L3] 479: Tokenized into 182 tokens.
[preprocess][L3] 479: Removed stop words; 97 tokens remain.
[preprocess][L3] 479: Stemming complete.
[indexDocument][L3] Preprocessing complete for '479'; 97 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '479'.
[indexDocument][L3] Recorded doc length 97 for '479'.
[indexDocument][L3] Tokens added to the inverted index for '479'.
[indexDocument][L2] Indexing document '821'.
[indexDocument][L3] Opened file '821'.
[preprocess][L2] Starting preprocessing for '821'.
[preprocess][L3] 821: Tokenized into 80 tokens.
[preprocess][L3] 821: Removed stop words; 50 tokens remain.
[preprocess][L3] 821: Stemming complete.
[indexDocument][L3] Preprocessing complete for '821'; 50 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '821'.
[indexDocument][L3] Recorded doc length 50 for '821'.
[indexDocument][L3] Tokens added to the inverted index for '821'.
[indexDocument][L2] Indexing document '1309'.
[indexDocument][L3] Opened file '1309'.
[preprocess][L2] Starting preprocessing for '1309'.
[preprocess][L3] 1309: Tokenized into 230 tokens.
[preprocess][L3] 1309: Removed stop words; 141 tokens remain.
[preprocess][L3] 1309: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1309'; 141 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1309'.
[indexDocument][L3] Recorded doc length 141 for '1309'.
[indexDocument][L3] Tokens added to the inverted index for '1309'.
[indexDocument][L2] Indexing document '1103'.
[indexDocument][L3] Opened file '1103'.
[preprocess][L2] Starting preprocessing for '1103'.
[preprocess][L3] 1103: Tokenized into 95 tokens.
[preprocess][L3] 1103: Removed stop words; 60 tokens remain.
[preprocess][L3] 1103: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1103'; 60 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1103'.
[indexDocument][L3] Recorded doc length 60 for '1103'.
[indexDocument][L3] Tokens added to the inverted index for '1103'.
[indexDocument][L2] Indexing document '1331'.
[indexDocument][L3] Opened file '1331'.
[preprocess][L2] Starting preprocessing for '1331'.
[preprocess][L3] 1331: Tokenized into 93 tokens.
[preprocess][L3] 1331: Removed stop words; 63 tokens remain.
[preprocess][L3] 1331: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1331'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1331'.
[indexDocument][L3] Recorded doc length 63 for '1331'.
[indexDocument][L3] Tokens added to the inverted index for '1331'.
[indexDocument][L2] Indexing document '687'.
[indexDocument][L3] Opened file '687'.
[preprocess][L2] Starting preprocessing for '687'.
[preprocess][L3] 687: Tokenized into 97 tokens.
[preprocess][L3] 687: Removed stop words; 64 tokens remain.
[preprocess][L3] 687: Stemming complete.
[indexDocument][L3] Preprocessing complete for '687'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '687'.
[indexDocument][L3] Recorded doc length 64 for '687'.
[indexDocument][L3] Tokens added to the inverted index for '687'.
[indexDocument][L2] Indexing document '673'.
[indexDocument][L3] Opened file '673'.
[preprocess][L2] Starting preprocessing for '673'.
[preprocess][L3] 673: Tokenized into 393 tokens.
[preprocess][L3] 673: Removed stop words; 235 tokens remain.
[preprocess][L3] 673: Stemming complete.
[indexDocument][L3] Preprocessing complete for '673'; 235 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '673'.
[indexDocument][L3] Recorded doc length 235 for '673'.
[indexDocument][L3] Tokens added to the inverted index for '673'.
[indexDocument][L2] Indexing document '441'.
[indexDocument][L3] Opened file '441'.
[preprocess][L2] Starting preprocessing for '441'.
[preprocess][L3] 441: Tokenized into 271 tokens.
[preprocess][L3] 441: Removed stop words; 167 tokens remain.
[preprocess][L3] 441: Stemming complete.
[indexDocument][L3] Preprocessing complete for '441'; 167 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '441'.
[indexDocument][L3] Recorded doc length 167 for '441'.
[indexDocument][L3] Tokens added to the inverted index for '441'.
[indexDocument][L2] Indexing document '25'.
[indexDocument][L3] Opened file '25'.
[preprocess][L2] Starting preprocessing for '25'.
[preprocess][L3] 25: Tokenized into 380 tokens.
[preprocess][L3] 25: Removed stop words; 225 tokens remain.
[preprocess][L3] 25: Stemming complete.
[indexDocument][L3] Preprocessing complete for '25'; 225 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '25'.
[indexDocument][L3] Recorded doc length 225 for '25'.
[indexDocument][L3] Tokens added to the inverted index for '25'.
[indexDocument][L2] Indexing document '819'.
[indexDocument][L3] Opened file '819'.
[preprocess][L2] Starting preprocessing for '819'.
[preprocess][L3] 819: Tokenized into 109 tokens.
[preprocess][L3] 819: Removed stop words; 67 tokens remain.
[preprocess][L3] 819: Stemming complete.
[indexDocument][L3] Preprocessing complete for '819'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '819'.
[indexDocument][L3] Recorded doc length 67 for '819'.
[indexDocument][L3] Tokens added to the inverted index for '819'.
[indexDocument][L2] Indexing document '217'.
[indexDocument][L3] Opened file '217'.
[preprocess][L2] Starting preprocessing for '217'.
[preprocess][L3] 217: Tokenized into 126 tokens.
[preprocess][L3] 217: Removed stop words; 71 tokens remain.
[preprocess][L3] 217: Stemming complete.
[indexDocument][L3] Preprocessing complete for '217'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '217'.
[indexDocument][L3] Recorded doc length 71 for '217'.
[indexDocument][L3] Tokens added to the inverted index for '217'.
[indexDocument][L2] Indexing document '228'.
[indexDocument][L3] Opened file '228'.
[preprocess][L2] Starting preprocessing for '228'.
[preprocess][L3] 228: Tokenized into 113 tokens.
[preprocess][L3] 228: Removed stop words; 59 tokens remain.
[preprocess][L3] 228: Stemming complete.
[indexDocument][L3] Preprocessing complete for '228'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '228'.
[indexDocument][L3] Recorded doc length 59 for '228'.
[indexDocument][L3] Tokens added to the inverted index for '228'.
[indexDocument][L2] Indexing document '826'.
[indexDocument][L3] Opened file '826'.
[preprocess][L2] Starting preprocessing for '826'.
[preprocess][L3] 826: Tokenized into 346 tokens.
[preprocess][L3] 826: Removed stop words; 189 tokens remain.
[preprocess][L3] 826: Stemming complete.
[indexDocument][L3] Preprocessing complete for '826'; 189 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '826'.
[indexDocument][L3] Recorded doc length 189 for '826'.
[indexDocument][L3] Tokens added to the inverted index for '826'.
[indexDocument][L2] Indexing document '483'.
[indexDocument][L3] Opened file '483'.
[preprocess][L2] Starting preprocessing for '483'.
[preprocess][L3] 483: Tokenized into 48 tokens.
[preprocess][L3] 483: Removed stop words; 34 tokens remain.
[preprocess][L3] 483: Stemming complete.
[indexDocument][L3] Preprocessing complete for '483'; 34 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '483'.
[indexDocument][L3] Recorded doc length 34 for '483'.
[indexDocument][L3] Tokens added to the inverted index for '483'.
[indexDocument][L2] Indexing document '1135'.
[indexDocument][L3] Opened file '1135'.
[preprocess][L2] Starting preprocessing for '1135'.
[preprocess][L3] 1135: Tokenized into 130 tokens.
[preprocess][L3] 1135: Removed stop words; 80 tokens remain.
[preprocess][L3] 1135: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1135'; 80 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1135'.
[indexDocument][L3] Recorded doc length 80 for '1135'.
[indexDocument][L3] Tokens added to the inverted index for '1135'.
[indexDocument][L2] Indexing document '1307'.
[indexDocument][L3] Opened file '1307'.
[preprocess][L2] Starting preprocessing for '1307'.
[preprocess][L3] 1307: Tokenized into 267 tokens.
[preprocess][L3] 1307: Removed stop words; 159 tokens remain.
[preprocess][L3] 1307: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1307'; 159 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1307'.
[indexDocument][L3] Recorded doc length 159 for '1307'.
[indexDocument][L3] Tokens added to the inverted index for '1307'.
[indexDocument][L2] Indexing document '13'.
[indexDocument][L3] Opened file '13'.
[preprocess][L2] Starting preprocessing for '13'.
[preprocess][L3] 13: Tokenized into 147 tokens.
[preprocess][L3] 13: Removed stop words; 85 tokens remain.
[preprocess][L3] 13: Stemming complete.
[indexDocument][L3] Preprocessing complete for '13'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '13'.
[indexDocument][L3] Recorded doc length 85 for '13'.
[indexDocument][L3] Tokens added to the inverted index for '13'.
[indexDocument][L2] Indexing document '221'.
[indexDocument][L3] Opened file '221'.
[preprocess][L2] Starting preprocessing for '221'.
[preprocess][L3] 221: Tokenized into 163 tokens.
[preprocess][L3] 221: Removed stop words; 100 tokens remain.
[preprocess][L3] 221: Stemming complete.
[indexDocument][L3] Preprocessing complete for '221'; 100 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '221'.
[indexDocument][L3] Recorded doc length 100 for '221'.
[indexDocument][L3] Tokens added to the inverted index for '221'.
[indexDocument][L2] Indexing document '645'.
[indexDocument][L3] Opened file '645'.
[preprocess][L2] Starting preprocessing for '645'.
[preprocess][L3] 645: Tokenized into 151 tokens.
[preprocess][L3] 645: Removed stop words; 97 tokens remain.
[preprocess][L3] 645: Stemming complete.
[indexDocument][L3] Preprocessing complete for '645'; 97 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '645'.
[indexDocument][L3] Recorded doc length 97 for '645'.
[indexDocument][L3] Tokens added to the inverted index for '645'.
[indexDocument][L2] Indexing document '477'.
[indexDocument][L3] Opened file '477'.
[preprocess][L2] Starting preprocessing for '477'.
[preprocess][L3] 477: Tokenized into 154 tokens.
[preprocess][L3] 477: Removed stop words; 86 tokens remain.
[preprocess][L3] 477: Stemming complete.
[indexDocument][L3] Preprocessing complete for '477'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '477'.
[indexDocument][L3] Recorded doc length 86 for '477'.
[indexDocument][L3] Tokens added to the inverted index for '477'.
[indexDocument][L2] Indexing document '1338'.
[indexDocument][L3] Opened file '1338'.
[preprocess][L2] Starting preprocessing for '1338'.
[preprocess][L3] 1338: Tokenized into 197 tokens.
[preprocess][L3] 1338: Removed stop words; 131 tokens remain.
[preprocess][L3] 1338: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1338'; 131 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1338'.
[indexDocument][L3] Recorded doc length 131 for '1338'.
[indexDocument][L3] Tokens added to the inverted index for '1338'.
[indexDocument][L2] Indexing document '448'.
[indexDocument][L3] Opened file '448'.
[preprocess][L2] Starting preprocessing for '448'.
[preprocess][L3] 448: Tokenized into 145 tokens.
[preprocess][L3] 448: Removed stop words; 91 tokens remain.
[preprocess][L3] 448: Stemming complete.
[indexDocument][L3] Preprocessing complete for '448'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '448'.
[indexDocument][L3] Recorded doc length 91 for '448'.
[indexDocument][L3] Tokens added to the inverted index for '448'.
[indexDocument][L2] Indexing document '810'.
[indexDocument][L3] Opened file '810'.
[preprocess][L2] Starting preprocessing for '810'.
[preprocess][L3] 810: Tokenized into 197 tokens.
[preprocess][L3] 810: Removed stop words; 112 tokens remain.
[preprocess][L3] 810: Stemming complete.
[indexDocument][L3] Preprocessing complete for '810'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '810'.
[indexDocument][L3] Recorded doc length 112 for '810'.
[indexDocument][L3] Tokens added to the inverted index for '810'.
[indexDocument][L2] Indexing document '470'.
[indexDocument][L3] Opened file '470'.
[preprocess][L2] Starting preprocessing for '470'.
[preprocess][L3] 470: Tokenized into 121 tokens.
[preprocess][L3] 470: Removed stop words; 69 tokens remain.
[preprocess][L3] 470: Stemming complete.
[indexDocument][L3] Preprocessing complete for '470'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '470'.
[indexDocument][L3] Recorded doc length 69 for '470'.
[indexDocument][L3] Tokens added to the inverted index for '470'.
[indexDocument][L2] Indexing document '642'.
[indexDocument][L3] Opened file '642'.
[preprocess][L2] Starting preprocessing for '642'.
[preprocess][L3] 642: Tokenized into 86 tokens.
[preprocess][L3] 642: Removed stop words; 52 tokens remain.
[preprocess][L3] 642: Stemming complete.
[indexDocument][L3] Preprocessing complete for '642'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '642'.
[indexDocument][L3] Recorded doc length 52 for '642'.
[indexDocument][L3] Tokens added to the inverted index for '642'.
[indexDocument][L2] Indexing document '226'.
[indexDocument][L3] Opened file '226'.
[preprocess][L2] Starting preprocessing for '226'.
[preprocess][L3] 226: Tokenized into 77 tokens.
[preprocess][L3] 226: Removed stop words; 47 tokens remain.
[preprocess][L3] 226: Stemming complete.
[indexDocument][L3] Preprocessing complete for '226'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '226'.
[indexDocument][L3] Recorded doc length 47 for '226'.
[indexDocument][L3] Tokens added to the inverted index for '226'.
[indexDocument][L2] Indexing document '14'.
[indexDocument][L3] Opened file '14'.
[preprocess][L2] Starting preprocessing for '14'.
[preprocess][L3] 14: Tokenized into 373 tokens.
[preprocess][L3] 14: Removed stop words; 232 tokens remain.
[preprocess][L3] 14: Stemming complete.
[indexDocument][L3] Preprocessing complete for '14'; 232 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '14'.
[indexDocument][L3] Recorded doc length 232 for '14'.
[indexDocument][L3] Tokens added to the inverted index for '14'.
[indexDocument][L2] Indexing document '828'.
[indexDocument][L3] Opened file '828'.
[preprocess][L2] Starting preprocessing for '828'.
[preprocess][L3] 828: Tokenized into 243 tokens.
[preprocess][L3] 828: Removed stop words; 128 tokens remain.
[preprocess][L3] 828: Stemming complete.
[indexDocument][L3] Preprocessing complete for '828'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '828'.
[indexDocument][L3] Recorded doc length 128 for '828'.
[indexDocument][L3] Tokens added to the inverted index for '828'.
[indexDocument][L2] Indexing document '1300'.
[indexDocument][L3] Opened file '1300'.
[preprocess][L2] Starting preprocessing for '1300'.
[preprocess][L3] 1300: Tokenized into 297 tokens.
[preprocess][L3] 1300: Removed stop words; 178 tokens remain.
[preprocess][L3] 1300: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1300'; 178 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1300'.
[indexDocument][L3] Recorded doc length 178 for '1300'.
[indexDocument][L3] Tokens added to the inverted index for '1300'.
[indexDocument][L2] Indexing document '1132'.
[indexDocument][L3] Opened file '1132'.
[preprocess][L2] Starting preprocessing for '1132'.
[preprocess][L3] 1132: Tokenized into 97 tokens.
[preprocess][L3] 1132: Removed stop words; 66 tokens remain.
[preprocess][L3] 1132: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1132'; 66 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1132'.
[indexDocument][L3] Recorded doc length 66 for '1132'.
[indexDocument][L3] Tokens added to the inverted index for '1132'.
[indexDocument][L2] Indexing document '484'.
[indexDocument][L3] Opened file '484'.
[preprocess][L2] Starting preprocessing for '484'.
[preprocess][L3] 484: Tokenized into 280 tokens.
[preprocess][L3] 484: Removed stop words; 159 tokens remain.
[preprocess][L3] 484: Stemming complete.
[indexDocument][L3] Preprocessing complete for '484'; 159 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '484'.
[indexDocument][L3] Recorded doc length 159 for '484'.
[indexDocument][L3] Tokens added to the inverted index for '484'.
[indexDocument][L2] Indexing document '817'.
[indexDocument][L3] Opened file '817'.
[preprocess][L2] Starting preprocessing for '817'.
[preprocess][L3] 817: Tokenized into 95 tokens.
[preprocess][L3] 817: Removed stop words; 56 tokens remain.
[preprocess][L3] 817: Stemming complete.
[indexDocument][L3] Preprocessing complete for '817'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '817'.
[indexDocument][L3] Recorded doc length 56 for '817'.
[indexDocument][L3] Tokens added to the inverted index for '817'.
[indexDocument][L2] Indexing document '219'.
[indexDocument][L3] Opened file '219'.
[preprocess][L2] Starting preprocessing for '219'.
[preprocess][L3] 219: Tokenized into 222 tokens.
[preprocess][L3] 219: Removed stop words; 131 tokens remain.
[preprocess][L3] 219: Stemming complete.
[indexDocument][L3] Preprocessing complete for '219'; 131 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '219'.
[indexDocument][L3] Recorded doc length 131 for '219'.
[indexDocument][L3] Tokens added to the inverted index for '219'.
[indexDocument][L2] Indexing document '689'.
[indexDocument][L3] Opened file '689'.
[preprocess][L2] Starting preprocessing for '689'.
[preprocess][L3] 689: Tokenized into 265 tokens.
[preprocess][L3] 689: Removed stop words; 161 tokens remain.
[preprocess][L3] 689: Stemming complete.
[indexDocument][L3] Preprocessing complete for '689'; 161 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '689'.
[indexDocument][L3] Recorded doc length 161 for '689'.
[indexDocument][L3] Tokens added to the inverted index for '689'.
[indexDocument][L2] Indexing document '880'.
[indexDocument][L3] Opened file '880'.
[preprocess][L2] Starting preprocessing for '880'.
[preprocess][L3] 880: Tokenized into 79 tokens.
[preprocess][L3] 880: Removed stop words; 47 tokens remain.
[preprocess][L3] 880: Stemming complete.
[indexDocument][L3] Preprocessing complete for '880'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '880'.
[indexDocument][L3] Recorded doc length 47 for '880'.
[indexDocument][L3] Tokens added to the inverted index for '880'.
[indexDocument][L2] Indexing document '48'.
[indexDocument][L3] Opened file '48'.
[preprocess][L2] Starting preprocessing for '48'.
[preprocess][L3] 48: Tokenized into 126 tokens.
[preprocess][L3] 48: Removed stop words; 74 tokens remain.
[preprocess][L3] 48: Stemming complete.
[indexDocument][L3] Preprocessing complete for '48'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '48'.
[indexDocument][L3] Recorded doc length 74 for '48'.
[indexDocument][L3] Tokens added to the inverted index for '48'.
[indexDocument][L2] Indexing document '874'.
[indexDocument][L3] Opened file '874'.
[preprocess][L2] Starting preprocessing for '874'.
[preprocess][L3] 874: Tokenized into 319 tokens.
[preprocess][L3] 874: Removed stop words; 161 tokens remain.
[preprocess][L3] 874: Stemming complete.
[indexDocument][L3] Preprocessing complete for '874'; 161 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '874'.
[indexDocument][L3] Recorded doc length 161 for '874'.
[indexDocument][L3] Tokens added to the inverted index for '874'.
[indexDocument][L2] Indexing document '1363'.
[indexDocument][L3] Opened file '1363'.
[preprocess][L2] Starting preprocessing for '1363'.
[preprocess][L3] 1363: Tokenized into 167 tokens.
[preprocess][L3] 1363: Removed stop words; 104 tokens remain.
[preprocess][L3] 1363: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1363'; 104 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1363'.
[indexDocument][L3] Recorded doc length 104 for '1363'.
[indexDocument][L3] Tokens added to the inverted index for '1363'.
[indexDocument][L2] Indexing document '1151'.
[indexDocument][L3] Opened file '1151'.
[preprocess][L2] Starting preprocessing for '1151'.
[preprocess][L3] 1151: Tokenized into 139 tokens.
[preprocess][L3] 1151: Removed stop words; 88 tokens remain.
[preprocess][L3] 1151: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1151'; 88 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1151'.
[indexDocument][L3] Recorded doc length 88 for '1151'.
[indexDocument][L3] Tokens added to the inverted index for '1151'.
[indexDocument][L2] Indexing document '83'.
[indexDocument][L3] Opened file '83'.
[preprocess][L2] Starting preprocessing for '83'.
[preprocess][L3] 83: Tokenized into 317 tokens.
[preprocess][L3] 83: Removed stop words; 179 tokens remain.
[preprocess][L3] 83: Stemming complete.
[indexDocument][L3] Preprocessing complete for '83'; 179 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '83'.
[indexDocument][L3] Recorded doc length 179 for '83'.
[indexDocument][L3] Tokens added to the inverted index for '83'.
[indexDocument][L2] Indexing document '1397'.
[indexDocument][L3] Opened file '1397'.
[preprocess][L2] Starting preprocessing for '1397'.
[preprocess][L3] 1397: Tokenized into 86 tokens.
[preprocess][L3] 1397: Removed stop words; 57 tokens remain.
[preprocess][L3] 1397: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1397'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1397'.
[indexDocument][L3] Recorded doc length 57 for '1397'.
[indexDocument][L3] Tokens added to the inverted index for '1397'.
[indexDocument][L2] Indexing document '245'.
[indexDocument][L3] Opened file '245'.
[preprocess][L2] Starting preprocessing for '245'.
[preprocess][L3] 245: Tokenized into 163 tokens.
[preprocess][L3] 245: Removed stop words; 94 tokens remain.
[preprocess][L3] 245: Stemming complete.
[indexDocument][L3] Preprocessing complete for '245'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '245'.
[indexDocument][L3] Recorded doc length 94 for '245'.
[indexDocument][L3] Tokens added to the inverted index for '245'.
[indexDocument][L2] Indexing document '77'.
[indexDocument][L3] Opened file '77'.
[preprocess][L2] Starting preprocessing for '77'.
[preprocess][L3] 77: Tokenized into 349 tokens.
[preprocess][L3] 77: Removed stop words; 203 tokens remain.
[preprocess][L3] 77: Stemming complete.
[indexDocument][L3] Preprocessing complete for '77'; 203 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '77'.
[indexDocument][L3] Recorded doc length 203 for '77'.
[indexDocument][L3] Tokens added to the inverted index for '77'.
[indexDocument][L2] Indexing document '413'.
[indexDocument][L3] Opened file '413'.
[preprocess][L2] Starting preprocessing for '413'.
[preprocess][L3] 413: Tokenized into 145 tokens.
[preprocess][L3] 413: Removed stop words; 98 tokens remain.
[preprocess][L3] 413: Stemming complete.
[indexDocument][L3] Preprocessing complete for '413'; 98 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '413'.
[indexDocument][L3] Recorded doc length 98 for '413'.
[indexDocument][L3] Tokens added to the inverted index for '413'.
[indexDocument][L2] Indexing document '621'.
[indexDocument][L3] Opened file '621'.
[preprocess][L2] Starting preprocessing for '621'.
[preprocess][L3] 621: Tokenized into 125 tokens.
[preprocess][L3] 621: Removed stop words; 74 tokens remain.
[preprocess][L3] 621: Stemming complete.
[indexDocument][L3] Preprocessing complete for '621'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '621'.
[indexDocument][L3] Recorded doc length 74 for '621'.
[indexDocument][L3] Tokens added to the inverted index for '621'.
[indexDocument][L2] Indexing document '873'.
[indexDocument][L3] Opened file '873'.
[preprocess][L2] Starting preprocessing for '873'.
[preprocess][L3] 873: Tokenized into 164 tokens.
[preprocess][L3] 873: Removed stop words; 107 tokens remain.
[preprocess][L3] 873: Stemming complete.
[indexDocument][L3] Preprocessing complete for '873'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '873'.
[indexDocument][L3] Recorded doc length 107 for '873'.
[indexDocument][L3] Tokens added to the inverted index for '873'.
[indexDocument][L2] Indexing document '619'.
[indexDocument][L3] Opened file '619'.
[preprocess][L2] Starting preprocessing for '619'.
[preprocess][L3] 619: Tokenized into 53 tokens.
[preprocess][L3] 619: Removed stop words; 36 tokens remain.
[preprocess][L3] 619: Stemming complete.
[indexDocument][L3] Preprocessing complete for '619'; 36 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '619'.
[indexDocument][L3] Recorded doc length 36 for '619'.
[indexDocument][L3] Tokens added to the inverted index for '619'.
[indexDocument][L2] Indexing document '289'.
[indexDocument][L3] Opened file '289'.
[preprocess][L2] Starting preprocessing for '289'.
[preprocess][L3] 289: Tokenized into 248 tokens.
[preprocess][L3] 289: Removed stop words; 140 tokens remain.
[preprocess][L3] 289: Stemming complete.
[indexDocument][L3] Preprocessing complete for '289'; 140 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '289'.
[indexDocument][L3] Recorded doc length 140 for '289'.
[indexDocument][L3] Tokens added to the inverted index for '289'.
[indexDocument][L2] Indexing document '1169'.
[indexDocument][L3] Opened file '1169'.
[preprocess][L2] Starting preprocessing for '1169'.
[preprocess][L3] 1169: Tokenized into 165 tokens.
[preprocess][L3] 1169: Removed stop words; 96 tokens remain.
[preprocess][L3] 1169: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1169'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1169'.
[indexDocument][L3] Recorded doc length 96 for '1169'.
[indexDocument][L3] Tokens added to the inverted index for '1169'.
[indexDocument][L2] Indexing document '887'.
[indexDocument][L3] Opened file '887'.
[preprocess][L2] Starting preprocessing for '887'.
[preprocess][L3] 887: Tokenized into 109 tokens.
[preprocess][L3] 887: Removed stop words; 71 tokens remain.
[preprocess][L3] 887: Stemming complete.
[indexDocument][L3] Preprocessing complete for '887'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '887'.
[indexDocument][L3] Recorded doc length 71 for '887'.
[indexDocument][L3] Tokens added to the inverted index for '887'.
[indexDocument][L2] Indexing document '626'.
[indexDocument][L3] Opened file '626'.
[preprocess][L2] Starting preprocessing for '626'.
[preprocess][L3] 626: Tokenized into 181 tokens.
[preprocess][L3] 626: Removed stop words; 106 tokens remain.
[preprocess][L3] 626: Stemming complete.
[indexDocument][L3] Preprocessing complete for '626'; 106 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '626'.
[indexDocument][L3] Recorded doc length 106 for '626'.
[indexDocument][L3] Tokens added to the inverted index for '626'.
[indexDocument][L2] Indexing document '414'.
[indexDocument][L3] Opened file '414'.
[preprocess][L2] Starting preprocessing for '414'.
[preprocess][L3] 414: Tokenized into 217 tokens.
[preprocess][L3] 414: Removed stop words; 117 tokens remain.
[preprocess][L3] 414: Stemming complete.
[indexDocument][L3] Preprocessing complete for '414'; 117 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '414'.
[indexDocument][L3] Recorded doc length 117 for '414'.
[indexDocument][L3] Tokens added to the inverted index for '414'.
[indexDocument][L2] Indexing document '70'.
[indexDocument][L3] Opened file '70'.
[preprocess][L2] Starting preprocessing for '70'.
[preprocess][L3] 70: Tokenized into 209 tokens.
[preprocess][L3] 70: Removed stop words; 127 tokens remain.
[preprocess][L3] 70: Stemming complete.
[indexDocument][L3] Preprocessing complete for '70'; 127 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '70'.
[indexDocument][L3] Recorded doc length 127 for '70'.
[indexDocument][L3] Tokens added to the inverted index for '70'.
[indexDocument][L2] Indexing document '1390'.
[indexDocument][L3] Opened file '1390'.
[preprocess][L2] Starting preprocessing for '1390'.
[preprocess][L3] 1390: Tokenized into 180 tokens.
[preprocess][L3] 1390: Removed stop words; 106 tokens remain.
[preprocess][L3] 1390: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1390'; 106 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1390'.
[indexDocument][L3] Recorded doc length 106 for '1390'.
[indexDocument][L3] Tokens added to the inverted index for '1390'.
[indexDocument][L2] Indexing document '242'.
[indexDocument][L3] Opened file '242'.
[preprocess][L2] Starting preprocessing for '242'.
[preprocess][L3] 242: Tokenized into 74 tokens.
[preprocess][L3] 242: Removed stop words; 47 tokens remain.
[preprocess][L3] 242: Stemming complete.
[indexDocument][L3] Preprocessing complete for '242'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '242'.
[indexDocument][L3] Recorded doc length 47 for '242'.
[indexDocument][L3] Tokens added to the inverted index for '242'.
[indexDocument][L2] Indexing document '1156'.
[indexDocument][L3] Opened file '1156'.
[preprocess][L2] Starting preprocessing for '1156'.
[preprocess][L3] 1156: Tokenized into 194 tokens.
[preprocess][L3] 1156: Removed stop words; 117 tokens remain.
[preprocess][L3] 1156: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1156'; 117 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1156'.
[indexDocument][L3] Recorded doc length 117 for '1156'.
[indexDocument][L3] Tokens added to the inverted index for '1156'.
[indexDocument][L2] Indexing document '84'.
[indexDocument][L3] Opened file '84'.
[preprocess][L2] Starting preprocessing for '84'.
[preprocess][L3] 84: Tokenized into 184 tokens.
[preprocess][L3] 84: Removed stop words; 113 tokens remain.
[preprocess][L3] 84: Stemming complete.
[indexDocument][L3] Preprocessing complete for '84'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '84'.
[indexDocument][L3] Recorded doc length 113 for '84'.
[indexDocument][L3] Tokens added to the inverted index for '84'.
[indexDocument][L2] Indexing document '1364'.
[indexDocument][L3] Opened file '1364'.
[preprocess][L2] Starting preprocessing for '1364'.
[preprocess][L3] 1364: Tokenized into 230 tokens.
[preprocess][L3] 1364: Removed stop words; 128 tokens remain.
[preprocess][L3] 1364: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1364'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1364'.
[indexDocument][L3] Recorded doc length 128 for '1364'.
[indexDocument][L3] Tokens added to the inverted index for '1364'.
[indexDocument][L2] Indexing document '1399'.
[indexDocument][L3] Opened file '1399'.
[preprocess][L2] Starting preprocessing for '1399'.
[preprocess][L3] 1399: Tokenized into 90 tokens.
[preprocess][L3] 1399: Removed stop words; 58 tokens remain.
[preprocess][L3] 1399: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1399'; 58 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1399'.
[indexDocument][L3] Recorded doc length 58 for '1399'.
[indexDocument][L3] Tokens added to the inverted index for '1399'.
[indexDocument][L2] Indexing document '79'.
[indexDocument][L3] Opened file '79'.
[preprocess][L2] Starting preprocessing for '79'.
[preprocess][L3] 79: Tokenized into 168 tokens.
[preprocess][L3] 79: Removed stop words; 109 tokens remain.
[preprocess][L3] 79: Stemming complete.
[indexDocument][L3] Preprocessing complete for '79'; 109 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '79'.
[indexDocument][L3] Recorded doc length 109 for '79'.
[indexDocument][L3] Tokens added to the inverted index for '79'.
[indexDocument][L2] Indexing document '845'.
[indexDocument][L3] Opened file '845'.
[preprocess][L2] Starting preprocessing for '845'.
[preprocess][L3] 845: Tokenized into 240 tokens.
[preprocess][L3] 845: Removed stop words; 132 tokens remain.
[preprocess][L3] 845: Stemming complete.
[indexDocument][L3] Preprocessing complete for '845'; 132 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '845'.
[indexDocument][L3] Recorded doc length 132 for '845'.
[indexDocument][L3] Tokens added to the inverted index for '845'.
[indexDocument][L2] Indexing document '46'.
[indexDocument][L3] Opened file '46'.
[preprocess][L2] Starting preprocessing for '46'.
[preprocess][L3] 46: Tokenized into 99 tokens.
[preprocess][L3] 46: Removed stop words; 52 tokens remain.
[preprocess][L3] 46: Stemming complete.
[indexDocument][L3] Preprocessing complete for '46'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '46'.
[indexDocument][L3] Recorded doc length 52 for '46'.
[indexDocument][L3] Tokens added to the inverted index for '46'.
[indexDocument][L2] Indexing document '1194'.
[indexDocument][L3] Opened file '1194'.
[preprocess][L2] Starting preprocessing for '1194'.
[preprocess][L3] 1194: Tokenized into 166 tokens.
[preprocess][L3] 1194: Removed stop words; 102 tokens remain.
[preprocess][L3] 1194: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1194'; 102 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1194'.
[indexDocument][L3] Recorded doc length 102 for '1194'.
[indexDocument][L3] Tokens added to the inverted index for '1194'.
[indexDocument][L2] Indexing document '274'.
[indexDocument][L3] Opened file '274'.
[preprocess][L2] Starting preprocessing for '274'.
[preprocess][L3] 274: Tokenized into 258 tokens.
[preprocess][L3] 274: Removed stop words; 153 tokens remain.
[preprocess][L3] 274: Stemming complete.
[indexDocument][L3] Preprocessing complete for '274'; 153 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '274'.
[indexDocument][L3] Recorded doc length 153 for '274'.
[indexDocument][L3] Tokens added to the inverted index for '274'.
[indexDocument][L2] Indexing document '610'.
[indexDocument][L3] Opened file '610'.
[preprocess][L2] Starting preprocessing for '610'.
[preprocess][L3] 610: Tokenized into 149 tokens.
[preprocess][L3] 610: Removed stop words; 90 tokens remain.
[preprocess][L3] 610: Stemming complete.
[indexDocument][L3] Preprocessing complete for '610'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '610'.
[indexDocument][L3] Recorded doc length 90 for '610'.
[indexDocument][L3] Tokens added to the inverted index for '610'.
[indexDocument][L2] Indexing document '422'.
[indexDocument][L3] Opened file '422'.
[preprocess][L2] Starting preprocessing for '422'.
[preprocess][L3] 422: Tokenized into 123 tokens.
[preprocess][L3] 422: Removed stop words; 76 tokens remain.
[preprocess][L3] 422: Stemming complete.
[indexDocument][L3] Preprocessing complete for '422'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '422'.
[indexDocument][L3] Recorded doc length 76 for '422'.
[indexDocument][L3] Tokens added to the inverted index for '422'.
[indexDocument][L2] Indexing document '1160'.
[indexDocument][L3] Opened file '1160'.
[preprocess][L2] Starting preprocessing for '1160'.
[preprocess][L3] 1160: Tokenized into 120 tokens.
[preprocess][L3] 1160: Removed stop words; 81 tokens remain.
[preprocess][L3] 1160: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1160'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1160'.
[indexDocument][L3] Recorded doc length 81 for '1160'.
[indexDocument][L3] Tokens added to the inverted index for '1160'.
[indexDocument][L2] Indexing document '280'.
[indexDocument][L3] Opened file '280'.
[preprocess][L2] Starting preprocessing for '280'.
[preprocess][L3] 280: Tokenized into 148 tokens.
[preprocess][L3] 280: Removed stop words; 87 tokens remain.
[preprocess][L3] 280: Stemming complete.
[indexDocument][L3] Preprocessing complete for '280'; 87 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '280'.
[indexDocument][L3] Recorded doc length 87 for '280'.
[indexDocument][L3] Tokens added to the inverted index for '280'.
[indexDocument][L2] Indexing document '1352'.
[indexDocument][L3] Opened file '1352'.
[preprocess][L2] Starting preprocessing for '1352'.
[preprocess][L3] 1352: Tokenized into 242 tokens.
[preprocess][L3] 1352: Removed stop words; 145 tokens remain.
[preprocess][L3] 1352: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1352'; 145 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1352'.
[indexDocument][L3] Recorded doc length 145 for '1352'.
[indexDocument][L3] Tokens added to the inverted index for '1352'.
[indexDocument][L2] Indexing document '1158'.
[indexDocument][L3] Opened file '1158'.
[preprocess][L2] Starting preprocessing for '1158'.
[preprocess][L3] 1158: Tokenized into 85 tokens.
[preprocess][L3] 1158: Removed stop words; 59 tokens remain.
[preprocess][L3] 1158: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1158'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1158'.
[indexDocument][L3] Recorded doc length 59 for '1158'.
[indexDocument][L3] Tokens added to the inverted index for '1158'.
[indexDocument][L2] Indexing document '842'.
[indexDocument][L3] Opened file '842'.
[preprocess][L2] Starting preprocessing for '842'.
[preprocess][L3] 842: Tokenized into 73 tokens.
[preprocess][L3] 842: Removed stop words; 48 tokens remain.
[preprocess][L3] 842: Stemming complete.
[indexDocument][L3] Preprocessing complete for '842'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '842'.
[indexDocument][L3] Recorded doc length 48 for '842'.
[indexDocument][L3] Tokens added to the inverted index for '842'.
[indexDocument][L2] Indexing document '628'.
[indexDocument][L3] Opened file '628'.
[preprocess][L2] Starting preprocessing for '628'.
[preprocess][L3] 628: Tokenized into 218 tokens.
[preprocess][L3] 628: Removed stop words; 128 tokens remain.
[preprocess][L3] 628: Stemming complete.
[indexDocument][L3] Preprocessing complete for '628'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '628'.
[indexDocument][L3] Recorded doc length 128 for '628'.
[indexDocument][L3] Tokens added to the inverted index for '628'.
[indexDocument][L2] Indexing document '287'.
[indexDocument][L3] Opened file '287'.
[preprocess][L2] Starting preprocessing for '287'.
[preprocess][L3] 287: Tokenized into 146 tokens.
[preprocess][L3] 287: Removed stop words; 87 tokens remain.
[preprocess][L3] 287: Stemming complete.
[indexDocument][L3] Preprocessing complete for '287'; 87 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '287'.
[indexDocument][L3] Recorded doc length 87 for '287'.
[indexDocument][L3] Tokens added to the inverted index for '287'.
[indexDocument][L2] Indexing document '1355'.
[indexDocument][L3] Opened file '1355'.
[preprocess][L2] Starting preprocessing for '1355'.
[preprocess][L3] 1355: Tokenized into 213 tokens.
[preprocess][L3] 1355: Removed stop words; 127 tokens remain.
[preprocess][L3] 1355: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1355'; 127 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1355'.
[indexDocument][L3] Recorded doc length 127 for '1355'.
[indexDocument][L3] Tokens added to the inverted index for '1355'.
[indexDocument][L2] Indexing document '1167'.
[indexDocument][L3] Opened file '1167'.
[preprocess][L2] Starting preprocessing for '1167'.
[preprocess][L3] 1167: Tokenized into 201 tokens.
[preprocess][L3] 1167: Removed stop words; 117 tokens remain.
[preprocess][L3] 1167: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1167'; 117 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1167'.
[indexDocument][L3] Recorded doc length 117 for '1167'.
[indexDocument][L3] Tokens added to the inverted index for '1167'.
[indexDocument][L2] Indexing document '889'.
[indexDocument][L3] Opened file '889'.
[preprocess][L2] Starting preprocessing for '889'.
[preprocess][L3] 889: Tokenized into 289 tokens.
[preprocess][L3] 889: Removed stop words; 166 tokens remain.
[preprocess][L3] 889: Stemming complete.
[indexDocument][L3] Preprocessing complete for '889'; 166 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '889'.
[indexDocument][L3] Recorded doc length 166 for '889'.
[indexDocument][L3] Tokens added to the inverted index for '889'.
[indexDocument][L2] Indexing document '425'.
[indexDocument][L3] Opened file '425'.
[preprocess][L2] Starting preprocessing for '425'.
[preprocess][L3] 425: Tokenized into 116 tokens.
[preprocess][L3] 425: Removed stop words; 71 tokens remain.
[preprocess][L3] 425: Stemming complete.
[indexDocument][L3] Preprocessing complete for '425'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '425'.
[indexDocument][L3] Recorded doc length 71 for '425'.
[indexDocument][L3] Tokens added to the inverted index for '425'.
[indexDocument][L2] Indexing document '617'.
[indexDocument][L3] Opened file '617'.
[preprocess][L2] Starting preprocessing for '617'.
[preprocess][L3] 617: Tokenized into 106 tokens.
[preprocess][L3] 617: Removed stop words; 62 tokens remain.
[preprocess][L3] 617: Stemming complete.
[indexDocument][L3] Preprocessing complete for '617'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '617'.
[indexDocument][L3] Recorded doc length 62 for '617'.
[indexDocument][L3] Tokens added to the inverted index for '617'.
[indexDocument][L2] Indexing document '273'.
[indexDocument][L3] Opened file '273'.
[preprocess][L2] Starting preprocessing for '273'.
[preprocess][L3] 273: Tokenized into 118 tokens.
[preprocess][L3] 273: Removed stop words; 71 tokens remain.
[preprocess][L3] 273: Stemming complete.
[indexDocument][L3] Preprocessing complete for '273'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '273'.
[indexDocument][L3] Recorded doc length 71 for '273'.
[indexDocument][L3] Tokens added to the inverted index for '273'.
[indexDocument][L2] Indexing document '41'.
[indexDocument][L3] Opened file '41'.
[preprocess][L2] Starting preprocessing for '41'.
[preprocess][L3] 41: Tokenized into 79 tokens.
[preprocess][L3] 41: Removed stop words; 50 tokens remain.
[preprocess][L3] 41: Stemming complete.
[indexDocument][L3] Preprocessing complete for '41'; 50 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '41'.
[indexDocument][L3] Recorded doc length 50 for '41'.
[indexDocument][L3] Tokens added to the inverted index for '41'.
[indexDocument][L2] Indexing document '1193'.
[indexDocument][L3] Opened file '1193'.
[preprocess][L2] Starting preprocessing for '1193'.
[preprocess][L3] 1193: Tokenized into 148 tokens.
[preprocess][L3] 1193: Removed stop words; 91 tokens remain.
[preprocess][L3] 1193: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1193'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1193'.
[indexDocument][L3] Recorded doc length 91 for '1193'.
[indexDocument][L3] Tokens added to the inverted index for '1193'.
[indexDocument][L2] Indexing document '1133'.
[indexDocument][L3] Opened file '1133'.
[preprocess][L2] Starting preprocessing for '1133'.
[preprocess][L3] 1133: Tokenized into 84 tokens.
[preprocess][L3] 1133: Removed stop words; 59 tokens remain.
[preprocess][L3] 1133: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1133'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1133'.
[indexDocument][L3] Recorded doc length 59 for '1133'.
[indexDocument][L3] Tokens added to the inverted index for '1133'.
[indexDocument][L2] Indexing document '1301'.
[indexDocument][L3] Opened file '1301'.
[preprocess][L2] Starting preprocessing for '1301'.
[preprocess][L3] 1301: Tokenized into 186 tokens.
[preprocess][L3] 1301: Removed stop words; 99 tokens remain.
[preprocess][L3] 1301: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1301'; 99 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1301'.
[indexDocument][L3] Recorded doc length 99 for '1301'.
[indexDocument][L3] Tokens added to the inverted index for '1301'.
[indexDocument][L2] Indexing document '485'.
[indexDocument][L3] Opened file '485'.
[preprocess][L2] Starting preprocessing for '485'.
[preprocess][L3] 485: Tokenized into 50 tokens.
[preprocess][L3] 485: Removed stop words; 31 tokens remain.
[preprocess][L3] 485: Stemming complete.
[indexDocument][L3] Preprocessing complete for '485'; 31 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '485'.
[indexDocument][L3] Recorded doc length 31 for '485'.
[indexDocument][L3] Tokens added to the inverted index for '485'.
[indexDocument][L2] Indexing document '643'.
[indexDocument][L3] Opened file '643'.
[preprocess][L2] Starting preprocessing for '643'.
[preprocess][L3] 643: Tokenized into 129 tokens.
[preprocess][L3] 643: Removed stop words; 86 tokens remain.
[preprocess][L3] 643: Stemming complete.
[indexDocument][L3] Preprocessing complete for '643'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '643'.
[indexDocument][L3] Recorded doc length 86 for '643'.
[indexDocument][L3] Tokens added to the inverted index for '643'.
[indexDocument][L2] Indexing document '471'.
[indexDocument][L3] Opened file '471'.
[preprocess][L2] Starting preprocessing for '471'.
[preprocess][L3] 471: Tokenized into 2 tokens.
[preprocess][L3] 471: Removed stop words; 2 tokens remain.
[preprocess][L3] 471: Stemming complete.
[indexDocument][L3] Preprocessing complete for '471'; 2 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '471'.
[indexDocument][L3] Recorded doc length 2 for '471'.
[indexDocument][L3] Tokens added to the inverted index for '471'.
[indexDocument][L2] Indexing document '829'.
[indexDocument][L3] Opened file '829'.
[preprocess][L2] Starting preprocessing for '829'.
[preprocess][L3] 829: Tokenized into 170 tokens.
[preprocess][L3] 829: Removed stop words; 96 tokens remain.
[preprocess][L3] 829: Stemming complete.
[indexDocument][L3] Preprocessing complete for '829'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '829'.
[indexDocument][L3] Recorded doc length 96 for '829'.
[indexDocument][L3] Tokens added to the inverted index for '829'.
[indexDocument][L2] Indexing document '15'.
[indexDocument][L3] Opened file '15'.
[preprocess][L2] Starting preprocessing for '15'.
[preprocess][L3] 15: Tokenized into 146 tokens.
[preprocess][L3] 15: Removed stop words; 82 tokens remain.
[preprocess][L3] 15: Stemming complete.
[indexDocument][L3] Preprocessing complete for '15'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '15'.
[indexDocument][L3] Recorded doc length 82 for '15'.
[indexDocument][L3] Tokens added to the inverted index for '15'.
[indexDocument][L2] Indexing document '227'.
[indexDocument][L3] Opened file '227'.
[preprocess][L2] Starting preprocessing for '227'.
[preprocess][L3] 227: Tokenized into 160 tokens.
[preprocess][L3] 227: Removed stop words; 92 tokens remain.
[preprocess][L3] 227: Stemming complete.
[indexDocument][L3] Preprocessing complete for '227'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '227'.
[indexDocument][L3] Recorded doc length 92 for '227'.
[indexDocument][L3] Tokens added to the inverted index for '227'.
[indexDocument][L2] Indexing document '688'.
[indexDocument][L3] Opened file '688'.
[preprocess][L2] Starting preprocessing for '688'.
[preprocess][L3] 688: Tokenized into 255 tokens.
[preprocess][L3] 688: Removed stop words; 162 tokens remain.
[preprocess][L3] 688: Stemming complete.
[indexDocument][L3] Preprocessing complete for '688'; 162 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '688'.
[indexDocument][L3] Recorded doc length 162 for '688'.
[indexDocument][L3] Tokens added to the inverted index for '688'.
[indexDocument][L2] Indexing document '218'.
[indexDocument][L3] Opened file '218'.
[preprocess][L2] Starting preprocessing for '218'.
[preprocess][L3] 218: Tokenized into 264 tokens.
[preprocess][L3] 218: Removed stop words; 153 tokens remain.
[preprocess][L3] 218: Stemming complete.
[indexDocument][L3] Preprocessing complete for '218'; 153 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '218'.
[indexDocument][L3] Recorded doc length 153 for '218'.
[indexDocument][L3] Tokens added to the inverted index for '218'.
[indexDocument][L2] Indexing document '816'.
[indexDocument][L3] Opened file '816'.
[preprocess][L2] Starting preprocessing for '816'.
[preprocess][L3] 816: Tokenized into 166 tokens.
[preprocess][L3] 816: Removed stop words; 101 tokens remain.
[preprocess][L3] 816: Stemming complete.
[indexDocument][L3] Preprocessing complete for '816'; 101 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '816'.
[indexDocument][L3] Recorded doc length 101 for '816'.
[indexDocument][L3] Tokens added to the inverted index for '816'.
[indexDocument][L2] Indexing document '220'.
[indexDocument][L3] Opened file '220'.
[preprocess][L2] Starting preprocessing for '220'.
[preprocess][L3] 220: Tokenized into 167 tokens.
[preprocess][L3] 220: Removed stop words; 95 tokens remain.
[preprocess][L3] 220: Stemming complete.
[indexDocument][L3] Preprocessing complete for '220'; 95 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '220'.
[indexDocument][L3] Recorded doc length 95 for '220'.
[indexDocument][L3] Tokens added to the inverted index for '220'.
[indexDocument][L2] Indexing document '12'.
[indexDocument][L3] Opened file '12'.
[preprocess][L2] Starting preprocessing for '12'.
[preprocess][L3] 12: Tokenized into 131 tokens.
[preprocess][L3] 12: Removed stop words; 79 tokens remain.
[preprocess][L3] 12: Stemming complete.
[indexDocument][L3] Preprocessing complete for '12'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '12'.
[indexDocument][L3] Recorded doc length 79 for '12'.
[indexDocument][L3] Tokens added to the inverted index for '12'.
[indexDocument][L2] Indexing document '476'.
[indexDocument][L3] Opened file '476'.
[preprocess][L2] Starting preprocessing for '476'.
[preprocess][L3] 476: Tokenized into 238 tokens.
[preprocess][L3] 476: Removed stop words; 151 tokens remain.
[preprocess][L3] 476: Stemming complete.
[indexDocument][L3] Preprocessing complete for '476'; 151 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '476'.
[indexDocument][L3] Recorded doc length 151 for '476'.
[indexDocument][L3] Tokens added to the inverted index for '476'.
[indexDocument][L2] Indexing document '644'.
[indexDocument][L3] Opened file '644'.
[preprocess][L2] Starting preprocessing for '644'.
[preprocess][L3] 644: Tokenized into 188 tokens.
[preprocess][L3] 644: Removed stop words; 115 tokens remain.
[preprocess][L3] 644: Stemming complete.
[indexDocument][L3] Preprocessing complete for '644'; 115 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '644'.
[indexDocument][L3] Recorded doc length 115 for '644'.
[indexDocument][L3] Tokens added to the inverted index for '644'.
[indexDocument][L2] Indexing document '482'.
[indexDocument][L3] Opened file '482'.
[preprocess][L2] Starting preprocessing for '482'.
[preprocess][L3] 482: Tokenized into 74 tokens.
[preprocess][L3] 482: Removed stop words; 46 tokens remain.
[preprocess][L3] 482: Stemming complete.
[indexDocument][L3] Preprocessing complete for '482'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '482'.
[indexDocument][L3] Recorded doc length 46 for '482'.
[indexDocument][L3] Tokens added to the inverted index for '482'.
[indexDocument][L2] Indexing document '1306'.
[indexDocument][L3] Opened file '1306'.
[preprocess][L2] Starting preprocessing for '1306'.
[preprocess][L3] 1306: Tokenized into 59 tokens.
[preprocess][L3] 1306: Removed stop words; 41 tokens remain.
[preprocess][L3] 1306: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1306'; 41 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1306'.
[indexDocument][L3] Recorded doc length 41 for '1306'.
[indexDocument][L3] Tokens added to the inverted index for '1306'.
[indexDocument][L2] Indexing document '1134'.
[indexDocument][L3] Opened file '1134'.
[preprocess][L2] Starting preprocessing for '1134'.
[preprocess][L3] 1134: Tokenized into 289 tokens.
[preprocess][L3] 1134: Removed stop words; 157 tokens remain.
[preprocess][L3] 1134: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1134'; 157 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1134'.
[indexDocument][L3] Recorded doc length 157 for '1134'.
[indexDocument][L3] Tokens added to the inverted index for '1134'.
[indexDocument][L2] Indexing document '449'.
[indexDocument][L3] Opened file '449'.
[preprocess][L2] Starting preprocessing for '449'.
[preprocess][L3] 449: Tokenized into 102 tokens.
[preprocess][L3] 449: Removed stop words; 61 tokens remain.
[preprocess][L3] 449: Stemming complete.
[indexDocument][L3] Preprocessing complete for '449'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '449'.
[indexDocument][L3] Recorded doc length 61 for '449'.
[indexDocument][L3] Tokens added to the inverted index for '449'.
[indexDocument][L2] Indexing document '811'.
[indexDocument][L3] Opened file '811'.
[preprocess][L2] Starting preprocessing for '811'.
[preprocess][L3] 811: Tokenized into 323 tokens.
[preprocess][L3] 811: Removed stop words; 188 tokens remain.
[preprocess][L3] 811: Stemming complete.
[indexDocument][L3] Preprocessing complete for '811'; 188 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '811'.
[indexDocument][L3] Recorded doc length 188 for '811'.
[indexDocument][L3] Tokens added to the inverted index for '811'.
[indexDocument][L2] Indexing document '1339'.
[indexDocument][L3] Opened file '1339'.
[preprocess][L2] Starting preprocessing for '1339'.
[preprocess][L3] 1339: Tokenized into 212 tokens.
[preprocess][L3] 1339: Removed stop words; 134 tokens remain.
[preprocess][L3] 1339: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1339'; 134 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1339'.
[indexDocument][L3] Recorded doc length 134 for '1339'.
[indexDocument][L3] Tokens added to the inverted index for '1339'.
[indexDocument][L2] Indexing document '440'.
[indexDocument][L3] Opened file '440'.
[preprocess][L2] Starting preprocessing for '440'.
[preprocess][L3] 440: Tokenized into 106 tokens.
[preprocess][L3] 440: Removed stop words; 63 tokens remain.
[preprocess][L3] 440: Stemming complete.
[indexDocument][L3] Preprocessing complete for '440'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '440'.
[indexDocument][L3] Recorded doc length 63 for '440'.
[indexDocument][L3] Tokens added to the inverted index for '440'.
[indexDocument][L2] Indexing document '672'.
[indexDocument][L3] Opened file '672'.
[preprocess][L2] Starting preprocessing for '672'.
[preprocess][L3] 672: Tokenized into 84 tokens.
[preprocess][L3] 672: Removed stop words; 61 tokens remain.
[preprocess][L3] 672: Stemming complete.
[indexDocument][L3] Preprocessing complete for '672'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '672'.
[indexDocument][L3] Recorded doc length 61 for '672'.
[indexDocument][L3] Tokens added to the inverted index for '672'.
[indexDocument][L2] Indexing document '216'.
[indexDocument][L3] Opened file '216'.
[preprocess][L2] Starting preprocessing for '216'.
[preprocess][L3] 216: Tokenized into 273 tokens.
[preprocess][L3] 216: Removed stop words; 154 tokens remain.
[preprocess][L3] 216: Stemming complete.
[indexDocument][L3] Preprocessing complete for '216'; 154 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '216'.
[indexDocument][L3] Recorded doc length 154 for '216'.
[indexDocument][L3] Tokens added to the inverted index for '216'.
[indexDocument][L2] Indexing document '818'.
[indexDocument][L3] Opened file '818'.
[preprocess][L2] Starting preprocessing for '818'.
[preprocess][L3] 818: Tokenized into 130 tokens.
[preprocess][L3] 818: Removed stop words; 73 tokens remain.
[preprocess][L3] 818: Stemming complete.
[indexDocument][L3] Preprocessing complete for '818'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '818'.
[indexDocument][L3] Recorded doc length 73 for '818'.
[indexDocument][L3] Tokens added to the inverted index for '818'.
[indexDocument][L2] Indexing document '24'.
[indexDocument][L3] Opened file '24'.
[preprocess][L2] Starting preprocessing for '24'.
[preprocess][L3] 24: Tokenized into 274 tokens.
[preprocess][L3] 24: Removed stop words; 159 tokens remain.
[preprocess][L3] 24: Stemming complete.
[indexDocument][L3] Preprocessing complete for '24'; 159 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '24'.
[indexDocument][L3] Recorded doc length 159 for '24'.
[indexDocument][L3] Tokens added to the inverted index for '24'.
[indexDocument][L2] Indexing document '1330'.
[indexDocument][L3] Opened file '1330'.
[preprocess][L2] Starting preprocessing for '1330'.
[preprocess][L3] 1330: Tokenized into 221 tokens.
[preprocess][L3] 1330: Removed stop words; 138 tokens remain.
[preprocess][L3] 1330: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1330'; 138 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1330'.
[indexDocument][L3] Recorded doc length 138 for '1330'.
[indexDocument][L3] Tokens added to the inverted index for '1330'.
[indexDocument][L2] Indexing document '1102'.
[indexDocument][L3] Opened file '1102'.
[preprocess][L2] Starting preprocessing for '1102'.
[preprocess][L3] 1102: Tokenized into 58 tokens.
[preprocess][L3] 1102: Removed stop words; 40 tokens remain.
[preprocess][L3] 1102: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1102'; 40 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1102'.
[indexDocument][L3] Recorded doc length 40 for '1102'.
[indexDocument][L3] Tokens added to the inverted index for '1102'.
[indexDocument][L2] Indexing document '686'.
[indexDocument][L3] Opened file '686'.
[preprocess][L2] Starting preprocessing for '686'.
[preprocess][L3] 686: Tokenized into 223 tokens.
[preprocess][L3] 686: Removed stop words; 140 tokens remain.
[preprocess][L3] 686: Stemming complete.
[indexDocument][L3] Preprocessing complete for '686'; 140 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '686'.
[indexDocument][L3] Recorded doc length 140 for '686'.
[indexDocument][L3] Tokens added to the inverted index for '686'.
[indexDocument][L2] Indexing document '827'.
[indexDocument][L3] Opened file '827'.
[preprocess][L2] Starting preprocessing for '827'.
[preprocess][L3] 827: Tokenized into 350 tokens.
[preprocess][L3] 827: Removed stop words; 191 tokens remain.
[preprocess][L3] 827: Stemming complete.
[indexDocument][L3] Preprocessing complete for '827'; 191 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '827'.
[indexDocument][L3] Recorded doc length 191 for '827'.
[indexDocument][L3] Tokens added to the inverted index for '827'.
[indexDocument][L2] Indexing document '229'.
[indexDocument][L3] Opened file '229'.
[preprocess][L2] Starting preprocessing for '229'.
[preprocess][L3] 229: Tokenized into 230 tokens.
[preprocess][L3] 229: Removed stop words; 133 tokens remain.
[preprocess][L3] 229: Stemming complete.
[indexDocument][L3] Preprocessing complete for '229'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '229'.
[indexDocument][L3] Recorded doc length 133 for '229'.
[indexDocument][L3] Tokens added to the inverted index for '229'.
[indexDocument][L2] Indexing document '681'.
[indexDocument][L3] Opened file '681'.
[preprocess][L2] Starting preprocessing for '681'.
[preprocess][L3] 681: Tokenized into 131 tokens.
[preprocess][L3] 681: Removed stop words; 75 tokens remain.
[preprocess][L3] 681: Stemming complete.
[indexDocument][L3] Preprocessing complete for '681'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '681'.
[indexDocument][L3] Recorded doc length 75 for '681'.
[indexDocument][L3] Tokens added to the inverted index for '681'.
[indexDocument][L2] Indexing document '1105'.
[indexDocument][L3] Opened file '1105'.
[preprocess][L2] Starting preprocessing for '1105'.
[preprocess][L3] 1105: Tokenized into 117 tokens.
[preprocess][L3] 1105: Removed stop words; 72 tokens remain.
[preprocess][L3] 1105: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1105'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1105'.
[indexDocument][L3] Recorded doc length 72 for '1105'.
[indexDocument][L3] Tokens added to the inverted index for '1105'.
[indexDocument][L2] Indexing document '1337'.
[indexDocument][L3] Opened file '1337'.
[preprocess][L2] Starting preprocessing for '1337'.
[preprocess][L3] 1337: Tokenized into 169 tokens.
[preprocess][L3] 1337: Removed stop words; 97 tokens remain.
[preprocess][L3] 1337: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1337'; 97 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1337'.
[indexDocument][L3] Recorded doc length 97 for '1337'.
[indexDocument][L3] Tokens added to the inverted index for '1337'.
[indexDocument][L2] Indexing document '23'.
[indexDocument][L3] Opened file '23'.
[preprocess][L2] Starting preprocessing for '23'.
[preprocess][L3] 23: Tokenized into 149 tokens.
[preprocess][L3] 23: Removed stop words; 83 tokens remain.
[preprocess][L3] 23: Stemming complete.
[indexDocument][L3] Preprocessing complete for '23'; 83 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '23'.
[indexDocument][L3] Recorded doc length 83 for '23'.
[indexDocument][L3] Tokens added to the inverted index for '23'.
[indexDocument][L2] Indexing document '211'.
[indexDocument][L3] Opened file '211'.
[preprocess][L2] Starting preprocessing for '211'.
[preprocess][L3] 211: Tokenized into 213 tokens.
[preprocess][L3] 211: Removed stop words; 121 tokens remain.
[preprocess][L3] 211: Stemming complete.
[indexDocument][L3] Preprocessing complete for '211'; 121 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '211'.
[indexDocument][L3] Recorded doc length 121 for '211'.
[indexDocument][L3] Tokens added to the inverted index for '211'.
[indexDocument][L2] Indexing document '675'.
[indexDocument][L3] Opened file '675'.
[preprocess][L2] Starting preprocessing for '675'.
[preprocess][L3] 675: Tokenized into 274 tokens.
[preprocess][L3] 675: Removed stop words; 174 tokens remain.
[preprocess][L3] 675: Stemming complete.
[indexDocument][L3] Preprocessing complete for '675'; 174 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '675'.
[indexDocument][L3] Recorded doc length 174 for '675'.
[indexDocument][L3] Tokens added to the inverted index for '675'.
[indexDocument][L2] Indexing document '447'.
[indexDocument][L3] Opened file '447'.
[preprocess][L2] Starting preprocessing for '447'.
[preprocess][L3] 447: Tokenized into 131 tokens.
[preprocess][L3] 447: Removed stop words; 72 tokens remain.
[preprocess][L3] 447: Stemming complete.
[indexDocument][L3] Preprocessing complete for '447'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '447'.
[indexDocument][L3] Recorded doc length 72 for '447'.
[indexDocument][L3] Tokens added to the inverted index for '447'.
[indexDocument][L2] Indexing document '1308'.
[indexDocument][L3] Opened file '1308'.
[preprocess][L2] Starting preprocessing for '1308'.
[preprocess][L3] 1308: Tokenized into 105 tokens.
[preprocess][L3] 1308: Removed stop words; 57 tokens remain.
[preprocess][L3] 1308: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1308'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1308'.
[indexDocument][L3] Recorded doc length 57 for '1308'.
[indexDocument][L3] Tokens added to the inverted index for '1308'.
[indexDocument][L2] Indexing document '478'.
[indexDocument][L3] Opened file '478'.
[preprocess][L2] Starting preprocessing for '478'.
[preprocess][L3] 478: Tokenized into 99 tokens.
[preprocess][L3] 478: Removed stop words; 72 tokens remain.
[preprocess][L3] 478: Stemming complete.
[indexDocument][L3] Preprocessing complete for '478'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '478'.
[indexDocument][L3] Recorded doc length 72 for '478'.
[indexDocument][L3] Tokens added to the inverted index for '478'.
[indexDocument][L2] Indexing document '820'.
[indexDocument][L3] Opened file '820'.
[preprocess][L2] Starting preprocessing for '820'.
[preprocess][L3] 820: Tokenized into 262 tokens.
[preprocess][L3] 820: Removed stop words; 149 tokens remain.
[preprocess][L3] 820: Stemming complete.
[indexDocument][L3] Preprocessing complete for '820'; 149 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '820'.
[indexDocument][L3] Recorded doc length 149 for '820'.
[indexDocument][L3] Tokens added to the inverted index for '820'.
[indexDocument][L2] Indexing document '1118'.
[indexDocument][L3] Opened file '1118'.
[preprocess][L2] Starting preprocessing for '1118'.
[preprocess][L3] 1118: Tokenized into 90 tokens.
[preprocess][L3] 1118: Removed stop words; 61 tokens remain.
[preprocess][L3] 1118: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1118'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1118'.
[indexDocument][L3] Recorded doc length 61 for '1118'.
[indexDocument][L3] Tokens added to the inverted index for '1118'.
[indexDocument][L2] Indexing document '802'.
[indexDocument][L3] Opened file '802'.
[preprocess][L2] Starting preprocessing for '802'.
[preprocess][L3] 802: Tokenized into 140 tokens.
[preprocess][L3] 802: Removed stop words; 73 tokens remain.
[preprocess][L3] 802: Stemming complete.
[indexDocument][L3] Preprocessing complete for '802'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '802'.
[indexDocument][L3] Recorded doc length 73 for '802'.
[indexDocument][L3] Tokens added to the inverted index for '802'.
[indexDocument][L2] Indexing document '668'.
[indexDocument][L3] Opened file '668'.
[preprocess][L2] Starting preprocessing for '668'.
[preprocess][L3] 668: Tokenized into 119 tokens.
[preprocess][L3] 668: Removed stop words; 76 tokens remain.
[preprocess][L3] 668: Stemming complete.
[indexDocument][L3] Preprocessing complete for '668'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '668'.
[indexDocument][L3] Recorded doc length 76 for '668'.
[indexDocument][L3] Tokens added to the inverted index for '668'.
[indexDocument][L2] Indexing document '1315'.
[indexDocument][L3] Opened file '1315'.
[preprocess][L2] Starting preprocessing for '1315'.
[preprocess][L3] 1315: Tokenized into 148 tokens.
[preprocess][L3] 1315: Removed stop words; 94 tokens remain.
[preprocess][L3] 1315: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1315'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1315'.
[indexDocument][L3] Recorded doc length 94 for '1315'.
[indexDocument][L3] Tokens added to the inverted index for '1315'.
[indexDocument][L2] Indexing document '1127'.
[indexDocument][L3] Opened file '1127'.
[preprocess][L2] Starting preprocessing for '1127'.
[preprocess][L3] 1127: Tokenized into 218 tokens.
[preprocess][L3] 1127: Removed stop words; 126 tokens remain.
[preprocess][L3] 1127: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1127'; 126 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1127'.
[indexDocument][L3] Recorded doc length 126 for '1127'.
[indexDocument][L3] Tokens added to the inverted index for '1127'.
[indexDocument][L2] Indexing document '491'.
[indexDocument][L3] Opened file '491'.
[preprocess][L2] Starting preprocessing for '491'.
[preprocess][L3] 491: Tokenized into 128 tokens.
[preprocess][L3] 491: Removed stop words; 71 tokens remain.
[preprocess][L3] 491: Stemming complete.
[indexDocument][L3] Preprocessing complete for '491'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '491'.
[indexDocument][L3] Recorded doc length 71 for '491'.
[indexDocument][L3] Tokens added to the inverted index for '491'.
[indexDocument][L2] Indexing document '465'.
[indexDocument][L3] Opened file '465'.
[preprocess][L2] Starting preprocessing for '465'.
[preprocess][L3] 465: Tokenized into 110 tokens.
[preprocess][L3] 465: Removed stop words; 67 tokens remain.
[preprocess][L3] 465: Stemming complete.
[indexDocument][L3] Preprocessing complete for '465'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '465'.
[indexDocument][L3] Recorded doc length 67 for '465'.
[indexDocument][L3] Tokens added to the inverted index for '465'.
[indexDocument][L2] Indexing document '657'.
[indexDocument][L3] Opened file '657'.
[preprocess][L2] Starting preprocessing for '657'.
[preprocess][L3] 657: Tokenized into 115 tokens.
[preprocess][L3] 657: Removed stop words; 71 tokens remain.
[preprocess][L3] 657: Stemming complete.
[indexDocument][L3] Preprocessing complete for '657'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '657'.
[indexDocument][L3] Recorded doc length 71 for '657'.
[indexDocument][L3] Tokens added to the inverted index for '657'.
[indexDocument][L2] Indexing document '233'.
[indexDocument][L3] Opened file '233'.
[preprocess][L2] Starting preprocessing for '233'.
[preprocess][L3] 233: Tokenized into 115 tokens.
[preprocess][L3] 233: Removed stop words; 64 tokens remain.
[preprocess][L3] 233: Stemming complete.
[indexDocument][L3] Preprocessing complete for '233'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '233'.
[indexDocument][L3] Recorded doc length 64 for '233'.
[indexDocument][L3] Tokens added to the inverted index for '233'.
[indexDocument][L2] Indexing document '1'.
[indexDocument][L3] Opened file '1'.
[preprocess][L2] Starting preprocessing for '1'.
[preprocess][L3] 1: Tokenized into 146 tokens.
[preprocess][L3] 1: Removed stop words; 86 tokens remain.
[preprocess][L3] 1: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1'.
[indexDocument][L3] Recorded doc length 86 for '1'.
[indexDocument][L3] Tokens added to the inverted index for '1'.
[indexDocument][L2] Indexing document '805'.
[indexDocument][L3] Opened file '805'.
[preprocess][L2] Starting preprocessing for '805'.
[preprocess][L3] 805: Tokenized into 233 tokens.
[preprocess][L3] 805: Removed stop words; 142 tokens remain.
[preprocess][L3] 805: Stemming complete.
[indexDocument][L3] Preprocessing complete for '805'; 142 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '805'.
[indexDocument][L3] Recorded doc length 142 for '805'.
[indexDocument][L3] Tokens added to the inverted index for '805'.
[indexDocument][L2] Indexing document '39'.
[indexDocument][L3] Opened file '39'.
[preprocess][L2] Starting preprocessing for '39'.
[preprocess][L3] 39: Tokenized into 163 tokens.
[preprocess][L3] 39: Removed stop words; 91 tokens remain.
[preprocess][L3] 39: Stemming complete.
[indexDocument][L3] Preprocessing complete for '39'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '39'.
[indexDocument][L3] Recorded doc length 91 for '39'.
[indexDocument][L3] Tokens added to the inverted index for '39'.
[indexDocument][L2] Indexing document '6'.
[indexDocument][L3] Opened file '6'.
[preprocess][L2] Starting preprocessing for '6'.
[preprocess][L3] 6: Tokenized into 112 tokens.
[preprocess][L3] 6: Removed stop words; 63 tokens remain.
[preprocess][L3] 6: Stemming complete.
[indexDocument][L3] Preprocessing complete for '6'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '6'.
[indexDocument][L3] Recorded doc length 63 for '6'.
[indexDocument][L3] Tokens added to the inverted index for '6'.
[indexDocument][L2] Indexing document '234'.
[indexDocument][L3] Opened file '234'.
[preprocess][L2] Starting preprocessing for '234'.
[preprocess][L3] 234: Tokenized into 319 tokens.
[preprocess][L3] 234: Removed stop words; 195 tokens remain.
[preprocess][L3] 234: Stemming complete.
[indexDocument][L3] Preprocessing complete for '234'; 195 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '234'.
[indexDocument][L3] Recorded doc length 195 for '234'.
[indexDocument][L3] Tokens added to the inverted index for '234'.
[indexDocument][L2] Indexing document '650'.
[indexDocument][L3] Opened file '650'.
[preprocess][L2] Starting preprocessing for '650'.
[preprocess][L3] 650: Tokenized into 69 tokens.
[preprocess][L3] 650: Removed stop words; 47 tokens remain.
[preprocess][L3] 650: Stemming complete.
[indexDocument][L3] Preprocessing complete for '650'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '650'.
[indexDocument][L3] Recorded doc length 47 for '650'.
[indexDocument][L3] Tokens added to the inverted index for '650'.
[indexDocument][L2] Indexing document '462'.
[indexDocument][L3] Opened file '462'.
[preprocess][L2] Starting preprocessing for '462'.
[preprocess][L3] 462: Tokenized into 142 tokens.
[preprocess][L3] 462: Removed stop words; 90 tokens remain.
[preprocess][L3] 462: Stemming complete.
[indexDocument][L3] Preprocessing complete for '462'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '462'.
[indexDocument][L3] Recorded doc length 90 for '462'.
[indexDocument][L3] Tokens added to the inverted index for '462'.
[indexDocument][L2] Indexing document '496'.
[indexDocument][L3] Opened file '496'.
[preprocess][L2] Starting preprocessing for '496'.
[preprocess][L3] 496: Tokenized into 119 tokens.
[preprocess][L3] 496: Removed stop words; 76 tokens remain.
[preprocess][L3] 496: Stemming complete.
[indexDocument][L3] Preprocessing complete for '496'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '496'.
[indexDocument][L3] Recorded doc length 76 for '496'.
[indexDocument][L3] Tokens added to the inverted index for '496'.
[indexDocument][L2] Indexing document '1120'.
[indexDocument][L3] Opened file '1120'.
[preprocess][L2] Starting preprocessing for '1120'.
[preprocess][L3] 1120: Tokenized into 94 tokens.
[preprocess][L3] 1120: Removed stop words; 58 tokens remain.
[preprocess][L3] 1120: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1120'; 58 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1120'.
[indexDocument][L3] Recorded doc length 58 for '1120'.
[indexDocument][L3] Tokens added to the inverted index for '1120'.
[indexDocument][L2] Indexing document '1312'.
[indexDocument][L3] Opened file '1312'.
[preprocess][L2] Starting preprocessing for '1312'.
[preprocess][L3] 1312: Tokenized into 118 tokens.
[preprocess][L3] 1312: Removed stop words; 68 tokens remain.
[preprocess][L3] 1312: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1312'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1312'.
[indexDocument][L3] Recorded doc length 68 for '1312'.
[indexDocument][L3] Tokens added to the inverted index for '1312'.
[indexDocument][L2] Indexing document '833'.
[indexDocument][L3] Opened file '833'.
[preprocess][L2] Starting preprocessing for '833'.
[preprocess][L3] 833: Tokenized into 84 tokens.
[preprocess][L3] 833: Removed stop words; 59 tokens remain.
[preprocess][L3] 833: Stemming complete.
[indexDocument][L3] Preprocessing complete for '833'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '833'.
[indexDocument][L3] Recorded doc length 59 for '833'.
[indexDocument][L3] Tokens added to the inverted index for '833'.
[indexDocument][L2] Indexing document '659'.
[indexDocument][L3] Opened file '659'.
[preprocess][L2] Starting preprocessing for '659'.
[preprocess][L3] 659: Tokenized into 149 tokens.
[preprocess][L3] 659: Removed stop words; 89 tokens remain.
[preprocess][L3] 659: Stemming complete.
[indexDocument][L3] Preprocessing complete for '659'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '659'.
[indexDocument][L3] Recorded doc length 89 for '659'.
[indexDocument][L3] Tokens added to the inverted index for '659'.
[indexDocument][L2] Indexing document '1129'.
[indexDocument][L3] Opened file '1129'.
[preprocess][L2] Starting preprocessing for '1129'.
[preprocess][L3] 1129: Tokenized into 136 tokens.
[preprocess][L3] 1129: Removed stop words; 77 tokens remain.
[preprocess][L3] 1129: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1129'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1129'.
[indexDocument][L3] Recorded doc length 77 for '1129'.
[indexDocument][L3] Tokens added to the inverted index for '1129'.
[indexDocument][L2] Indexing document '666'.
[indexDocument][L3] Opened file '666'.
[preprocess][L2] Starting preprocessing for '666'.
[preprocess][L3] 666: Tokenized into 161 tokens.
[preprocess][L3] 666: Removed stop words; 93 tokens remain.
[preprocess][L3] 666: Stemming complete.
[indexDocument][L3] Preprocessing complete for '666'; 93 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '666'.
[indexDocument][L3] Recorded doc length 93 for '666'.
[indexDocument][L3] Tokens added to the inverted index for '666'.
[indexDocument][L2] Indexing document '454'.
[indexDocument][L3] Opened file '454'.
[preprocess][L2] Starting preprocessing for '454'.
[preprocess][L3] 454: Tokenized into 197 tokens.
[preprocess][L3] 454: Removed stop words; 119 tokens remain.
[preprocess][L3] 454: Stemming complete.
[indexDocument][L3] Preprocessing complete for '454'; 119 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '454'.
[indexDocument][L3] Recorded doc length 119 for '454'.
[indexDocument][L3] Tokens added to the inverted index for '454'.
[indexDocument][L2] Indexing document '30'.
[indexDocument][L3] Opened file '30'.
[preprocess][L2] Starting preprocessing for '30'.
[preprocess][L3] 30: Tokenized into 123 tokens.
[preprocess][L3] 30: Removed stop words; 78 tokens remain.
[preprocess][L3] 30: Stemming complete.
[indexDocument][L3] Preprocessing complete for '30'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '30'.
[indexDocument][L3] Recorded doc length 78 for '30'.
[indexDocument][L3] Tokens added to the inverted index for '30'.
[indexDocument][L2] Indexing document '202'.
[indexDocument][L3] Opened file '202'.
[preprocess][L2] Starting preprocessing for '202'.
[preprocess][L3] 202: Tokenized into 311 tokens.
[preprocess][L3] 202: Removed stop words; 175 tokens remain.
[preprocess][L3] 202: Stemming complete.
[indexDocument][L3] Preprocessing complete for '202'; 175 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '202'.
[indexDocument][L3] Recorded doc length 175 for '202'.
[indexDocument][L3] Tokens added to the inverted index for '202'.
[indexDocument][L2] Indexing document '1116'.
[indexDocument][L3] Opened file '1116'.
[preprocess][L2] Starting preprocessing for '1116'.
[preprocess][L3] 1116: Tokenized into 80 tokens.
[preprocess][L3] 1116: Removed stop words; 52 tokens remain.
[preprocess][L3] 1116: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1116'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1116'.
[indexDocument][L3] Recorded doc length 52 for '1116'.
[indexDocument][L3] Tokens added to the inverted index for '1116'.
[indexDocument][L2] Indexing document '1324'.
[indexDocument][L3] Opened file '1324'.
[preprocess][L2] Starting preprocessing for '1324'.
[preprocess][L3] 1324: Tokenized into 171 tokens.
[preprocess][L3] 1324: Removed stop words; 103 tokens remain.
[preprocess][L3] 1324: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1324'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1324'.
[indexDocument][L3] Recorded doc length 103 for '1324'.
[indexDocument][L3] Tokens added to the inverted index for '1324'.
[indexDocument][L2] Indexing document '692'.
[indexDocument][L3] Opened file '692'.
[preprocess][L2] Starting preprocessing for '692'.
[preprocess][L3] 692: Tokenized into 222 tokens.
[preprocess][L3] 692: Removed stop words; 128 tokens remain.
[preprocess][L3] 692: Stemming complete.
[indexDocument][L3] Preprocessing complete for '692'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '692'.
[indexDocument][L3] Recorded doc length 128 for '692'.
[indexDocument][L3] Tokens added to the inverted index for '692'.
[indexDocument][L2] Indexing document '498'.
[indexDocument][L3] Opened file '498'.
[preprocess][L2] Starting preprocessing for '498'.
[preprocess][L3] 498: Tokenized into 167 tokens.
[preprocess][L3] 498: Removed stop words; 92 tokens remain.
[preprocess][L3] 498: Stemming complete.
[indexDocument][L3] Preprocessing complete for '498'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '498'.
[indexDocument][L3] Recorded doc length 92 for '498'.
[indexDocument][L3] Tokens added to the inverted index for '498'.
[indexDocument][L2] Indexing document '834'.
[indexDocument][L3] Opened file '834'.
[preprocess][L2] Starting preprocessing for '834'.
[preprocess][L3] 834: Tokenized into 62 tokens.
[preprocess][L3] 834: Removed stop words; 43 tokens remain.
[preprocess][L3] 834: Stemming complete.
[indexDocument][L3] Preprocessing complete for '834'; 43 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '834'.
[indexDocument][L3] Recorded doc length 43 for '834'.
[indexDocument][L3] Tokens added to the inverted index for '834'.
[indexDocument][L2] Indexing document '8'.
[indexDocument][L3] Opened file '8'.
[preprocess][L2] Starting preprocessing for '8'.
[preprocess][L3] 8: Tokenized into 168 tokens.
[preprocess][L3] 8: Removed stop words; 89 tokens remain.
[preprocess][L3] 8: Stemming complete.
[indexDocument][L3] Preprocessing complete for '8'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '8'.
[indexDocument][L3] Recorded doc length 89 for '8'.
[indexDocument][L3] Tokens added to the inverted index for '8'.
[indexDocument][L2] Indexing document '695'.
[indexDocument][L3] Opened file '695'.
[preprocess][L2] Starting preprocessing for '695'.
[preprocess][L3] 695: Tokenized into 366 tokens.
[preprocess][L3] 695: Removed stop words; 214 tokens remain.
[preprocess][L3] 695: Stemming complete.
[indexDocument][L3] Preprocessing complete for '695'; 214 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '695'.
[indexDocument][L3] Recorded doc length 214 for '695'.
[indexDocument][L3] Tokens added to the inverted index for '695'.
[indexDocument][L2] Indexing document '1323'.
[indexDocument][L3] Opened file '1323'.
[preprocess][L2] Starting preprocessing for '1323'.
[preprocess][L3] 1323: Tokenized into 120 tokens.
[preprocess][L3] 1323: Removed stop words; 71 tokens remain.
[preprocess][L3] 1323: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1323'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1323'.
[indexDocument][L3] Recorded doc length 71 for '1323'.
[indexDocument][L3] Tokens added to the inverted index for '1323'.
[indexDocument][L2] Indexing document '1111'.
[indexDocument][L3] Opened file '1111'.
[preprocess][L2] Starting preprocessing for '1111'.
[preprocess][L3] 1111: Tokenized into 61 tokens.
[preprocess][L3] 1111: Removed stop words; 48 tokens remain.
[preprocess][L3] 1111: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1111'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1111'.
[indexDocument][L3] Recorded doc length 48 for '1111'.
[indexDocument][L3] Tokens added to the inverted index for '1111'.
[indexDocument][L2] Indexing document '205'.
[indexDocument][L3] Opened file '205'.
[preprocess][L2] Starting preprocessing for '205'.
[preprocess][L3] 205: Tokenized into 267 tokens.
[preprocess][L3] 205: Removed stop words; 165 tokens remain.
[preprocess][L3] 205: Stemming complete.
[indexDocument][L3] Preprocessing complete for '205'; 165 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '205'.
[indexDocument][L3] Recorded doc length 165 for '205'.
[indexDocument][L3] Tokens added to the inverted index for '205'.
[indexDocument][L2] Indexing document '37'.
[indexDocument][L3] Opened file '37'.
[preprocess][L2] Starting preprocessing for '37'.
[preprocess][L3] 37: Tokenized into 170 tokens.
[preprocess][L3] 37: Removed stop words; 108 tokens remain.
[preprocess][L3] 37: Stemming complete.
[indexDocument][L3] Preprocessing complete for '37'; 108 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '37'.
[indexDocument][L3] Recorded doc length 108 for '37'.
[indexDocument][L3] Tokens added to the inverted index for '37'.
[indexDocument][L2] Indexing document '453'.
[indexDocument][L3] Opened file '453'.
[preprocess][L2] Starting preprocessing for '453'.
[preprocess][L3] 453: Tokenized into 206 tokens.
[preprocess][L3] 453: Removed stop words; 122 tokens remain.
[preprocess][L3] 453: Stemming complete.
[indexDocument][L3] Preprocessing complete for '453'; 122 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '453'.
[indexDocument][L3] Recorded doc length 122 for '453'.
[indexDocument][L3] Tokens added to the inverted index for '453'.
[indexDocument][L2] Indexing document '661'.
[indexDocument][L3] Opened file '661'.
[preprocess][L2] Starting preprocessing for '661'.
[preprocess][L3] 661: Tokenized into 207 tokens.
[preprocess][L3] 661: Removed stop words; 134 tokens remain.
[preprocess][L3] 661: Stemming complete.
[indexDocument][L3] Preprocessing complete for '661'; 134 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '661'.
[indexDocument][L3] Recorded doc length 134 for '661'.
[indexDocument][L3] Tokens added to the inverted index for '661'.
[indexDocument][L2] Indexing document '1145'.
[indexDocument][L3] Opened file '1145'.
[preprocess][L2] Starting preprocessing for '1145'.
[preprocess][L3] 1145: Tokenized into 108 tokens.
[preprocess][L3] 1145: Removed stop words; 71 tokens remain.
[preprocess][L3] 1145: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1145'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1145'.
[indexDocument][L3] Recorded doc length 71 for '1145'.
[indexDocument][L3] Tokens added to the inverted index for '1145'.
[indexDocument][L2] Indexing document '97'.
[indexDocument][L3] Opened file '97'.
[preprocess][L2] Starting preprocessing for '97'.
[preprocess][L3] 97: Tokenized into 239 tokens.
[preprocess][L3] 97: Removed stop words; 145 tokens remain.
[preprocess][L3] 97: Stemming complete.
[indexDocument][L3] Preprocessing complete for '97'; 145 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '97'.
[indexDocument][L3] Recorded doc length 145 for '97'.
[indexDocument][L3] Tokens added to the inverted index for '97'.
[indexDocument][L2] Indexing document '1377'.
[indexDocument][L3] Opened file '1377'.
[preprocess][L2] Starting preprocessing for '1377'.
[preprocess][L3] 1377: Tokenized into 112 tokens.
[preprocess][L3] 1377: Removed stop words; 70 tokens remain.
[preprocess][L3] 1377: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1377'; 70 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1377'.
[indexDocument][L3] Recorded doc length 70 for '1377'.
[indexDocument][L3] Tokens added to the inverted index for '1377'.
[indexDocument][L2] Indexing document '63'.
[indexDocument][L3] Opened file '63'.
[preprocess][L2] Starting preprocessing for '63'.
[preprocess][L3] 63: Tokenized into 144 tokens.
[preprocess][L3] 63: Removed stop words; 91 tokens remain.
[preprocess][L3] 63: Stemming complete.
[indexDocument][L3] Preprocessing complete for '63'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '63'.
[indexDocument][L3] Recorded doc length 91 for '63'.
[indexDocument][L3] Tokens added to the inverted index for '63'.
[indexDocument][L2] Indexing document '1383'.
[indexDocument][L3] Opened file '1383'.
[preprocess][L2] Starting preprocessing for '1383'.
[preprocess][L3] 1383: Tokenized into 236 tokens.
[preprocess][L3] 1383: Removed stop words; 133 tokens remain.
[preprocess][L3] 1383: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1383'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1383'.
[indexDocument][L3] Recorded doc length 133 for '1383'.
[indexDocument][L3] Tokens added to the inverted index for '1383'.
[indexDocument][L2] Indexing document '251'.
[indexDocument][L3] Opened file '251'.
[preprocess][L2] Starting preprocessing for '251'.
[preprocess][L3] 251: Tokenized into 100 tokens.
[preprocess][L3] 251: Removed stop words; 52 tokens remain.
[preprocess][L3] 251: Stemming complete.
[indexDocument][L3] Preprocessing complete for '251'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '251'.
[indexDocument][L3] Recorded doc length 52 for '251'.
[indexDocument][L3] Tokens added to the inverted index for '251'.
[indexDocument][L2] Indexing document '635'.
[indexDocument][L3] Opened file '635'.
[preprocess][L2] Starting preprocessing for '635'.
[preprocess][L3] 635: Tokenized into 192 tokens.
[preprocess][L3] 635: Removed stop words; 112 tokens remain.
[preprocess][L3] 635: Stemming complete.
[indexDocument][L3] Preprocessing complete for '635'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '635'.
[indexDocument][L3] Recorded doc length 112 for '635'.
[indexDocument][L3] Tokens added to the inverted index for '635'.
[indexDocument][L2] Indexing document '407'.
[indexDocument][L3] Opened file '407'.
[preprocess][L2] Starting preprocessing for '407'.
[preprocess][L3] 407: Tokenized into 73 tokens.
[preprocess][L3] 407: Removed stop words; 46 tokens remain.
[preprocess][L3] 407: Stemming complete.
[indexDocument][L3] Preprocessing complete for '407'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '407'.
[indexDocument][L3] Recorded doc length 46 for '407'.
[indexDocument][L3] Tokens added to the inverted index for '407'.
[indexDocument][L2] Indexing document '1348'.
[indexDocument][L3] Opened file '1348'.
[preprocess][L2] Starting preprocessing for '1348'.
[preprocess][L3] 1348: Tokenized into 84 tokens.
[preprocess][L3] 1348: Removed stop words; 59 tokens remain.
[preprocess][L3] 1348: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1348'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1348'.
[indexDocument][L3] Recorded doc length 59 for '1348'.
[indexDocument][L3] Tokens added to the inverted index for '1348'.
[indexDocument][L2] Indexing document '894'.
[indexDocument][L3] Opened file '894'.
[preprocess][L2] Starting preprocessing for '894'.
[preprocess][L3] 894: Tokenized into 228 tokens.
[preprocess][L3] 894: Removed stop words; 140 tokens remain.
[preprocess][L3] 894: Stemming complete.
[indexDocument][L3] Preprocessing complete for '894'; 140 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '894'.
[indexDocument][L3] Recorded doc length 140 for '894'.
[indexDocument][L3] Tokens added to the inverted index for '894'.
[indexDocument][L2] Indexing document '438'.
[indexDocument][L3] Opened file '438'.
[preprocess][L2] Starting preprocessing for '438'.
[preprocess][L3] 438: Tokenized into 78 tokens.
[preprocess][L3] 438: Removed stop words; 59 tokens remain.
[preprocess][L3] 438: Stemming complete.
[indexDocument][L3] Preprocessing complete for '438'; 59 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '438'.
[indexDocument][L3] Recorded doc length 59 for '438'.
[indexDocument][L3] Tokens added to the inverted index for '438'.
[indexDocument][L2] Indexing document '860'.
[indexDocument][L3] Opened file '860'.
[preprocess][L2] Starting preprocessing for '860'.
[preprocess][L3] 860: Tokenized into 118 tokens.
[preprocess][L3] 860: Removed stop words; 75 tokens remain.
[preprocess][L3] 860: Stemming complete.
[indexDocument][L3] Preprocessing complete for '860'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '860'.
[indexDocument][L3] Recorded doc length 75 for '860'.
[indexDocument][L3] Tokens added to the inverted index for '860'.
[indexDocument][L2] Indexing document '400'.
[indexDocument][L3] Opened file '400'.
[preprocess][L2] Starting preprocessing for '400'.
[preprocess][L3] 400: Tokenized into 72 tokens.
[preprocess][L3] 400: Removed stop words; 49 tokens remain.
[preprocess][L3] 400: Stemming complete.
[indexDocument][L3] Preprocessing complete for '400'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '400'.
[indexDocument][L3] Recorded doc length 49 for '400'.
[indexDocument][L3] Tokens added to the inverted index for '400'.
[indexDocument][L2] Indexing document '632'.
[indexDocument][L3] Opened file '632'.
[preprocess][L2] Starting preprocessing for '632'.
[preprocess][L3] 632: Tokenized into 87 tokens.
[preprocess][L3] 632: Removed stop words; 54 tokens remain.
[preprocess][L3] 632: Stemming complete.
[indexDocument][L3] Preprocessing complete for '632'; 54 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '632'.
[indexDocument][L3] Recorded doc length 54 for '632'.
[indexDocument][L3] Tokens added to the inverted index for '632'.
[indexDocument][L2] Indexing document '1384'.
[indexDocument][L3] Opened file '1384'.
[preprocess][L2] Starting preprocessing for '1384'.
[preprocess][L3] 1384: Tokenized into 168 tokens.
[preprocess][L3] 1384: Removed stop words; 96 tokens remain.
[preprocess][L3] 1384: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1384'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1384'.
[indexDocument][L3] Recorded doc length 96 for '1384'.
[indexDocument][L3] Tokens added to the inverted index for '1384'.
[indexDocument][L2] Indexing document '256'.
[indexDocument][L3] Opened file '256'.
[preprocess][L2] Starting preprocessing for '256'.
[preprocess][L3] 256: Tokenized into 133 tokens.
[preprocess][L3] 256: Removed stop words; 75 tokens remain.
[preprocess][L3] 256: Stemming complete.
[indexDocument][L3] Preprocessing complete for '256'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '256'.
[indexDocument][L3] Recorded doc length 75 for '256'.
[indexDocument][L3] Tokens added to the inverted index for '256'.
[indexDocument][L2] Indexing document '64'.
[indexDocument][L3] Opened file '64'.
[preprocess][L2] Starting preprocessing for '64'.
[preprocess][L3] 64: Tokenized into 149 tokens.
[preprocess][L3] 64: Removed stop words; 98 tokens remain.
[preprocess][L3] 64: Stemming complete.
[indexDocument][L3] Preprocessing complete for '64'; 98 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '64'.
[indexDocument][L3] Recorded doc length 98 for '64'.
[indexDocument][L3] Tokens added to the inverted index for '64'.
[indexDocument][L2] Indexing document '858'.
[indexDocument][L3] Opened file '858'.
[preprocess][L2] Starting preprocessing for '858'.
[preprocess][L3] 858: Tokenized into 199 tokens.
[preprocess][L3] 858: Removed stop words; 120 tokens remain.
[preprocess][L3] 858: Stemming complete.
[indexDocument][L3] Preprocessing complete for '858'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '858'.
[indexDocument][L3] Recorded doc length 120 for '858'.
[indexDocument][L3] Tokens added to the inverted index for '858'.
[indexDocument][L2] Indexing document '1370'.
[indexDocument][L3] Opened file '1370'.
[preprocess][L2] Starting preprocessing for '1370'.
[preprocess][L3] 1370: Tokenized into 259 tokens.
[preprocess][L3] 1370: Removed stop words; 135 tokens remain.
[preprocess][L3] 1370: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1370'; 135 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1370'.
[indexDocument][L3] Recorded doc length 135 for '1370'.
[indexDocument][L3] Tokens added to the inverted index for '1370'.
[indexDocument][L2] Indexing document '1142'.
[indexDocument][L3] Opened file '1142'.
[preprocess][L2] Starting preprocessing for '1142'.
[preprocess][L3] 1142: Tokenized into 69 tokens.
[preprocess][L3] 1142: Removed stop words; 45 tokens remain.
[preprocess][L3] 1142: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1142'; 45 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1142'.
[indexDocument][L3] Recorded doc length 45 for '1142'.
[indexDocument][L3] Tokens added to the inverted index for '1142'.
[indexDocument][L2] Indexing document '90'.
[indexDocument][L3] Opened file '90'.
[preprocess][L2] Starting preprocessing for '90'.
[preprocess][L3] 90: Tokenized into 110 tokens.
[preprocess][L3] 90: Removed stop words; 66 tokens remain.
[preprocess][L3] 90: Stemming complete.
[indexDocument][L3] Preprocessing complete for '90'; 66 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '90'.
[indexDocument][L3] Recorded doc length 66 for '90'.
[indexDocument][L3] Tokens added to the inverted index for '90'.
[indexDocument][L2] Indexing document '1189'.
[indexDocument][L3] Opened file '1189'.
[preprocess][L2] Starting preprocessing for '1189'.
[preprocess][L3] 1189: Tokenized into 118 tokens.
[preprocess][L3] 1189: Removed stop words; 69 tokens remain.
[preprocess][L3] 1189: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1189'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1189'.
[indexDocument][L3] Recorded doc length 69 for '1189'.
[indexDocument][L3] Tokens added to the inverted index for '1189'.
[indexDocument][L2] Indexing document '867'.
[indexDocument][L3] Opened file '867'.
[preprocess][L2] Starting preprocessing for '867'.
[preprocess][L3] 867: Tokenized into 142 tokens.
[preprocess][L3] 867: Removed stop words; 89 tokens remain.
[preprocess][L3] 867: Stemming complete.
[indexDocument][L3] Preprocessing complete for '867'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '867'.
[indexDocument][L3] Recorded doc length 89 for '867'.
[indexDocument][L3] Tokens added to the inverted index for '867'.
[indexDocument][L2] Indexing document '269'.
[indexDocument][L3] Opened file '269'.
[preprocess][L2] Starting preprocessing for '269'.
[preprocess][L3] 269: Tokenized into 124 tokens.
[preprocess][L3] 269: Removed stop words; 76 tokens remain.
[preprocess][L3] 269: Stemming complete.
[indexDocument][L3] Preprocessing complete for '269'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '269'.
[indexDocument][L3] Recorded doc length 76 for '269'.
[indexDocument][L3] Tokens added to the inverted index for '269'.
[indexDocument][L2] Indexing document '893'.
[indexDocument][L3] Opened file '893'.
[preprocess][L2] Starting preprocessing for '893'.
[preprocess][L3] 893: Tokenized into 128 tokens.
[preprocess][L3] 893: Removed stop words; 69 tokens remain.
[preprocess][L3] 893: Stemming complete.
[indexDocument][L3] Preprocessing complete for '893'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '893'.
[indexDocument][L3] Recorded doc length 69 for '893'.
[indexDocument][L3] Tokens added to the inverted index for '893'.
[indexDocument][L2] Indexing document '260'.
[indexDocument][L3] Opened file '260'.
[preprocess][L2] Starting preprocessing for '260'.
[preprocess][L3] 260: Tokenized into 120 tokens.
[preprocess][L3] 260: Removed stop words; 75 tokens remain.
[preprocess][L3] 260: Stemming complete.
[indexDocument][L3] Preprocessing complete for '260'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '260'.
[indexDocument][L3] Recorded doc length 75 for '260'.
[indexDocument][L3] Tokens added to the inverted index for '260'.
[indexDocument][L2] Indexing document '52'.
[indexDocument][L3] Opened file '52'.
[preprocess][L2] Starting preprocessing for '52'.
[preprocess][L3] 52: Tokenized into 187 tokens.
[preprocess][L3] 52: Removed stop words; 112 tokens remain.
[preprocess][L3] 52: Stemming complete.
[indexDocument][L3] Preprocessing complete for '52'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '52'.
[indexDocument][L3] Recorded doc length 112 for '52'.
[indexDocument][L3] Tokens added to the inverted index for '52'.
[indexDocument][L2] Indexing document '1180'.
[indexDocument][L3] Opened file '1180'.
[preprocess][L2] Starting preprocessing for '1180'.
[preprocess][L3] 1180: Tokenized into 162 tokens.
[preprocess][L3] 1180: Removed stop words; 103 tokens remain.
[preprocess][L3] 1180: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1180'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1180'.
[indexDocument][L3] Recorded doc length 103 for '1180'.
[indexDocument][L3] Tokens added to the inverted index for '1180'.
[indexDocument][L2] Indexing document '436'.
[indexDocument][L3] Opened file '436'.
[preprocess][L2] Starting preprocessing for '436'.
[preprocess][L3] 436: Tokenized into 72 tokens.
[preprocess][L3] 436: Removed stop words; 48 tokens remain.
[preprocess][L3] 436: Stemming complete.
[indexDocument][L3] Preprocessing complete for '436'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '436'.
[indexDocument][L3] Recorded doc length 48 for '436'.
[indexDocument][L3] Tokens added to the inverted index for '436'.
[indexDocument][L2] Indexing document '604'.
[indexDocument][L3] Opened file '604'.
[preprocess][L2] Starting preprocessing for '604'.
[preprocess][L3] 604: Tokenized into 141 tokens.
[preprocess][L3] 604: Removed stop words; 86 tokens remain.
[preprocess][L3] 604: Stemming complete.
[indexDocument][L3] Preprocessing complete for '604'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '604'.
[indexDocument][L3] Recorded doc length 86 for '604'.
[indexDocument][L3] Tokens added to the inverted index for '604'.
[indexDocument][L2] Indexing document '294'.
[indexDocument][L3] Opened file '294'.
[preprocess][L2] Starting preprocessing for '294'.
[preprocess][L3] 294: Tokenized into 272 tokens.
[preprocess][L3] 294: Removed stop words; 168 tokens remain.
[preprocess][L3] 294: Stemming complete.
[indexDocument][L3] Preprocessing complete for '294'; 168 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '294'.
[indexDocument][L3] Recorded doc length 168 for '294'.
[indexDocument][L3] Tokens added to the inverted index for '294'.
[indexDocument][L2] Indexing document '1346'.
[indexDocument][L3] Opened file '1346'.
[preprocess][L2] Starting preprocessing for '1346'.
[preprocess][L3] 1346: Tokenized into 124 tokens.
[preprocess][L3] 1346: Removed stop words; 78 tokens remain.
[preprocess][L3] 1346: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1346'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1346'.
[indexDocument][L3] Recorded doc length 78 for '1346'.
[indexDocument][L3] Tokens added to the inverted index for '1346'.
[indexDocument][L2] Indexing document '1174'.
[indexDocument][L3] Opened file '1174'.
[preprocess][L2] Starting preprocessing for '1174'.
[preprocess][L3] 1174: Tokenized into 81 tokens.
[preprocess][L3] 1174: Removed stop words; 55 tokens remain.
[preprocess][L3] 1174: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1174'; 55 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1174'.
[indexDocument][L3] Recorded doc length 55 for '1174'.
[indexDocument][L3] Tokens added to the inverted index for '1174'.
[indexDocument][L2] Indexing document '409'.
[indexDocument][L3] Opened file '409'.
[preprocess][L2] Starting preprocessing for '409'.
[preprocess][L3] 409: Tokenized into 103 tokens.
[preprocess][L3] 409: Removed stop words; 62 tokens remain.
[preprocess][L3] 409: Stemming complete.
[indexDocument][L3] Preprocessing complete for '409'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '409'.
[indexDocument][L3] Recorded doc length 62 for '409'.
[indexDocument][L3] Tokens added to the inverted index for '409'.
[indexDocument][L2] Indexing document '851'.
[indexDocument][L3] Opened file '851'.
[preprocess][L2] Starting preprocessing for '851'.
[preprocess][L3] 851: Tokenized into 132 tokens.
[preprocess][L3] 851: Removed stop words; 82 tokens remain.
[preprocess][L3] 851: Stemming complete.
[indexDocument][L3] Preprocessing complete for '851'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '851'.
[indexDocument][L3] Recorded doc length 82 for '851'.
[indexDocument][L3] Tokens added to the inverted index for '851'.
[indexDocument][L2] Indexing document '99'.
[indexDocument][L3] Opened file '99'.
[preprocess][L2] Starting preprocessing for '99'.
[preprocess][L3] 99: Tokenized into 301 tokens.
[preprocess][L3] 99: Removed stop words; 171 tokens remain.
[preprocess][L3] 99: Stemming complete.
[indexDocument][L3] Preprocessing complete for '99'; 171 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '99'.
[indexDocument][L3] Recorded doc length 171 for '99'.
[indexDocument][L3] Tokens added to the inverted index for '99'.
[indexDocument][L2] Indexing document '1379'.
[indexDocument][L3] Opened file '1379'.
[preprocess][L2] Starting preprocessing for '1379'.
[preprocess][L3] 1379: Tokenized into 112 tokens.
[preprocess][L3] 1379: Removed stop words; 69 tokens remain.
[preprocess][L3] 1379: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1379'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1379'.
[indexDocument][L3] Recorded doc length 69 for '1379'.
[indexDocument][L3] Tokens added to the inverted index for '1379'.
[indexDocument][L2] Indexing document '1173'.
[indexDocument][L3] Opened file '1173'.
[preprocess][L2] Starting preprocessing for '1173'.
[preprocess][L3] 1173: Tokenized into 169 tokens.
[preprocess][L3] 1173: Removed stop words; 109 tokens remain.
[preprocess][L3] 1173: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1173'; 109 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1173'.
[indexDocument][L3] Recorded doc length 109 for '1173'.
[indexDocument][L3] Tokens added to the inverted index for '1173'.
[indexDocument][L2] Indexing document '293'.
[indexDocument][L3] Opened file '293'.
[preprocess][L2] Starting preprocessing for '293'.
[preprocess][L3] 293: Tokenized into 138 tokens.
[preprocess][L3] 293: Removed stop words; 85 tokens remain.
[preprocess][L3] 293: Stemming complete.
[indexDocument][L3] Preprocessing complete for '293'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '293'.
[indexDocument][L3] Recorded doc length 85 for '293'.
[indexDocument][L3] Tokens added to the inverted index for '293'.
[indexDocument][L2] Indexing document '1341'.
[indexDocument][L3] Opened file '1341'.
[preprocess][L2] Starting preprocessing for '1341'.
[preprocess][L3] 1341: Tokenized into 214 tokens.
[preprocess][L3] 1341: Removed stop words; 128 tokens remain.
[preprocess][L3] 1341: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1341'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1341'.
[indexDocument][L3] Recorded doc length 128 for '1341'.
[indexDocument][L3] Tokens added to the inverted index for '1341'.
[indexDocument][L2] Indexing document '603'.
[indexDocument][L3] Opened file '603'.
[preprocess][L2] Starting preprocessing for '603'.
[preprocess][L3] 603: Tokenized into 223 tokens.
[preprocess][L3] 603: Removed stop words; 126 tokens remain.
[preprocess][L3] 603: Stemming complete.
[indexDocument][L3] Preprocessing complete for '603'; 126 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '603'.
[indexDocument][L3] Recorded doc length 126 for '603'.
[indexDocument][L3] Tokens added to the inverted index for '603'.
[indexDocument][L2] Indexing document '431'.
[indexDocument][L3] Opened file '431'.
[preprocess][L2] Starting preprocessing for '431'.
[preprocess][L3] 431: Tokenized into 164 tokens.
[preprocess][L3] 431: Removed stop words; 103 tokens remain.
[preprocess][L3] 431: Stemming complete.
[indexDocument][L3] Preprocessing complete for '431'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '431'.
[indexDocument][L3] Recorded doc length 103 for '431'.
[indexDocument][L3] Tokens added to the inverted index for '431'.
[indexDocument][L2] Indexing document '55'.
[indexDocument][L3] Opened file '55'.
[preprocess][L2] Starting preprocessing for '55'.
[preprocess][L3] 55: Tokenized into 146 tokens.
[preprocess][L3] 55: Removed stop words; 87 tokens remain.
[preprocess][L3] 55: Stemming complete.
[indexDocument][L3] Preprocessing complete for '55'; 87 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '55'.
[indexDocument][L3] Recorded doc length 87 for '55'.
[indexDocument][L3] Tokens added to the inverted index for '55'.
[indexDocument][L2] Indexing document '1187'.
[indexDocument][L3] Opened file '1187'.
[preprocess][L2] Starting preprocessing for '1187'.
[preprocess][L3] 1187: Tokenized into 126 tokens.
[preprocess][L3] 1187: Removed stop words; 84 tokens remain.
[preprocess][L3] 1187: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1187'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1187'.
[indexDocument][L3] Recorded doc length 84 for '1187'.
[indexDocument][L3] Tokens added to the inverted index for '1187'.
[indexDocument][L2] Indexing document '869'.
[indexDocument][L3] Opened file '869'.
[preprocess][L2] Starting preprocessing for '869'.
[preprocess][L3] 869: Tokenized into 297 tokens.
[preprocess][L3] 869: Removed stop words; 190 tokens remain.
[preprocess][L3] 869: Stemming complete.
[indexDocument][L3] Preprocessing complete for '869'; 190 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '869'.
[indexDocument][L3] Recorded doc length 190 for '869'.
[indexDocument][L3] Tokens added to the inverted index for '869'.
[indexDocument][L2] Indexing document '267'.
[indexDocument][L3] Opened file '267'.
[preprocess][L2] Starting preprocessing for '267'.
[preprocess][L3] 267: Tokenized into 203 tokens.
[preprocess][L3] 267: Removed stop words; 115 tokens remain.
[preprocess][L3] 267: Stemming complete.
[indexDocument][L3] Preprocessing complete for '267'; 115 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '267'.
[indexDocument][L3] Recorded doc length 115 for '267'.
[indexDocument][L3] Tokens added to the inverted index for '267'.
[indexDocument][L2] Indexing document '258'.
[indexDocument][L3] Opened file '258'.
[preprocess][L2] Starting preprocessing for '258'.
[preprocess][L3] 258: Tokenized into 69 tokens.
[preprocess][L3] 258: Removed stop words; 46 tokens remain.
[preprocess][L3] 258: Stemming complete.
[indexDocument][L3] Preprocessing complete for '258'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '258'.
[indexDocument][L3] Recorded doc length 46 for '258'.
[indexDocument][L3] Tokens added to the inverted index for '258'.
[indexDocument][L2] Indexing document '856'.
[indexDocument][L3] Opened file '856'.
[preprocess][L2] Starting preprocessing for '856'.
[preprocess][L3] 856: Tokenized into 251 tokens.
[preprocess][L3] 856: Removed stop words; 149 tokens remain.
[preprocess][L3] 856: Stemming complete.
[indexDocument][L3] Preprocessing complete for '856'; 149 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '856'.
[indexDocument][L3] Recorded doc length 149 for '856'.
[indexDocument][L3] Tokens added to the inverted index for '856'.
[indexDocument][L2] Indexing document '9'.
[indexDocument][L3] Opened file '9'.
[preprocess][L2] Starting preprocessing for '9'.
[preprocess][L3] 9: Tokenized into 339 tokens.
[preprocess][L3] 9: Removed stop words; 208 tokens remain.
[preprocess][L3] 9: Stemming complete.
[indexDocument][L3] Preprocessing complete for '9'; 208 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '9'.
[indexDocument][L3] Recorded doc length 208 for '9'.
[indexDocument][L3] Tokens added to the inverted index for '9'.
[indexDocument][L2] Indexing document '835'.
[indexDocument][L3] Opened file '835'.
[preprocess][L2] Starting preprocessing for '835'.
[preprocess][L3] 835: Tokenized into 77 tokens.
[preprocess][L3] 835: Removed stop words; 49 tokens remain.
[preprocess][L3] 835: Stemming complete.
[indexDocument][L3] Preprocessing complete for '835'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '835'.
[indexDocument][L3] Recorded doc length 49 for '835'.
[indexDocument][L3] Tokens added to the inverted index for '835'.
[indexDocument][L2] Indexing document '499'.
[indexDocument][L3] Opened file '499'.
[preprocess][L2] Starting preprocessing for '499'.
[preprocess][L3] 499: Tokenized into 393 tokens.
[preprocess][L3] 499: Removed stop words; 198 tokens remain.
[preprocess][L3] 499: Stemming complete.
[indexDocument][L3] Preprocessing complete for '499'; 198 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '499'.
[indexDocument][L3] Recorded doc length 198 for '499'.
[indexDocument][L3] Tokens added to the inverted index for '499'.
[indexDocument][L2] Indexing document '36'.
[indexDocument][L3] Opened file '36'.
[preprocess][L2] Starting preprocessing for '36'.
[preprocess][L3] 36: Tokenized into 146 tokens.
[preprocess][L3] 36: Removed stop words; 89 tokens remain.
[preprocess][L3] 36: Stemming complete.
[indexDocument][L3] Preprocessing complete for '36'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '36'.
[indexDocument][L3] Recorded doc length 89 for '36'.
[indexDocument][L3] Tokens added to the inverted index for '36'.
[indexDocument][L2] Indexing document '204'.
[indexDocument][L3] Opened file '204'.
[preprocess][L2] Starting preprocessing for '204'.
[preprocess][L3] 204: Tokenized into 188 tokens.
[preprocess][L3] 204: Removed stop words; 114 tokens remain.
[preprocess][L3] 204: Stemming complete.
[indexDocument][L3] Preprocessing complete for '204'; 114 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '204'.
[indexDocument][L3] Recorded doc length 114 for '204'.
[indexDocument][L3] Tokens added to the inverted index for '204'.
[indexDocument][L2] Indexing document '660'.
[indexDocument][L3] Opened file '660'.
[preprocess][L2] Starting preprocessing for '660'.
[preprocess][L3] 660: Tokenized into 297 tokens.
[preprocess][L3] 660: Removed stop words; 169 tokens remain.
[preprocess][L3] 660: Stemming complete.
[indexDocument][L3] Preprocessing complete for '660'; 169 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '660'.
[indexDocument][L3] Recorded doc length 169 for '660'.
[indexDocument][L3] Tokens added to the inverted index for '660'.
[indexDocument][L2] Indexing document '452'.
[indexDocument][L3] Opened file '452'.
[preprocess][L2] Starting preprocessing for '452'.
[preprocess][L3] 452: Tokenized into 354 tokens.
[preprocess][L3] 452: Removed stop words; 185 tokens remain.
[preprocess][L3] 452: Stemming complete.
[indexDocument][L3] Preprocessing complete for '452'; 185 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '452'.
[indexDocument][L3] Recorded doc length 185 for '452'.
[indexDocument][L3] Tokens added to the inverted index for '452'.
[indexDocument][L2] Indexing document '694'.
[indexDocument][L3] Opened file '694'.
[preprocess][L2] Starting preprocessing for '694'.
[preprocess][L3] 694: Tokenized into 181 tokens.
[preprocess][L3] 694: Removed stop words; 114 tokens remain.
[preprocess][L3] 694: Stemming complete.
[indexDocument][L3] Preprocessing complete for '694'; 114 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '694'.
[indexDocument][L3] Recorded doc length 114 for '694'.
[indexDocument][L3] Tokens added to the inverted index for '694'.
[indexDocument][L2] Indexing document '1110'.
[indexDocument][L3] Opened file '1110'.
[preprocess][L2] Starting preprocessing for '1110'.
[preprocess][L3] 1110: Tokenized into 182 tokens.
[preprocess][L3] 1110: Removed stop words; 93 tokens remain.
[preprocess][L3] 1110: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1110'; 93 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1110'.
[indexDocument][L3] Recorded doc length 93 for '1110'.
[indexDocument][L3] Tokens added to the inverted index for '1110'.
[indexDocument][L2] Indexing document '1322'.
[indexDocument][L3] Opened file '1322'.
[preprocess][L2] Starting preprocessing for '1322'.
[preprocess][L3] 1322: Tokenized into 243 tokens.
[preprocess][L3] 1322: Removed stop words; 136 tokens remain.
[preprocess][L3] 1322: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1322'; 136 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1322'.
[indexDocument][L3] Recorded doc length 136 for '1322'.
[indexDocument][L3] Tokens added to the inverted index for '1322'.
[indexDocument][L2] Indexing document '1128'.
[indexDocument][L3] Opened file '1128'.
[preprocess][L2] Starting preprocessing for '1128'.
[preprocess][L3] 1128: Tokenized into 134 tokens.
[preprocess][L3] 1128: Removed stop words; 80 tokens remain.
[preprocess][L3] 1128: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1128'; 80 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1128'.
[indexDocument][L3] Recorded doc length 80 for '1128'.
[indexDocument][L3] Tokens added to the inverted index for '1128'.
[indexDocument][L2] Indexing document '832'.
[indexDocument][L3] Opened file '832'.
[preprocess][L2] Starting preprocessing for '832'.
[preprocess][L3] 832: Tokenized into 53 tokens.
[preprocess][L3] 832: Removed stop words; 31 tokens remain.
[preprocess][L3] 832: Stemming complete.
[indexDocument][L3] Preprocessing complete for '832'; 31 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '832'.
[indexDocument][L3] Recorded doc length 31 for '832'.
[indexDocument][L3] Tokens added to the inverted index for '832'.
[indexDocument][L2] Indexing document '658'.
[indexDocument][L3] Opened file '658'.
[preprocess][L2] Starting preprocessing for '658'.
[preprocess][L3] 658: Tokenized into 264 tokens.
[preprocess][L3] 658: Removed stop words; 146 tokens remain.
[preprocess][L3] 658: Stemming complete.
[indexDocument][L3] Preprocessing complete for '658'; 146 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '658'.
[indexDocument][L3] Recorded doc length 146 for '658'.
[indexDocument][L3] Tokens added to the inverted index for '658'.
[indexDocument][L2] Indexing document '1325'.
[indexDocument][L3] Opened file '1325'.
[preprocess][L2] Starting preprocessing for '1325'.
[preprocess][L3] 1325: Tokenized into 336 tokens.
[preprocess][L3] 1325: Removed stop words; 189 tokens remain.
[preprocess][L3] 1325: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1325'; 189 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1325'.
[indexDocument][L3] Recorded doc length 189 for '1325'.
[indexDocument][L3] Tokens added to the inverted index for '1325'.
[indexDocument][L2] Indexing document '1117'.
[indexDocument][L3] Opened file '1117'.
[preprocess][L2] Starting preprocessing for '1117'.
[preprocess][L3] 1117: Tokenized into 136 tokens.
[preprocess][L3] 1117: Removed stop words; 90 tokens remain.
[preprocess][L3] 1117: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1117'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1117'.
[indexDocument][L3] Recorded doc length 90 for '1117'.
[indexDocument][L3] Tokens added to the inverted index for '1117'.
[indexDocument][L2] Indexing document '693'.
[indexDocument][L3] Opened file '693'.
[preprocess][L2] Starting preprocessing for '693'.
[preprocess][L3] 693: Tokenized into 192 tokens.
[preprocess][L3] 693: Removed stop words; 108 tokens remain.
[preprocess][L3] 693: Stemming complete.
[indexDocument][L3] Preprocessing complete for '693'; 108 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '693'.
[indexDocument][L3] Recorded doc length 108 for '693'.
[indexDocument][L3] Tokens added to the inverted index for '693'.
[indexDocument][L2] Indexing document '455'.
[indexDocument][L3] Opened file '455'.
[preprocess][L2] Starting preprocessing for '455'.
[preprocess][L3] 455: Tokenized into 206 tokens.
[preprocess][L3] 455: Removed stop words; 121 tokens remain.
[preprocess][L3] 455: Stemming complete.
[indexDocument][L3] Preprocessing complete for '455'; 121 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '455'.
[indexDocument][L3] Recorded doc length 121 for '455'.
[indexDocument][L3] Tokens added to the inverted index for '455'.
[indexDocument][L2] Indexing document '667'.
[indexDocument][L3] Opened file '667'.
[preprocess][L2] Starting preprocessing for '667'.
[preprocess][L3] 667: Tokenized into 249 tokens.
[preprocess][L3] 667: Removed stop words; 153 tokens remain.
[preprocess][L3] 667: Stemming complete.
[indexDocument][L3] Preprocessing complete for '667'; 153 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '667'.
[indexDocument][L3] Recorded doc length 153 for '667'.
[indexDocument][L3] Tokens added to the inverted index for '667'.
[indexDocument][L2] Indexing document '203'.
[indexDocument][L3] Opened file '203'.
[preprocess][L2] Starting preprocessing for '203'.
[preprocess][L3] 203: Tokenized into 71 tokens.
[preprocess][L3] 203: Removed stop words; 46 tokens remain.
[preprocess][L3] 203: Stemming complete.
[indexDocument][L3] Preprocessing complete for '203'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '203'.
[indexDocument][L3] Recorded doc length 46 for '203'.
[indexDocument][L3] Tokens added to the inverted index for '203'.
[indexDocument][L2] Indexing document '31'.
[indexDocument][L3] Opened file '31'.
[preprocess][L2] Starting preprocessing for '31'.
[preprocess][L3] 31: Tokenized into 43 tokens.
[preprocess][L3] 31: Removed stop words; 32 tokens remain.
[preprocess][L3] 31: Stemming complete.
[indexDocument][L3] Preprocessing complete for '31'; 32 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '31'.
[indexDocument][L3] Recorded doc length 32 for '31'.
[indexDocument][L3] Tokens added to the inverted index for '31'.
[indexDocument][L2] Indexing document '38'.
[indexDocument][L3] Opened file '38'.
[preprocess][L2] Starting preprocessing for '38'.
[preprocess][L3] 38: Tokenized into 89 tokens.
[preprocess][L3] 38: Removed stop words; 57 tokens remain.
[preprocess][L3] 38: Stemming complete.
[indexDocument][L3] Preprocessing complete for '38'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '38'.
[indexDocument][L3] Recorded doc length 57 for '38'.
[indexDocument][L3] Tokens added to the inverted index for '38'.
[indexDocument][L2] Indexing document '804'.
[indexDocument][L3] Opened file '804'.
[preprocess][L2] Starting preprocessing for '804'.
[preprocess][L3] 804: Tokenized into 164 tokens.
[preprocess][L3] 804: Removed stop words; 103 tokens remain.
[preprocess][L3] 804: Stemming complete.
[indexDocument][L3] Preprocessing complete for '804'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '804'.
[indexDocument][L3] Recorded doc length 103 for '804'.
[indexDocument][L3] Tokens added to the inverted index for '804'.
[indexDocument][L2] Indexing document '497'.
[indexDocument][L3] Opened file '497'.
[preprocess][L2] Starting preprocessing for '497'.
[preprocess][L3] 497: Tokenized into 154 tokens.
[preprocess][L3] 497: Removed stop words; 100 tokens remain.
[preprocess][L3] 497: Stemming complete.
[indexDocument][L3] Preprocessing complete for '497'; 100 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '497'.
[indexDocument][L3] Recorded doc length 100 for '497'.
[indexDocument][L3] Tokens added to the inverted index for '497'.
[indexDocument][L2] Indexing document '1313'.
[indexDocument][L3] Opened file '1313'.
[preprocess][L2] Starting preprocessing for '1313'.
[preprocess][L3] 1313: Tokenized into 663 tokens.
[preprocess][L3] 1313: Removed stop words; 374 tokens remain.
[preprocess][L3] 1313: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1313'; 374 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1313'.
[indexDocument][L3] Recorded doc length 374 for '1313'.
[indexDocument][L3] Tokens added to the inverted index for '1313'.
[indexDocument][L2] Indexing document '1121'.
[indexDocument][L3] Opened file '1121'.
[preprocess][L2] Starting preprocessing for '1121'.
[preprocess][L3] 1121: Tokenized into 129 tokens.
[preprocess][L3] 1121: Removed stop words; 79 tokens remain.
[preprocess][L3] 1121: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1121'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1121'.
[indexDocument][L3] Recorded doc length 79 for '1121'.
[indexDocument][L3] Tokens added to the inverted index for '1121'.
[indexDocument][L2] Indexing document '235'.
[indexDocument][L3] Opened file '235'.
[preprocess][L2] Starting preprocessing for '235'.
[preprocess][L3] 235: Tokenized into 145 tokens.
[preprocess][L3] 235: Removed stop words; 84 tokens remain.
[preprocess][L3] 235: Stemming complete.
[indexDocument][L3] Preprocessing complete for '235'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '235'.
[indexDocument][L3] Recorded doc length 84 for '235'.
[indexDocument][L3] Tokens added to the inverted index for '235'.
[indexDocument][L2] Indexing document '7'.
[indexDocument][L3] Opened file '7'.
[preprocess][L2] Starting preprocessing for '7'.
[preprocess][L3] 7: Tokenized into 233 tokens.
[preprocess][L3] 7: Removed stop words; 152 tokens remain.
[preprocess][L3] 7: Stemming complete.
[indexDocument][L3] Preprocessing complete for '7'; 152 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '7'.
[indexDocument][L3] Recorded doc length 152 for '7'.
[indexDocument][L3] Tokens added to the inverted index for '7'.
[indexDocument][L2] Indexing document '463'.
[indexDocument][L3] Opened file '463'.
[preprocess][L2] Starting preprocessing for '463'.
[preprocess][L3] 463: Tokenized into 110 tokens.
[preprocess][L3] 463: Removed stop words; 69 tokens remain.
[preprocess][L3] 463: Stemming complete.
[indexDocument][L3] Preprocessing complete for '463'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '463'.
[indexDocument][L3] Recorded doc length 69 for '463'.
[indexDocument][L3] Tokens added to the inverted index for '463'.
[indexDocument][L2] Indexing document '651'.
[indexDocument][L3] Opened file '651'.
[preprocess][L2] Starting preprocessing for '651'.
[preprocess][L3] 651: Tokenized into 191 tokens.
[preprocess][L3] 651: Removed stop words; 117 tokens remain.
[preprocess][L3] 651: Stemming complete.
[indexDocument][L3] Preprocessing complete for '651'; 117 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '651'.
[indexDocument][L3] Recorded doc length 117 for '651'.
[indexDocument][L3] Tokens added to the inverted index for '651'.
[indexDocument][L2] Indexing document '803'.
[indexDocument][L3] Opened file '803'.
[preprocess][L2] Starting preprocessing for '803'.
[preprocess][L3] 803: Tokenized into 142 tokens.
[preprocess][L3] 803: Removed stop words; 74 tokens remain.
[preprocess][L3] 803: Stemming complete.
[indexDocument][L3] Preprocessing complete for '803'; 74 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '803'.
[indexDocument][L3] Recorded doc length 74 for '803'.
[indexDocument][L3] Tokens added to the inverted index for '803'.
[indexDocument][L2] Indexing document '669'.
[indexDocument][L3] Opened file '669'.
[preprocess][L2] Starting preprocessing for '669'.
[preprocess][L3] 669: Tokenized into 99 tokens.
[preprocess][L3] 669: Removed stop words; 64 tokens remain.
[preprocess][L3] 669: Stemming complete.
[indexDocument][L3] Preprocessing complete for '669'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '669'.
[indexDocument][L3] Recorded doc length 64 for '669'.
[indexDocument][L3] Tokens added to the inverted index for '669'.
[indexDocument][L2] Indexing document '1119'.
[indexDocument][L3] Opened file '1119'.
[preprocess][L2] Starting preprocessing for '1119'.
[preprocess][L3] 1119: Tokenized into 290 tokens.
[preprocess][L3] 1119: Removed stop words; 181 tokens remain.
[preprocess][L3] 1119: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1119'; 181 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1119'.
[indexDocument][L3] Recorded doc length 181 for '1119'.
[indexDocument][L3] Tokens added to the inverted index for '1119'.
[indexDocument][L2] Indexing document '656'.
[indexDocument][L3] Opened file '656'.
[preprocess][L2] Starting preprocessing for '656'.
[preprocess][L3] 656: Tokenized into 252 tokens.
[preprocess][L3] 656: Removed stop words; 140 tokens remain.
[preprocess][L3] 656: Stemming complete.
[indexDocument][L3] Preprocessing complete for '656'; 140 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '656'.
[indexDocument][L3] Recorded doc length 140 for '656'.
[indexDocument][L3] Tokens added to the inverted index for '656'.
[indexDocument][L2] Indexing document '464'.
[indexDocument][L3] Opened file '464'.
[preprocess][L2] Starting preprocessing for '464'.
[preprocess][L3] 464: Tokenized into 179 tokens.
[preprocess][L3] 464: Removed stop words; 112 tokens remain.
[preprocess][L3] 464: Stemming complete.
[indexDocument][L3] Preprocessing complete for '464'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '464'.
[indexDocument][L3] Recorded doc length 112 for '464'.
[indexDocument][L3] Tokens added to the inverted index for '464'.
[indexDocument][L2] Indexing document '232'.
[indexDocument][L3] Opened file '232'.
[preprocess][L2] Starting preprocessing for '232'.
[preprocess][L3] 232: Tokenized into 228 tokens.
[preprocess][L3] 232: Removed stop words; 146 tokens remain.
[preprocess][L3] 232: Stemming complete.
[indexDocument][L3] Preprocessing complete for '232'; 146 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '232'.
[indexDocument][L3] Recorded doc length 146 for '232'.
[indexDocument][L3] Tokens added to the inverted index for '232'.
[indexDocument][L2] Indexing document '1126'.
[indexDocument][L3] Opened file '1126'.
[preprocess][L2] Starting preprocessing for '1126'.
[preprocess][L3] 1126: Tokenized into 98 tokens.
[preprocess][L3] 1126: Removed stop words; 60 tokens remain.
[preprocess][L3] 1126: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1126'; 60 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1126'.
[indexDocument][L3] Recorded doc length 60 for '1126'.
[indexDocument][L3] Tokens added to the inverted index for '1126'.
[indexDocument][L2] Indexing document '1314'.
[indexDocument][L3] Opened file '1314'.
[preprocess][L2] Starting preprocessing for '1314'.
[preprocess][L3] 1314: Tokenized into 84 tokens.
[preprocess][L3] 1314: Removed stop words; 56 tokens remain.
[preprocess][L3] 1314: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1314'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1314'.
[indexDocument][L3] Recorded doc length 56 for '1314'.
[indexDocument][L3] Tokens added to the inverted index for '1314'.
[indexDocument][L2] Indexing document '490'.
[indexDocument][L3] Opened file '490'.
[preprocess][L2] Starting preprocessing for '490'.
[preprocess][L3] 490: Tokenized into 88 tokens.
[preprocess][L3] 490: Removed stop words; 53 tokens remain.
[preprocess][L3] 490: Stemming complete.
[indexDocument][L3] Preprocessing complete for '490'; 53 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '490'.
[indexDocument][L3] Recorded doc length 53 for '490'.
[indexDocument][L3] Tokens added to the inverted index for '490'.
[indexDocument][L2] Indexing document '430'.
[indexDocument][L3] Opened file '430'.
[preprocess][L2] Starting preprocessing for '430'.
[preprocess][L3] 430: Tokenized into 68 tokens.
[preprocess][L3] 430: Removed stop words; 48 tokens remain.
[preprocess][L3] 430: Stemming complete.
[indexDocument][L3] Preprocessing complete for '430'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '430'.
[indexDocument][L3] Recorded doc length 48 for '430'.
[indexDocument][L3] Tokens added to the inverted index for '430'.
[indexDocument][L2] Indexing document '602'.
[indexDocument][L3] Opened file '602'.
[preprocess][L2] Starting preprocessing for '602'.
[preprocess][L3] 602: Tokenized into 144 tokens.
[preprocess][L3] 602: Removed stop words; 87 tokens remain.
[preprocess][L3] 602: Stemming complete.
[indexDocument][L3] Preprocessing complete for '602'; 87 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '602'.
[indexDocument][L3] Recorded doc length 87 for '602'.
[indexDocument][L3] Tokens added to the inverted index for '602'.
[indexDocument][L2] Indexing document '266'.
[indexDocument][L3] Opened file '266'.
[preprocess][L2] Starting preprocessing for '266'.
[preprocess][L3] 266: Tokenized into 264 tokens.
[preprocess][L3] 266: Removed stop words; 151 tokens remain.
[preprocess][L3] 266: Stemming complete.
[indexDocument][L3] Preprocessing complete for '266'; 151 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '266'.
[indexDocument][L3] Recorded doc length 151 for '266'.
[indexDocument][L3] Tokens added to the inverted index for '266'.
[indexDocument][L2] Indexing document '868'.
[indexDocument][L3] Opened file '868'.
[preprocess][L2] Starting preprocessing for '868'.
[preprocess][L3] 868: Tokenized into 100 tokens.
[preprocess][L3] 868: Removed stop words; 56 tokens remain.
[preprocess][L3] 868: Stemming complete.
[indexDocument][L3] Preprocessing complete for '868'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '868'.
[indexDocument][L3] Recorded doc length 56 for '868'.
[indexDocument][L3] Tokens added to the inverted index for '868'.
[indexDocument][L2] Indexing document '54'.
[indexDocument][L3] Opened file '54'.
[preprocess][L2] Starting preprocessing for '54'.
[preprocess][L3] 54: Tokenized into 219 tokens.
[preprocess][L3] 54: Removed stop words; 134 tokens remain.
[preprocess][L3] 54: Stemming complete.
[indexDocument][L3] Preprocessing complete for '54'; 134 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '54'.
[indexDocument][L3] Recorded doc length 134 for '54'.
[indexDocument][L3] Tokens added to the inverted index for '54'.
[indexDocument][L2] Indexing document '1186'.
[indexDocument][L3] Opened file '1186'.
[preprocess][L2] Starting preprocessing for '1186'.
[preprocess][L3] 1186: Tokenized into 130 tokens.
[preprocess][L3] 1186: Removed stop words; 78 tokens remain.
[preprocess][L3] 1186: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1186'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1186'.
[indexDocument][L3] Recorded doc length 78 for '1186'.
[indexDocument][L3] Tokens added to the inverted index for '1186'.
[indexDocument][L2] Indexing document '292'.
[indexDocument][L3] Opened file '292'.
[preprocess][L2] Starting preprocessing for '292'.
[preprocess][L3] 292: Tokenized into 255 tokens.
[preprocess][L3] 292: Removed stop words; 145 tokens remain.
[preprocess][L3] 292: Stemming complete.
[indexDocument][L3] Preprocessing complete for '292'; 145 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '292'.
[indexDocument][L3] Recorded doc length 145 for '292'.
[indexDocument][L3] Tokens added to the inverted index for '292'.
[indexDocument][L2] Indexing document '1340'.
[indexDocument][L3] Opened file '1340'.
[preprocess][L2] Starting preprocessing for '1340'.
[preprocess][L3] 1340: Tokenized into 116 tokens.
[preprocess][L3] 1340: Removed stop words; 70 tokens remain.
[preprocess][L3] 1340: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1340'; 70 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1340'.
[indexDocument][L3] Recorded doc length 70 for '1340'.
[indexDocument][L3] Tokens added to the inverted index for '1340'.
[indexDocument][L2] Indexing document '1172'.
[indexDocument][L3] Opened file '1172'.
[preprocess][L2] Starting preprocessing for '1172'.
[preprocess][L3] 1172: Tokenized into 183 tokens.
[preprocess][L3] 1172: Removed stop words; 104 tokens remain.
[preprocess][L3] 1172: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1172'; 104 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1172'.
[indexDocument][L3] Recorded doc length 104 for '1172'.
[indexDocument][L3] Tokens added to the inverted index for '1172'.
[indexDocument][L2] Indexing document '857'.
[indexDocument][L3] Opened file '857'.
[preprocess][L2] Starting preprocessing for '857'.
[preprocess][L3] 857: Tokenized into 234 tokens.
[preprocess][L3] 857: Removed stop words; 144 tokens remain.
[preprocess][L3] 857: Stemming complete.
[indexDocument][L3] Preprocessing complete for '857'; 144 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '857'.
[indexDocument][L3] Recorded doc length 144 for '857'.
[indexDocument][L3] Tokens added to the inverted index for '857'.
[indexDocument][L2] Indexing document '259'.
[indexDocument][L3] Opened file '259'.
[preprocess][L2] Starting preprocessing for '259'.
[preprocess][L3] 259: Tokenized into 166 tokens.
[preprocess][L3] 259: Removed stop words; 108 tokens remain.
[preprocess][L3] 259: Stemming complete.
[indexDocument][L3] Preprocessing complete for '259'; 108 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '259'.
[indexDocument][L3] Recorded doc length 108 for '259'.
[indexDocument][L3] Tokens added to the inverted index for '259'.
[indexDocument][L2] Indexing document '1175'.
[indexDocument][L3] Opened file '1175'.
[preprocess][L2] Starting preprocessing for '1175'.
[preprocess][L3] 1175: Tokenized into 287 tokens.
[preprocess][L3] 1175: Removed stop words; 158 tokens remain.
[preprocess][L3] 1175: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1175'; 158 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1175'.
[indexDocument][L3] Recorded doc length 158 for '1175'.
[indexDocument][L3] Tokens added to the inverted index for '1175'.
[indexDocument][L2] Indexing document '295'.
[indexDocument][L3] Opened file '295'.
[preprocess][L2] Starting preprocessing for '295'.
[preprocess][L3] 295: Tokenized into 126 tokens.
[preprocess][L3] 295: Removed stop words; 76 tokens remain.
[preprocess][L3] 295: Stemming complete.
[indexDocument][L3] Preprocessing complete for '295'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '295'.
[indexDocument][L3] Recorded doc length 76 for '295'.
[indexDocument][L3] Tokens added to the inverted index for '295'.
[indexDocument][L2] Indexing document '1347'.
[indexDocument][L3] Opened file '1347'.
[preprocess][L2] Starting preprocessing for '1347'.
[preprocess][L3] 1347: Tokenized into 239 tokens.
[preprocess][L3] 1347: Removed stop words; 155 tokens remain.
[preprocess][L3] 1347: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1347'; 155 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1347'.
[indexDocument][L3] Recorded doc length 155 for '1347'.
[indexDocument][L3] Tokens added to the inverted index for '1347'.
[indexDocument][L2] Indexing document '53'.
[indexDocument][L3] Opened file '53'.
[preprocess][L2] Starting preprocessing for '53'.
[preprocess][L3] 53: Tokenized into 221 tokens.
[preprocess][L3] 53: Removed stop words; 144 tokens remain.
[preprocess][L3] 53: Stemming complete.
[indexDocument][L3] Preprocessing complete for '53'; 144 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '53'.
[indexDocument][L3] Recorded doc length 144 for '53'.
[indexDocument][L3] Tokens added to the inverted index for '53'.
[indexDocument][L2] Indexing document '1181'.
[indexDocument][L3] Opened file '1181'.
[preprocess][L2] Starting preprocessing for '1181'.
[preprocess][L3] 1181: Tokenized into 146 tokens.
[preprocess][L3] 1181: Removed stop words; 86 tokens remain.
[preprocess][L3] 1181: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1181'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1181'.
[indexDocument][L3] Recorded doc length 86 for '1181'.
[indexDocument][L3] Tokens added to the inverted index for '1181'.
[indexDocument][L2] Indexing document '261'.
[indexDocument][L3] Opened file '261'.
[preprocess][L2] Starting preprocessing for '261'.
[preprocess][L3] 261: Tokenized into 162 tokens.
[preprocess][L3] 261: Removed stop words; 101 tokens remain.
[preprocess][L3] 261: Stemming complete.
[indexDocument][L3] Preprocessing complete for '261'; 101 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '261'.
[indexDocument][L3] Recorded doc length 101 for '261'.
[indexDocument][L3] Tokens added to the inverted index for '261'.
[indexDocument][L2] Indexing document '605'.
[indexDocument][L3] Opened file '605'.
[preprocess][L2] Starting preprocessing for '605'.
[preprocess][L3] 605: Tokenized into 163 tokens.
[preprocess][L3] 605: Removed stop words; 95 tokens remain.
[preprocess][L3] 605: Stemming complete.
[indexDocument][L3] Preprocessing complete for '605'; 95 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '605'.
[indexDocument][L3] Recorded doc length 95 for '605'.
[indexDocument][L3] Tokens added to the inverted index for '605'.
[indexDocument][L2] Indexing document '437'.
[indexDocument][L3] Opened file '437'.
[preprocess][L2] Starting preprocessing for '437'.
[preprocess][L3] 437: Tokenized into 74 tokens.
[preprocess][L3] 437: Removed stop words; 54 tokens remain.
[preprocess][L3] 437: Stemming complete.
[indexDocument][L3] Preprocessing complete for '437'; 54 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '437'.
[indexDocument][L3] Recorded doc length 54 for '437'.
[indexDocument][L3] Tokens added to the inverted index for '437'.
[indexDocument][L2] Indexing document '1378'.
[indexDocument][L3] Opened file '1378'.
[preprocess][L2] Starting preprocessing for '1378'.
[preprocess][L3] 1378: Tokenized into 109 tokens.
[preprocess][L3] 1378: Removed stop words; 80 tokens remain.
[preprocess][L3] 1378: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1378'; 80 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1378'.
[indexDocument][L3] Recorded doc length 80 for '1378'.
[indexDocument][L3] Tokens added to the inverted index for '1378'.
[indexDocument][L2] Indexing document '98'.
[indexDocument][L3] Opened file '98'.
[preprocess][L2] Starting preprocessing for '98'.
[preprocess][L3] 98: Tokenized into 82 tokens.
[preprocess][L3] 98: Removed stop words; 52 tokens remain.
[preprocess][L3] 98: Stemming complete.
[indexDocument][L3] Preprocessing complete for '98'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '98'.
[indexDocument][L3] Recorded doc length 52 for '98'.
[indexDocument][L3] Tokens added to the inverted index for '98'.
[indexDocument][L2] Indexing document '408'.
[indexDocument][L3] Opened file '408'.
[preprocess][L2] Starting preprocessing for '408'.
[preprocess][L3] 408: Tokenized into 78 tokens.
[preprocess][L3] 408: Removed stop words; 46 tokens remain.
[preprocess][L3] 408: Stemming complete.
[indexDocument][L3] Preprocessing complete for '408'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '408'.
[indexDocument][L3] Recorded doc length 46 for '408'.
[indexDocument][L3] Tokens added to the inverted index for '408'.
[indexDocument][L2] Indexing document '850'.
[indexDocument][L3] Opened file '850'.
[preprocess][L2] Starting preprocessing for '850'.
[preprocess][L3] 850: Tokenized into 81 tokens.
[preprocess][L3] 850: Removed stop words; 53 tokens remain.
[preprocess][L3] 850: Stemming complete.
[indexDocument][L3] Preprocessing complete for '850'; 53 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '850'.
[indexDocument][L3] Recorded doc length 53 for '850'.
[indexDocument][L3] Tokens added to the inverted index for '850'.
[indexDocument][L2] Indexing document '1143'.
[indexDocument][L3] Opened file '1143'.
[preprocess][L2] Starting preprocessing for '1143'.
[preprocess][L3] 1143: Tokenized into 133 tokens.
[preprocess][L3] 1143: Removed stop words; 85 tokens remain.
[preprocess][L3] 1143: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1143'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1143'.
[indexDocument][L3] Recorded doc length 85 for '1143'.
[indexDocument][L3] Tokens added to the inverted index for '1143'.
[indexDocument][L2] Indexing document '91'.
[indexDocument][L3] Opened file '91'.
[preprocess][L2] Starting preprocessing for '91'.
[preprocess][L3] 91: Tokenized into 162 tokens.
[preprocess][L3] 91: Removed stop words; 96 tokens remain.
[preprocess][L3] 91: Stemming complete.
[indexDocument][L3] Preprocessing complete for '91'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '91'.
[indexDocument][L3] Recorded doc length 96 for '91'.
[indexDocument][L3] Tokens added to the inverted index for '91'.
[indexDocument][L2] Indexing document '1371'.
[indexDocument][L3] Opened file '1371'.
[preprocess][L2] Starting preprocessing for '1371'.
[preprocess][L3] 1371: Tokenized into 181 tokens.
[preprocess][L3] 1371: Removed stop words; 120 tokens remain.
[preprocess][L3] 1371: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1371'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1371'.
[indexDocument][L3] Recorded doc length 120 for '1371'.
[indexDocument][L3] Tokens added to the inverted index for '1371'.
[indexDocument][L2] Indexing document '633'.
[indexDocument][L3] Opened file '633'.
[preprocess][L2] Starting preprocessing for '633'.
[preprocess][L3] 633: Tokenized into 103 tokens.
[preprocess][L3] 633: Removed stop words; 61 tokens remain.
[preprocess][L3] 633: Stemming complete.
[indexDocument][L3] Preprocessing complete for '633'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '633'.
[indexDocument][L3] Recorded doc length 61 for '633'.
[indexDocument][L3] Tokens added to the inverted index for '633'.
[indexDocument][L2] Indexing document '401'.
[indexDocument][L3] Opened file '401'.
[preprocess][L2] Starting preprocessing for '401'.
[preprocess][L3] 401: Tokenized into 328 tokens.
[preprocess][L3] 401: Removed stop words; 203 tokens remain.
[preprocess][L3] 401: Stemming complete.
[indexDocument][L3] Preprocessing complete for '401'; 203 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '401'.
[indexDocument][L3] Recorded doc length 203 for '401'.
[indexDocument][L3] Tokens added to the inverted index for '401'.
[indexDocument][L2] Indexing document '859'.
[indexDocument][L3] Opened file '859'.
[preprocess][L2] Starting preprocessing for '859'.
[preprocess][L3] 859: Tokenized into 249 tokens.
[preprocess][L3] 859: Removed stop words; 154 tokens remain.
[preprocess][L3] 859: Stemming complete.
[indexDocument][L3] Preprocessing complete for '859'; 154 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '859'.
[indexDocument][L3] Recorded doc length 154 for '859'.
[indexDocument][L3] Tokens added to the inverted index for '859'.
[indexDocument][L2] Indexing document '65'.
[indexDocument][L3] Opened file '65'.
[preprocess][L2] Starting preprocessing for '65'.
[preprocess][L3] 65: Tokenized into 88 tokens.
[preprocess][L3] 65: Removed stop words; 57 tokens remain.
[preprocess][L3] 65: Stemming complete.
[indexDocument][L3] Preprocessing complete for '65'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '65'.
[indexDocument][L3] Recorded doc length 57 for '65'.
[indexDocument][L3] Tokens added to the inverted index for '65'.
[indexDocument][L2] Indexing document '1385'.
[indexDocument][L3] Opened file '1385'.
[preprocess][L2] Starting preprocessing for '1385'.
[preprocess][L3] 1385: Tokenized into 182 tokens.
[preprocess][L3] 1385: Removed stop words; 104 tokens remain.
[preprocess][L3] 1385: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1385'; 104 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1385'.
[indexDocument][L3] Recorded doc length 104 for '1385'.
[indexDocument][L3] Tokens added to the inverted index for '1385'.
[indexDocument][L2] Indexing document '257'.
[indexDocument][L3] Opened file '257'.
[preprocess][L2] Starting preprocessing for '257'.
[preprocess][L3] 257: Tokenized into 225 tokens.
[preprocess][L3] 257: Removed stop words; 123 tokens remain.
[preprocess][L3] 257: Stemming complete.
[indexDocument][L3] Preprocessing complete for '257'; 123 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '257'.
[indexDocument][L3] Recorded doc length 123 for '257'.
[indexDocument][L3] Tokens added to the inverted index for '257'.
[indexDocument][L2] Indexing document '892'.
[indexDocument][L3] Opened file '892'.
[preprocess][L2] Starting preprocessing for '892'.
[preprocess][L3] 892: Tokenized into 89 tokens.
[preprocess][L3] 892: Removed stop words; 55 tokens remain.
[preprocess][L3] 892: Stemming complete.
[indexDocument][L3] Preprocessing complete for '892'; 55 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '892'.
[indexDocument][L3] Recorded doc length 55 for '892'.
[indexDocument][L3] Tokens added to the inverted index for '892'.
[indexDocument][L2] Indexing document '268'.
[indexDocument][L3] Opened file '268'.
[preprocess][L2] Starting preprocessing for '268'.
[preprocess][L3] 268: Tokenized into 123 tokens.
[preprocess][L3] 268: Removed stop words; 82 tokens remain.
[preprocess][L3] 268: Stemming complete.
[indexDocument][L3] Preprocessing complete for '268'; 82 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '268'.
[indexDocument][L3] Recorded doc length 82 for '268'.
[indexDocument][L3] Tokens added to the inverted index for '268'.
[indexDocument][L2] Indexing document '866'.
[indexDocument][L3] Opened file '866'.
[preprocess][L2] Starting preprocessing for '866'.
[preprocess][L3] 866: Tokenized into 115 tokens.
[preprocess][L3] 866: Removed stop words; 71 tokens remain.
[preprocess][L3] 866: Stemming complete.
[indexDocument][L3] Preprocessing complete for '866'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '866'.
[indexDocument][L3] Recorded doc length 71 for '866'.
[indexDocument][L3] Tokens added to the inverted index for '866'.
[indexDocument][L2] Indexing document '1188'.
[indexDocument][L3] Opened file '1188'.
[preprocess][L2] Starting preprocessing for '1188'.
[preprocess][L3] 1188: Tokenized into 173 tokens.
[preprocess][L3] 1188: Removed stop words; 115 tokens remain.
[preprocess][L3] 1188: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1188'; 115 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1188'.
[indexDocument][L3] Recorded doc length 115 for '1188'.
[indexDocument][L3] Tokens added to the inverted index for '1188'.
[indexDocument][L2] Indexing document '1382'.
[indexDocument][L3] Opened file '1382'.
[preprocess][L2] Starting preprocessing for '1382'.
[preprocess][L3] 1382: Tokenized into 320 tokens.
[preprocess][L3] 1382: Removed stop words; 171 tokens remain.
[preprocess][L3] 1382: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1382'; 171 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1382'.
[indexDocument][L3] Recorded doc length 171 for '1382'.
[indexDocument][L3] Tokens added to the inverted index for '1382'.
[indexDocument][L2] Indexing document '250'.
[indexDocument][L3] Opened file '250'.
[preprocess][L2] Starting preprocessing for '250'.
[preprocess][L3] 250: Tokenized into 61 tokens.
[preprocess][L3] 250: Removed stop words; 39 tokens remain.
[preprocess][L3] 250: Stemming complete.
[indexDocument][L3] Preprocessing complete for '250'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '250'.
[indexDocument][L3] Recorded doc length 39 for '250'.
[indexDocument][L3] Tokens added to the inverted index for '250'.
[indexDocument][L2] Indexing document '62'.
[indexDocument][L3] Opened file '62'.
[preprocess][L2] Starting preprocessing for '62'.
[preprocess][L3] 62: Tokenized into 294 tokens.
[preprocess][L3] 62: Removed stop words; 182 tokens remain.
[preprocess][L3] 62: Stemming complete.
[indexDocument][L3] Preprocessing complete for '62'; 182 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '62'.
[indexDocument][L3] Recorded doc length 182 for '62'.
[indexDocument][L3] Tokens added to the inverted index for '62'.
[indexDocument][L2] Indexing document '406'.
[indexDocument][L3] Opened file '406'.
[preprocess][L2] Starting preprocessing for '406'.
[preprocess][L3] 406: Tokenized into 224 tokens.
[preprocess][L3] 406: Removed stop words; 126 tokens remain.
[preprocess][L3] 406: Stemming complete.
[indexDocument][L3] Preprocessing complete for '406'; 126 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '406'.
[indexDocument][L3] Recorded doc length 126 for '406'.
[indexDocument][L3] Tokens added to the inverted index for '406'.
[indexDocument][L2] Indexing document '634'.
[indexDocument][L3] Opened file '634'.
[preprocess][L2] Starting preprocessing for '634'.
[preprocess][L3] 634: Tokenized into 138 tokens.
[preprocess][L3] 634: Removed stop words; 89 tokens remain.
[preprocess][L3] 634: Stemming complete.
[indexDocument][L3] Preprocessing complete for '634'; 89 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '634'.
[indexDocument][L3] Recorded doc length 89 for '634'.
[indexDocument][L3] Tokens added to the inverted index for '634'.
[indexDocument][L2] Indexing document '1376'.
[indexDocument][L3] Opened file '1376'.
[preprocess][L2] Starting preprocessing for '1376'.
[preprocess][L3] 1376: Tokenized into 87 tokens.
[preprocess][L3] 1376: Removed stop words; 52 tokens remain.
[preprocess][L3] 1376: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1376'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1376'.
[indexDocument][L3] Recorded doc length 52 for '1376'.
[indexDocument][L3] Tokens added to the inverted index for '1376'.
[indexDocument][L2] Indexing document '1144'.
[indexDocument][L3] Opened file '1144'.
[preprocess][L2] Starting preprocessing for '1144'.
[preprocess][L3] 1144: Tokenized into 321 tokens.
[preprocess][L3] 1144: Removed stop words; 177 tokens remain.
[preprocess][L3] 1144: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1144'; 177 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1144'.
[indexDocument][L3] Recorded doc length 177 for '1144'.
[indexDocument][L3] Tokens added to the inverted index for '1144'.
[indexDocument][L2] Indexing document '96'.
[indexDocument][L3] Opened file '96'.
[preprocess][L2] Starting preprocessing for '96'.
[preprocess][L3] 96: Tokenized into 270 tokens.
[preprocess][L3] 96: Removed stop words; 145 tokens remain.
[preprocess][L3] 96: Stemming complete.
[indexDocument][L3] Preprocessing complete for '96'; 145 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '96'.
[indexDocument][L3] Recorded doc length 145 for '96'.
[indexDocument][L3] Tokens added to the inverted index for '96'.
[indexDocument][L2] Indexing document '439'.
[indexDocument][L3] Opened file '439'.
[preprocess][L2] Starting preprocessing for '439'.
[preprocess][L3] 439: Tokenized into 188 tokens.
[preprocess][L3] 439: Removed stop words; 111 tokens remain.
[preprocess][L3] 439: Stemming complete.
[indexDocument][L3] Preprocessing complete for '439'; 111 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '439'.
[indexDocument][L3] Recorded doc length 111 for '439'.
[indexDocument][L3] Tokens added to the inverted index for '439'.
[indexDocument][L2] Indexing document '861'.
[indexDocument][L3] Opened file '861'.
[preprocess][L2] Starting preprocessing for '861'.
[preprocess][L3] 861: Tokenized into 119 tokens.
[preprocess][L3] 861: Removed stop words; 84 tokens remain.
[preprocess][L3] 861: Stemming complete.
[indexDocument][L3] Preprocessing complete for '861'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '861'.
[indexDocument][L3] Recorded doc length 84 for '861'.
[indexDocument][L3] Tokens added to the inverted index for '861'.
[indexDocument][L2] Indexing document '895'.
[indexDocument][L3] Opened file '895'.
[preprocess][L2] Starting preprocessing for '895'.
[preprocess][L3] 895: Tokenized into 114 tokens.
[preprocess][L3] 895: Removed stop words; 68 tokens remain.
[preprocess][L3] 895: Stemming complete.
[indexDocument][L3] Preprocessing complete for '895'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '895'.
[indexDocument][L3] Recorded doc length 68 for '895'.
[indexDocument][L3] Tokens added to the inverted index for '895'.
[indexDocument][L2] Indexing document '1349'.
[indexDocument][L3] Opened file '1349'.
[preprocess][L2] Starting preprocessing for '1349'.
[preprocess][L3] 1349: Tokenized into 199 tokens.
[preprocess][L3] 1349: Removed stop words; 115 tokens remain.
[preprocess][L3] 1349: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1349'; 115 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1349'.
[indexDocument][L3] Recorded doc length 115 for '1349'.
[indexDocument][L3] Tokens added to the inverted index for '1349'.
[indexDocument][L2] Indexing document '1009'.
[indexDocument][L3] Opened file '1009'.
[preprocess][L2] Starting preprocessing for '1009'.
[preprocess][L3] 1009: Tokenized into 161 tokens.
[preprocess][L3] 1009: Removed stop words; 94 tokens remain.
[preprocess][L3] 1009: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1009'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1009'.
[indexDocument][L3] Recorded doc length 94 for '1009'.
[indexDocument][L3] Tokens added to the inverted index for '1009'.
[indexDocument][L2] Indexing document '913'.
[indexDocument][L3] Opened file '913'.
[preprocess][L2] Starting preprocessing for '913'.
[preprocess][L3] 913: Tokenized into 118 tokens.
[preprocess][L3] 913: Removed stop words; 75 tokens remain.
[preprocess][L3] 913: Stemming complete.
[indexDocument][L3] Preprocessing complete for '913'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '913'.
[indexDocument][L3] Recorded doc length 75 for '913'.
[indexDocument][L3] Tokens added to the inverted index for '913'.
[indexDocument][L2] Indexing document '779'.
[indexDocument][L3] Opened file '779'.
[preprocess][L2] Starting preprocessing for '779'.
[preprocess][L3] 779: Tokenized into 202 tokens.
[preprocess][L3] 779: Removed stop words; 120 tokens remain.
[preprocess][L3] 779: Stemming complete.
[indexDocument][L3] Preprocessing complete for '779'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '779'.
[indexDocument][L3] Recorded doc length 120 for '779'.
[indexDocument][L3] Tokens added to the inverted index for '779'.
[indexDocument][L2] Indexing document '1204'.
[indexDocument][L3] Opened file '1204'.
[preprocess][L2] Starting preprocessing for '1204'.
[preprocess][L3] 1204: Tokenized into 312 tokens.
[preprocess][L3] 1204: Removed stop words; 200 tokens remain.
[preprocess][L3] 1204: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1204'; 200 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1204'.
[indexDocument][L3] Recorded doc length 200 for '1204'.
[indexDocument][L3] Tokens added to the inverted index for '1204'.
[indexDocument][L2] Indexing document '1036'.
[indexDocument][L3] Opened file '1036'.
[preprocess][L2] Starting preprocessing for '1036'.
[preprocess][L3] 1036: Tokenized into 106 tokens.
[preprocess][L3] 1036: Removed stop words; 72 tokens remain.
[preprocess][L3] 1036: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1036'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1036'.
[indexDocument][L3] Recorded doc length 72 for '1036'.
[indexDocument][L3] Tokens added to the inverted index for '1036'.
[indexDocument][L2] Indexing document '580'.
[indexDocument][L3] Opened file '580'.
[preprocess][L2] Starting preprocessing for '580'.
[preprocess][L3] 580: Tokenized into 153 tokens.
[preprocess][L3] 580: Removed stop words; 93 tokens remain.
[preprocess][L3] 580: Stemming complete.
[indexDocument][L3] Preprocessing complete for '580'; 93 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '580'.
[indexDocument][L3] Recorded doc length 93 for '580'.
[indexDocument][L3] Tokens added to the inverted index for '580'.
[indexDocument][L2] Indexing document '574'.
[indexDocument][L3] Opened file '574'.
[preprocess][L2] Starting preprocessing for '574'.
[preprocess][L3] 574: Tokenized into 194 tokens.
[preprocess][L3] 574: Removed stop words; 125 tokens remain.
[preprocess][L3] 574: Stemming complete.
[indexDocument][L3] Preprocessing complete for '574'; 125 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '574'.
[indexDocument][L3] Recorded doc length 125 for '574'.
[indexDocument][L3] Tokens added to the inverted index for '574'.
[indexDocument][L2] Indexing document '746'.
[indexDocument][L3] Opened file '746'.
[preprocess][L2] Starting preprocessing for '746'.
[preprocess][L3] 746: Tokenized into 143 tokens.
[preprocess][L3] 746: Removed stop words; 81 tokens remain.
[preprocess][L3] 746: Stemming complete.
[indexDocument][L3] Preprocessing complete for '746'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '746'.
[indexDocument][L3] Recorded doc length 81 for '746'.
[indexDocument][L3] Tokens added to the inverted index for '746'.
[indexDocument][L2] Indexing document '322'.
[indexDocument][L3] Opened file '322'.
[preprocess][L2] Starting preprocessing for '322'.
[preprocess][L3] 322: Tokenized into 62 tokens.
[preprocess][L3] 322: Removed stop words; 38 tokens remain.
[preprocess][L3] 322: Stemming complete.
[indexDocument][L3] Preprocessing complete for '322'; 38 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '322'.
[indexDocument][L3] Recorded doc length 38 for '322'.
[indexDocument][L3] Tokens added to the inverted index for '322'.
[indexDocument][L2] Indexing document '110'.
[indexDocument][L3] Opened file '110'.
[preprocess][L2] Starting preprocessing for '110'.
[preprocess][L3] 110: Tokenized into 345 tokens.
[preprocess][L3] 110: Removed stop words; 208 tokens remain.
[preprocess][L3] 110: Stemming complete.
[indexDocument][L3] Preprocessing complete for '110'; 208 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '110'.
[indexDocument][L3] Recorded doc length 208 for '110'.
[indexDocument][L3] Tokens added to the inverted index for '110'.
[indexDocument][L2] Indexing document '128'.
[indexDocument][L3] Opened file '128'.
[preprocess][L2] Starting preprocessing for '128'.
[preprocess][L3] 128: Tokenized into 161 tokens.
[preprocess][L3] 128: Removed stop words; 104 tokens remain.
[preprocess][L3] 128: Stemming complete.
[indexDocument][L3] Preprocessing complete for '128'; 104 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '128'.
[indexDocument][L3] Recorded doc length 104 for '128'.
[indexDocument][L3] Tokens added to the inverted index for '128'.
[indexDocument][L2] Indexing document '914'.
[indexDocument][L3] Opened file '914'.
[preprocess][L2] Starting preprocessing for '914'.
[preprocess][L3] 914: Tokenized into 189 tokens.
[preprocess][L3] 914: Removed stop words; 111 tokens remain.
[preprocess][L3] 914: Stemming complete.
[indexDocument][L3] Preprocessing complete for '914'; 111 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '914'.
[indexDocument][L3] Recorded doc length 111 for '914'.
[indexDocument][L3] Tokens added to the inverted index for '914'.
[indexDocument][L2] Indexing document '117'.
[indexDocument][L3] Opened file '117'.
[preprocess][L2] Starting preprocessing for '117'.
[preprocess][L3] 117: Tokenized into 141 tokens.
[preprocess][L3] 117: Removed stop words; 83 tokens remain.
[preprocess][L3] 117: Stemming complete.
[indexDocument][L3] Preprocessing complete for '117'; 83 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '117'.
[indexDocument][L3] Recorded doc length 83 for '117'.
[indexDocument][L3] Tokens added to the inverted index for '117'.
[indexDocument][L2] Indexing document '325'.
[indexDocument][L3] Opened file '325'.
[preprocess][L2] Starting preprocessing for '325'.
[preprocess][L3] 325: Tokenized into 249 tokens.
[preprocess][L3] 325: Removed stop words; 144 tokens remain.
[preprocess][L3] 325: Stemming complete.
[indexDocument][L3] Preprocessing complete for '325'; 144 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '325'.
[indexDocument][L3] Recorded doc length 144 for '325'.
[indexDocument][L3] Tokens added to the inverted index for '325'.
[indexDocument][L2] Indexing document '741'.
[indexDocument][L3] Opened file '741'.
[preprocess][L2] Starting preprocessing for '741'.
[preprocess][L3] 741: Tokenized into 100 tokens.
[preprocess][L3] 741: Removed stop words; 62 tokens remain.
[preprocess][L3] 741: Stemming complete.
[indexDocument][L3] Preprocessing complete for '741'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '741'.
[indexDocument][L3] Recorded doc length 62 for '741'.
[indexDocument][L3] Tokens added to the inverted index for '741'.
[indexDocument][L2] Indexing document '573'.
[indexDocument][L3] Opened file '573'.
[preprocess][L2] Starting preprocessing for '573'.
[preprocess][L3] 573: Tokenized into 161 tokens.
[preprocess][L3] 573: Removed stop words; 107 tokens remain.
[preprocess][L3] 573: Stemming complete.
[indexDocument][L3] Preprocessing complete for '573'; 107 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '573'.
[indexDocument][L3] Recorded doc length 107 for '573'.
[indexDocument][L3] Tokens added to the inverted index for '573'.
[indexDocument][L2] Indexing document '587'.
[indexDocument][L3] Opened file '587'.
[preprocess][L2] Starting preprocessing for '587'.
[preprocess][L3] 587: Tokenized into 128 tokens.
[preprocess][L3] 587: Removed stop words; 77 tokens remain.
[preprocess][L3] 587: Stemming complete.
[indexDocument][L3] Preprocessing complete for '587'; 77 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '587'.
[indexDocument][L3] Recorded doc length 77 for '587'.
[indexDocument][L3] Tokens added to the inverted index for '587'.
[indexDocument][L2] Indexing document '1031'.
[indexDocument][L3] Opened file '1031'.
[preprocess][L2] Starting preprocessing for '1031'.
[preprocess][L3] 1031: Tokenized into 111 tokens.
[preprocess][L3] 1031: Removed stop words; 64 tokens remain.
[preprocess][L3] 1031: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1031'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1031'.
[indexDocument][L3] Recorded doc length 64 for '1031'.
[indexDocument][L3] Tokens added to the inverted index for '1031'.
[indexDocument][L2] Indexing document '1203'.
[indexDocument][L3] Opened file '1203'.
[preprocess][L2] Starting preprocessing for '1203'.
[preprocess][L3] 1203: Tokenized into 214 tokens.
[preprocess][L3] 1203: Removed stop words; 124 tokens remain.
[preprocess][L3] 1203: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1203'; 124 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1203'.
[indexDocument][L3] Recorded doc length 124 for '1203'.
[indexDocument][L3] Tokens added to the inverted index for '1203'.
[indexDocument][L2] Indexing document '922'.
[indexDocument][L3] Opened file '922'.
[preprocess][L2] Starting preprocessing for '922'.
[preprocess][L3] 922: Tokenized into 124 tokens.
[preprocess][L3] 922: Removed stop words; 75 tokens remain.
[preprocess][L3] 922: Stemming complete.
[indexDocument][L3] Preprocessing complete for '922'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '922'.
[indexDocument][L3] Recorded doc length 75 for '922'.
[indexDocument][L3] Tokens added to the inverted index for '922'.
[indexDocument][L2] Indexing document '748'.
[indexDocument][L3] Opened file '748'.
[preprocess][L2] Starting preprocessing for '748'.
[preprocess][L3] 748: Tokenized into 196 tokens.
[preprocess][L3] 748: Removed stop words; 140 tokens remain.
[preprocess][L3] 748: Stemming complete.
[indexDocument][L3] Preprocessing complete for '748'; 140 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '748'.
[indexDocument][L3] Recorded doc length 140 for '748'.
[indexDocument][L3] Tokens added to the inverted index for '748'.
[indexDocument][L2] Indexing document '1038'.
[indexDocument][L3] Opened file '1038'.
[preprocess][L2] Starting preprocessing for '1038'.
[preprocess][L3] 1038: Tokenized into 95 tokens.
[preprocess][L3] 1038: Removed stop words; 57 tokens remain.
[preprocess][L3] 1038: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1038'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1038'.
[indexDocument][L3] Recorded doc length 57 for '1038'.
[indexDocument][L3] Tokens added to the inverted index for '1038'.
[indexDocument][L2] Indexing document '777'.
[indexDocument][L3] Opened file '777'.
[preprocess][L2] Starting preprocessing for '777'.
[preprocess][L3] 777: Tokenized into 165 tokens.
[preprocess][L3] 777: Removed stop words; 96 tokens remain.
[preprocess][L3] 777: Stemming complete.
[indexDocument][L3] Preprocessing complete for '777'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '777'.
[indexDocument][L3] Recorded doc length 96 for '777'.
[indexDocument][L3] Tokens added to the inverted index for '777'.
[indexDocument][L2] Indexing document '545'.
[indexDocument][L3] Opened file '545'.
[preprocess][L2] Starting preprocessing for '545'.
[preprocess][L3] 545: Tokenized into 97 tokens.
[preprocess][L3] 545: Removed stop words; 61 tokens remain.
[preprocess][L3] 545: Stemming complete.
[indexDocument][L3] Preprocessing complete for '545'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '545'.
[indexDocument][L3] Recorded doc length 61 for '545'.
[indexDocument][L3] Tokens added to the inverted index for '545'.
[indexDocument][L2] Indexing document '121'.
[indexDocument][L3] Opened file '121'.
[preprocess][L2] Starting preprocessing for '121'.
[preprocess][L3] 121: Tokenized into 141 tokens.
[preprocess][L3] 121: Removed stop words; 85 tokens remain.
[preprocess][L3] 121: Stemming complete.
[indexDocument][L3] Preprocessing complete for '121'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '121'.
[indexDocument][L3] Recorded doc length 85 for '121'.
[indexDocument][L3] Tokens added to the inverted index for '121'.
[indexDocument][L2] Indexing document '313'.
[indexDocument][L3] Opened file '313'.
[preprocess][L2] Starting preprocessing for '313'.
[preprocess][L3] 313: Tokenized into 75 tokens.
[preprocess][L3] 313: Removed stop words; 42 tokens remain.
[preprocess][L3] 313: Stemming complete.
[indexDocument][L3] Preprocessing complete for '313'; 42 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '313'.
[indexDocument][L3] Recorded doc length 42 for '313'.
[indexDocument][L3] Tokens added to the inverted index for '313'.
[indexDocument][L2] Indexing document '1007'.
[indexDocument][L3] Opened file '1007'.
[preprocess][L2] Starting preprocessing for '1007'.
[preprocess][L3] 1007: Tokenized into 171 tokens.
[preprocess][L3] 1007: Removed stop words; 97 tokens remain.
[preprocess][L3] 1007: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1007'; 97 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1007'.
[indexDocument][L3] Recorded doc length 97 for '1007'.
[indexDocument][L3] Tokens added to the inverted index for '1007'.
[indexDocument][L2] Indexing document '1235'.
[indexDocument][L3] Opened file '1235'.
[preprocess][L2] Starting preprocessing for '1235'.
[preprocess][L3] 1235: Tokenized into 296 tokens.
[preprocess][L3] 1235: Removed stop words; 168 tokens remain.
[preprocess][L3] 1235: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1235'; 168 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1235'.
[indexDocument][L3] Recorded doc length 168 for '1235'.
[indexDocument][L3] Tokens added to the inverted index for '1235'.
[indexDocument][L2] Indexing document '783'.
[indexDocument][L3] Opened file '783'.
[preprocess][L2] Starting preprocessing for '783'.
[preprocess][L3] 783: Tokenized into 141 tokens.
[preprocess][L3] 783: Removed stop words; 88 tokens remain.
[preprocess][L3] 783: Stemming complete.
[indexDocument][L3] Preprocessing complete for '783'; 88 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '783'.
[indexDocument][L3] Recorded doc length 88 for '783'.
[indexDocument][L3] Tokens added to the inverted index for '783'.
[indexDocument][L2] Indexing document '589'.
[indexDocument][L3] Opened file '589'.
[preprocess][L2] Starting preprocessing for '589'.
[preprocess][L3] 589: Tokenized into 228 tokens.
[preprocess][L3] 589: Removed stop words; 138 tokens remain.
[preprocess][L3] 589: Stemming complete.
[indexDocument][L3] Preprocessing complete for '589'; 138 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '589'.
[indexDocument][L3] Recorded doc length 138 for '589'.
[indexDocument][L3] Tokens added to the inverted index for '589'.
[indexDocument][L2] Indexing document '119'.
[indexDocument][L3] Opened file '119'.
[preprocess][L2] Starting preprocessing for '119'.
[preprocess][L3] 119: Tokenized into 81 tokens.
[preprocess][L3] 119: Removed stop words; 52 tokens remain.
[preprocess][L3] 119: Stemming complete.
[indexDocument][L3] Preprocessing complete for '119'; 52 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '119'.
[indexDocument][L3] Recorded doc length 52 for '119'.
[indexDocument][L3] Tokens added to the inverted index for '119'.
[indexDocument][L2] Indexing document '925'.
[indexDocument][L3] Opened file '925'.
[preprocess][L2] Starting preprocessing for '925'.
[preprocess][L3] 925: Tokenized into 67 tokens.
[preprocess][L3] 925: Removed stop words; 44 tokens remain.
[preprocess][L3] 925: Stemming complete.
[indexDocument][L3] Preprocessing complete for '925'; 44 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '925'.
[indexDocument][L3] Recorded doc length 44 for '925'.
[indexDocument][L3] Tokens added to the inverted index for '925'.
[indexDocument][L2] Indexing document '784'.
[indexDocument][L3] Opened file '784'.
[preprocess][L2] Starting preprocessing for '784'.
[preprocess][L3] 784: Tokenized into 199 tokens.
[preprocess][L3] 784: Removed stop words; 120 tokens remain.
[preprocess][L3] 784: Stemming complete.
[indexDocument][L3] Preprocessing complete for '784'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '784'.
[indexDocument][L3] Recorded doc length 120 for '784'.
[indexDocument][L3] Tokens added to the inverted index for '784'.
[indexDocument][L2] Indexing document '1232'.
[indexDocument][L3] Opened file '1232'.
[preprocess][L2] Starting preprocessing for '1232'.
[preprocess][L3] 1232: Tokenized into 113 tokens.
[preprocess][L3] 1232: Removed stop words; 71 tokens remain.
[preprocess][L3] 1232: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1232'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1232'.
[indexDocument][L3] Recorded doc length 71 for '1232'.
[indexDocument][L3] Tokens added to the inverted index for '1232'.
[indexDocument][L2] Indexing document '1000'.
[indexDocument][L3] Opened file '1000'.
[preprocess][L2] Starting preprocessing for '1000'.
[preprocess][L3] 1000: Tokenized into 206 tokens.
[preprocess][L3] 1000: Removed stop words; 126 tokens remain.
[preprocess][L3] 1000: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1000'; 126 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1000'.
[indexDocument][L3] Recorded doc length 126 for '1000'.
[indexDocument][L3] Tokens added to the inverted index for '1000'.
[indexDocument][L2] Indexing document '314'.
[indexDocument][L3] Opened file '314'.
[preprocess][L2] Starting preprocessing for '314'.
[preprocess][L3] 314: Tokenized into 187 tokens.
[preprocess][L3] 314: Removed stop words; 112 tokens remain.
[preprocess][L3] 314: Stemming complete.
[indexDocument][L3] Preprocessing complete for '314'; 112 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '314'.
[indexDocument][L3] Recorded doc length 112 for '314'.
[indexDocument][L3] Tokens added to the inverted index for '314'.
[indexDocument][L2] Indexing document '126'.
[indexDocument][L3] Opened file '126'.
[preprocess][L2] Starting preprocessing for '126'.
[preprocess][L3] 126: Tokenized into 129 tokens.
[preprocess][L3] 126: Removed stop words; 83 tokens remain.
[preprocess][L3] 126: Stemming complete.
[indexDocument][L3] Preprocessing complete for '126'; 83 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '126'.
[indexDocument][L3] Recorded doc length 83 for '126'.
[indexDocument][L3] Tokens added to the inverted index for '126'.
[indexDocument][L2] Indexing document '542'.
[indexDocument][L3] Opened file '542'.
[preprocess][L2] Starting preprocessing for '542'.
[preprocess][L3] 542: Tokenized into 205 tokens.
[preprocess][L3] 542: Removed stop words; 126 tokens remain.
[preprocess][L3] 542: Stemming complete.
[indexDocument][L3] Preprocessing complete for '542'; 126 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '542'.
[indexDocument][L3] Recorded doc length 126 for '542'.
[indexDocument][L3] Tokens added to the inverted index for '542'.
[indexDocument][L2] Indexing document '770'.
[indexDocument][L3] Opened file '770'.
[preprocess][L2] Starting preprocessing for '770'.
[preprocess][L3] 770: Tokenized into 141 tokens.
[preprocess][L3] 770: Removed stop words; 73 tokens remain.
[preprocess][L3] 770: Stemming complete.
[indexDocument][L3] Preprocessing complete for '770'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '770'.
[indexDocument][L3] Recorded doc length 73 for '770'.
[indexDocument][L3] Tokens added to the inverted index for '770'.
[indexDocument][L2] Indexing document '186'.
[indexDocument][L3] Opened file '186'.
[preprocess][L2] Starting preprocessing for '186'.
[preprocess][L3] 186: Tokenized into 194 tokens.
[preprocess][L3] 186: Removed stop words; 111 tokens remain.
[preprocess][L3] 186: Stemming complete.
[indexDocument][L3] Preprocessing complete for '186'; 111 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '186'.
[indexDocument][L3] Recorded doc length 111 for '186'.
[indexDocument][L3] Tokens added to the inverted index for '186'.
[indexDocument][L2] Indexing document '1054'.
[indexDocument][L3] Opened file '1054'.
[preprocess][L2] Starting preprocessing for '1054'.
[preprocess][L3] 1054: Tokenized into 118 tokens.
[preprocess][L3] 1054: Removed stop words; 75 tokens remain.
[preprocess][L3] 1054: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1054'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1054'.
[indexDocument][L3] Recorded doc length 75 for '1054'.
[indexDocument][L3] Tokens added to the inverted index for '1054'.
[indexDocument][L2] Indexing document '1266'.
[indexDocument][L3] Opened file '1266'.
[preprocess][L2] Starting preprocessing for '1266'.
[preprocess][L3] 1266: Tokenized into 59 tokens.
[preprocess][L3] 1266: Removed stop words; 45 tokens remain.
[preprocess][L3] 1266: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1266'; 45 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1266'.
[indexDocument][L3] Recorded doc length 45 for '1266'.
[indexDocument][L3] Tokens added to the inverted index for '1266'.
[indexDocument][L2] Indexing document '172'.
[indexDocument][L3] Opened file '172'.
[preprocess][L2] Starting preprocessing for '172'.
[preprocess][L3] 172: Tokenized into 227 tokens.
[preprocess][L3] 172: Removed stop words; 139 tokens remain.
[preprocess][L3] 172: Stemming complete.
[indexDocument][L3] Preprocessing complete for '172'; 139 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '172'.
[indexDocument][L3] Recorded doc length 139 for '172'.
[indexDocument][L3] Tokens added to the inverted index for '172'.
[indexDocument][L2] Indexing document '340'.
[indexDocument][L3] Opened file '340'.
[preprocess][L2] Starting preprocessing for '340'.
[preprocess][L3] 340: Tokenized into 76 tokens.
[preprocess][L3] 340: Removed stop words; 48 tokens remain.
[preprocess][L3] 340: Stemming complete.
[indexDocument][L3] Preprocessing complete for '340'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '340'.
[indexDocument][L3] Recorded doc length 48 for '340'.
[indexDocument][L3] Tokens added to the inverted index for '340'.
[indexDocument][L2] Indexing document '1292'.
[indexDocument][L3] Opened file '1292'.
[preprocess][L2] Starting preprocessing for '1292'.
[preprocess][L3] 1292: Tokenized into 276 tokens.
[preprocess][L3] 1292: Removed stop words; 170 tokens remain.
[preprocess][L3] 1292: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1292'; 170 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1292'.
[indexDocument][L3] Recorded doc length 170 for '1292'.
[indexDocument][L3] Tokens added to the inverted index for '1292'.
[indexDocument][L2] Indexing document '724'.
[indexDocument][L3] Opened file '724'.
[preprocess][L2] Starting preprocessing for '724'.
[preprocess][L3] 724: Tokenized into 148 tokens.
[preprocess][L3] 724: Removed stop words; 91 tokens remain.
[preprocess][L3] 724: Stemming complete.
[indexDocument][L3] Preprocessing complete for '724'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '724'.
[indexDocument][L3] Recorded doc length 91 for '724'.
[indexDocument][L3] Tokens added to the inverted index for '724'.
[indexDocument][L2] Indexing document '516'.
[indexDocument][L3] Opened file '516'.
[preprocess][L2] Starting preprocessing for '516'.
[preprocess][L3] 516: Tokenized into 102 tokens.
[preprocess][L3] 516: Removed stop words; 69 tokens remain.
[preprocess][L3] 516: Stemming complete.
[indexDocument][L3] Preprocessing complete for '516'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '516'.
[indexDocument][L3] Recorded doc length 69 for '516'.
[indexDocument][L3] Tokens added to the inverted index for '516'.
[indexDocument][L2] Indexing document '1259'.
[indexDocument][L3] Opened file '1259'.
[preprocess][L2] Starting preprocessing for '1259'.
[preprocess][L3] 1259: Tokenized into 162 tokens.
[preprocess][L3] 1259: Removed stop words; 103 tokens remain.
[preprocess][L3] 1259: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1259'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1259'.
[indexDocument][L3] Recorded doc length 103 for '1259'.
[indexDocument][L3] Tokens added to the inverted index for '1259'.
[indexDocument][L2] Indexing document '985'.
[indexDocument][L3] Opened file '985'.
[preprocess][L2] Starting preprocessing for '985'.
[preprocess][L3] 985: Tokenized into 145 tokens.
[preprocess][L3] 985: Removed stop words; 100 tokens remain.
[preprocess][L3] 985: Stemming complete.
[indexDocument][L3] Preprocessing complete for '985'; 100 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '985'.
[indexDocument][L3] Recorded doc length 100 for '985'.
[indexDocument][L3] Tokens added to the inverted index for '985'.
[indexDocument][L2] Indexing document '529'.
[indexDocument][L3] Opened file '529'.
[preprocess][L2] Starting preprocessing for '529'.
[preprocess][L3] 529: Tokenized into 288 tokens.
[preprocess][L3] 529: Removed stop words; 186 tokens remain.
[preprocess][L3] 529: Stemming complete.
[indexDocument][L3] Preprocessing complete for '529'; 186 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '529'.
[indexDocument][L3] Recorded doc length 186 for '529'.
[indexDocument][L3] Tokens added to the inverted index for '529'.
[indexDocument][L2] Indexing document '971'.
[indexDocument][L3] Opened file '971'.
[preprocess][L2] Starting preprocessing for '971'.
[preprocess][L3] 971: Tokenized into 154 tokens.
[preprocess][L3] 971: Removed stop words; 102 tokens remain.
[preprocess][L3] 971: Stemming complete.
[indexDocument][L3] Preprocessing complete for '971'; 102 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '971'.
[indexDocument][L3] Recorded doc length 102 for '971'.
[indexDocument][L3] Tokens added to the inverted index for '971'.
[indexDocument][L2] Indexing document '511'.
[indexDocument][L3] Opened file '511'.
[preprocess][L2] Starting preprocessing for '511'.
[preprocess][L3] 511: Tokenized into 165 tokens.
[preprocess][L3] 511: Removed stop words; 91 tokens remain.
[preprocess][L3] 511: Stemming complete.
[indexDocument][L3] Preprocessing complete for '511'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '511'.
[indexDocument][L3] Recorded doc length 91 for '511'.
[indexDocument][L3] Tokens added to the inverted index for '511'.
[indexDocument][L2] Indexing document '723'.
[indexDocument][L3] Opened file '723'.
[preprocess][L2] Starting preprocessing for '723'.
[preprocess][L3] 723: Tokenized into 137 tokens.
[preprocess][L3] 723: Removed stop words; 79 tokens remain.
[preprocess][L3] 723: Stemming complete.
[indexDocument][L3] Preprocessing complete for '723'; 79 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '723'.
[indexDocument][L3] Recorded doc length 79 for '723'.
[indexDocument][L3] Tokens added to the inverted index for '723'.
[indexDocument][L2] Indexing document '347'.
[indexDocument][L3] Opened file '347'.
[preprocess][L2] Starting preprocessing for '347'.
[preprocess][L3] 347: Tokenized into 131 tokens.
[preprocess][L3] 347: Removed stop words; 84 tokens remain.
[preprocess][L3] 347: Stemming complete.
[indexDocument][L3] Preprocessing complete for '347'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '347'.
[indexDocument][L3] Recorded doc length 84 for '347'.
[indexDocument][L3] Tokens added to the inverted index for '347'.
[indexDocument][L2] Indexing document '1295'.
[indexDocument][L3] Opened file '1295'.
[preprocess][L2] Starting preprocessing for '1295'.
[preprocess][L3] 1295: Tokenized into 159 tokens.
[preprocess][L3] 1295: Removed stop words; 95 tokens remain.
[preprocess][L3] 1295: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1295'; 95 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1295'.
[indexDocument][L3] Recorded doc length 95 for '1295'.
[indexDocument][L3] Tokens added to the inverted index for '1295'.
[indexDocument][L2] Indexing document '949'.
[indexDocument][L3] Opened file '949'.
[preprocess][L2] Starting preprocessing for '949'.
[preprocess][L3] 949: Tokenized into 125 tokens.
[preprocess][L3] 949: Removed stop words; 86 tokens remain.
[preprocess][L3] 949: Stemming complete.
[indexDocument][L3] Preprocessing complete for '949'; 86 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '949'.
[indexDocument][L3] Recorded doc length 86 for '949'.
[indexDocument][L3] Tokens added to the inverted index for '949'.
[indexDocument][L2] Indexing document '175'.
[indexDocument][L3] Opened file '175'.
[preprocess][L2] Starting preprocessing for '175'.
[preprocess][L3] 175: Tokenized into 84 tokens.
[preprocess][L3] 175: Removed stop words; 44 tokens remain.
[preprocess][L3] 175: Stemming complete.
[indexDocument][L3] Preprocessing complete for '175'; 44 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '175'.
[indexDocument][L3] Recorded doc length 44 for '175'.
[indexDocument][L3] Tokens added to the inverted index for '175'.
[indexDocument][L2] Indexing document '1261'.
[indexDocument][L3] Opened file '1261'.
[preprocess][L2] Starting preprocessing for '1261'.
[preprocess][L3] 1261: Tokenized into 264 tokens.
[preprocess][L3] 1261: Removed stop words; 164 tokens remain.
[preprocess][L3] 1261: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1261'; 164 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1261'.
[indexDocument][L3] Recorded doc length 164 for '1261'.
[indexDocument][L3] Tokens added to the inverted index for '1261'.
[indexDocument][L2] Indexing document '181'.
[indexDocument][L3] Opened file '181'.
[preprocess][L2] Starting preprocessing for '181'.
[preprocess][L3] 181: Tokenized into 57 tokens.
[preprocess][L3] 181: Removed stop words; 41 tokens remain.
[preprocess][L3] 181: Stemming complete.
[indexDocument][L3] Preprocessing complete for '181'; 41 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '181'.
[indexDocument][L3] Recorded doc length 41 for '181'.
[indexDocument][L3] Tokens added to the inverted index for '181'.
[indexDocument][L2] Indexing document '1053'.
[indexDocument][L3] Opened file '1053'.
[preprocess][L2] Starting preprocessing for '1053'.
[preprocess][L3] 1053: Tokenized into 183 tokens.
[preprocess][L3] 1053: Removed stop words; 106 tokens remain.
[preprocess][L3] 1053: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1053'; 106 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1053'.
[indexDocument][L3] Recorded doc length 106 for '1053'.
[indexDocument][L3] Tokens added to the inverted index for '1053'.
[indexDocument][L2] Indexing document '976'.
[indexDocument][L3] Opened file '976'.
[preprocess][L2] Starting preprocessing for '976'.
[preprocess][L3] 976: Tokenized into 417 tokens.
[preprocess][L3] 976: Removed stop words; 261 tokens remain.
[preprocess][L3] 976: Stemming complete.
[indexDocument][L3] Preprocessing complete for '976'; 261 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '976'.
[indexDocument][L3] Recorded doc length 261 for '976'.
[indexDocument][L3] Tokens added to the inverted index for '976'.
[indexDocument][L2] Indexing document '1098'.
[indexDocument][L3] Opened file '1098'.
[preprocess][L2] Starting preprocessing for '1098'.
[preprocess][L3] 1098: Tokenized into 170 tokens.
[preprocess][L3] 1098: Removed stop words; 103 tokens remain.
[preprocess][L3] 1098: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1098'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1098'.
[indexDocument][L3] Recorded doc length 103 for '1098'.
[indexDocument][L3] Tokens added to the inverted index for '1098'.
[indexDocument][L2] Indexing document '378'.
[indexDocument][L3] Opened file '378'.
[preprocess][L2] Starting preprocessing for '378'.
[preprocess][L3] 378: Tokenized into 86 tokens.
[preprocess][L3] 378: Removed stop words; 54 tokens remain.
[preprocess][L3] 378: Stemming complete.
[indexDocument][L3] Preprocessing complete for '378'; 54 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '378'.
[indexDocument][L3] Recorded doc length 54 for '378'.
[indexDocument][L3] Tokens added to the inverted index for '378'.
[indexDocument][L2] Indexing document '982'.
[indexDocument][L3] Opened file '982'.
[preprocess][L2] Starting preprocessing for '982'.
[preprocess][L3] 982: Tokenized into 173 tokens.
[preprocess][L3] 982: Removed stop words; 106 tokens remain.
[preprocess][L3] 982: Stemming complete.
[indexDocument][L3] Preprocessing complete for '982'; 106 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '982'.
[indexDocument][L3] Recorded doc length 106 for '982'.
[indexDocument][L3] Tokens added to the inverted index for '982'.
[indexDocument][L2] Indexing document '371'.
[indexDocument][L3] Opened file '371'.
[preprocess][L2] Starting preprocessing for '371'.
[preprocess][L3] 371: Tokenized into 159 tokens.
[preprocess][L3] 371: Removed stop words; 102 tokens remain.
[preprocess][L3] 371: Stemming complete.
[indexDocument][L3] Preprocessing complete for '371'; 102 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '371'.
[indexDocument][L3] Recorded doc length 102 for '371'.
[indexDocument][L3] Tokens added to the inverted index for '371'.
[indexDocument][L2] Indexing document '1091'.
[indexDocument][L3] Opened file '1091'.
[preprocess][L2] Starting preprocessing for '1091'.
[preprocess][L3] 1091: Tokenized into 125 tokens.
[preprocess][L3] 1091: Removed stop words; 81 tokens remain.
[preprocess][L3] 1091: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1091'; 81 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1091'.
[indexDocument][L3] Recorded doc length 81 for '1091'.
[indexDocument][L3] Tokens added to the inverted index for '1091'.
[indexDocument][L2] Indexing document '143'.
[indexDocument][L3] Opened file '143'.
[preprocess][L2] Starting preprocessing for '143'.
[preprocess][L3] 143: Tokenized into 66 tokens.
[preprocess][L3] 143: Removed stop words; 39 tokens remain.
[preprocess][L3] 143: Stemming complete.
[indexDocument][L3] Preprocessing complete for '143'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '143'.
[indexDocument][L3] Recorded doc length 39 for '143'.
[indexDocument][L3] Tokens added to the inverted index for '143'.
[indexDocument][L2] Indexing document '527'.
[indexDocument][L3] Opened file '527'.
[preprocess][L2] Starting preprocessing for '527'.
[preprocess][L3] 527: Tokenized into 102 tokens.
[preprocess][L3] 527: Removed stop words; 63 tokens remain.
[preprocess][L3] 527: Stemming complete.
[indexDocument][L3] Preprocessing complete for '527'; 63 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '527'.
[indexDocument][L3] Recorded doc length 63 for '527'.
[indexDocument][L3] Tokens added to the inverted index for '527'.
[indexDocument][L2] Indexing document '715'.
[indexDocument][L3] Opened file '715'.
[preprocess][L2] Starting preprocessing for '715'.
[preprocess][L3] 715: Tokenized into 122 tokens.
[preprocess][L3] 715: Removed stop words; 71 tokens remain.
[preprocess][L3] 715: Stemming complete.
[indexDocument][L3] Preprocessing complete for '715'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '715'.
[indexDocument][L3] Recorded doc length 71 for '715'.
[indexDocument][L3] Tokens added to the inverted index for '715'.
[indexDocument][L2] Indexing document '1257'.
[indexDocument][L3] Opened file '1257'.
[preprocess][L2] Starting preprocessing for '1257'.
[preprocess][L3] 1257: Tokenized into 152 tokens.
[preprocess][L3] 1257: Removed stop words; 85 tokens remain.
[preprocess][L3] 1257: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1257'; 85 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1257'.
[indexDocument][L3] Recorded doc length 85 for '1257'.
[indexDocument][L3] Tokens added to the inverted index for '1257'.
[indexDocument][L2] Indexing document '385'.
[indexDocument][L3] Opened file '385'.
[preprocess][L2] Starting preprocessing for '385'.
[preprocess][L3] 385: Tokenized into 64 tokens.
[preprocess][L3] 385: Removed stop words; 36 tokens remain.
[preprocess][L3] 385: Stemming complete.
[indexDocument][L3] Preprocessing complete for '385'; 36 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '385'.
[indexDocument][L3] Recorded doc length 36 for '385'.
[indexDocument][L3] Tokens added to the inverted index for '385'.
[indexDocument][L2] Indexing document '1065'.
[indexDocument][L3] Opened file '1065'.
[preprocess][L2] Starting preprocessing for '1065'.
[preprocess][L3] 1065: Tokenized into 147 tokens.
[preprocess][L3] 1065: Removed stop words; 94 tokens remain.
[preprocess][L3] 1065: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1065'; 94 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1065'.
[indexDocument][L3] Recorded doc length 94 for '1065'.
[indexDocument][L3] Tokens added to the inverted index for '1065'.
[indexDocument][L2] Indexing document '518'.
[indexDocument][L3] Opened file '518'.
[preprocess][L2] Starting preprocessing for '518'.
[preprocess][L3] 518: Tokenized into 200 tokens.
[preprocess][L3] 518: Removed stop words; 120 tokens remain.
[preprocess][L3] 518: Stemming complete.
[indexDocument][L3] Preprocessing complete for '518'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '518'.
[indexDocument][L3] Recorded doc length 120 for '518'.
[indexDocument][L3] Tokens added to the inverted index for '518'.
[indexDocument][L2] Indexing document '940'.
[indexDocument][L3] Opened file '940'.
[preprocess][L2] Starting preprocessing for '940'.
[preprocess][L3] 940: Tokenized into 104 tokens.
[preprocess][L3] 940: Removed stop words; 61 tokens remain.
[preprocess][L3] 940: Stemming complete.
[indexDocument][L3] Preprocessing complete for '940'; 61 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '940'.
[indexDocument][L3] Recorded doc length 61 for '940'.
[indexDocument][L3] Tokens added to the inverted index for '940'.
[indexDocument][L2] Indexing document '188'.
[indexDocument][L3] Opened file '188'.
[preprocess][L2] Starting preprocessing for '188'.
[preprocess][L3] 188: Tokenized into 233 tokens.
[preprocess][L3] 188: Removed stop words; 133 tokens remain.
[preprocess][L3] 188: Stemming complete.
[indexDocument][L3] Preprocessing complete for '188'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '188'.
[indexDocument][L3] Recorded doc length 133 for '188'.
[indexDocument][L3] Tokens added to the inverted index for '188'.
[indexDocument][L2] Indexing document '1268'.
[indexDocument][L3] Opened file '1268'.
[preprocess][L2] Starting preprocessing for '1268'.
[preprocess][L3] 1268: Tokenized into 367 tokens.
[preprocess][L3] 1268: Removed stop words; 210 tokens remain.
[preprocess][L3] 1268: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1268'; 210 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1268'.
[indexDocument][L3] Recorded doc length 210 for '1268'.
[indexDocument][L3] Tokens added to the inverted index for '1268'.
[indexDocument][L2] Indexing document '1062'.
[indexDocument][L3] Opened file '1062'.
[preprocess][L2] Starting preprocessing for '1062'.
[preprocess][L3] 1062: Tokenized into 103 tokens.
[preprocess][L3] 1062: Removed stop words; 65 tokens remain.
[preprocess][L3] 1062: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1062'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1062'.
[indexDocument][L3] Recorded doc length 65 for '1062'.
[indexDocument][L3] Tokens added to the inverted index for '1062'.
[indexDocument][L2] Indexing document '1250'.
[indexDocument][L3] Opened file '1250'.
[preprocess][L2] Starting preprocessing for '1250'.
[preprocess][L3] 1250: Tokenized into 256 tokens.
[preprocess][L3] 1250: Removed stop words; 160 tokens remain.
[preprocess][L3] 1250: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1250'; 160 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1250'.
[indexDocument][L3] Recorded doc length 160 for '1250'.
[indexDocument][L3] Tokens added to the inverted index for '1250'.
[indexDocument][L2] Indexing document '382'.
[indexDocument][L3] Opened file '382'.
[preprocess][L2] Starting preprocessing for '382'.
[preprocess][L3] 382: Tokenized into 54 tokens.
[preprocess][L3] 382: Removed stop words; 34 tokens remain.
[preprocess][L3] 382: Stemming complete.
[indexDocument][L3] Preprocessing complete for '382'; 34 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '382'.
[indexDocument][L3] Recorded doc length 34 for '382'.
[indexDocument][L3] Tokens added to the inverted index for '382'.
[indexDocument][L2] Indexing document '712'.
[indexDocument][L3] Opened file '712'.
[preprocess][L2] Starting preprocessing for '712'.
[preprocess][L3] 712: Tokenized into 297 tokens.
[preprocess][L3] 712: Removed stop words; 185 tokens remain.
[preprocess][L3] 712: Stemming complete.
[indexDocument][L3] Preprocessing complete for '712'; 185 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '712'.
[indexDocument][L3] Recorded doc length 185 for '712'.
[indexDocument][L3] Tokens added to the inverted index for '712'.
[indexDocument][L2] Indexing document '520'.
[indexDocument][L3] Opened file '520'.
[preprocess][L2] Starting preprocessing for '520'.
[preprocess][L3] 520: Tokenized into 221 tokens.
[preprocess][L3] 520: Removed stop words; 137 tokens remain.
[preprocess][L3] 520: Stemming complete.
[indexDocument][L3] Preprocessing complete for '520'; 137 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '520'.
[indexDocument][L3] Recorded doc length 137 for '520'.
[indexDocument][L3] Tokens added to the inverted index for '520'.
[indexDocument][L2] Indexing document '978'.
[indexDocument][L3] Opened file '978'.
[preprocess][L2] Starting preprocessing for '978'.
[preprocess][L3] 978: Tokenized into 82 tokens.
[preprocess][L3] 978: Removed stop words; 55 tokens remain.
[preprocess][L3] 978: Stemming complete.
[indexDocument][L3] Preprocessing complete for '978'; 55 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '978'.
[indexDocument][L3] Recorded doc length 55 for '978'.
[indexDocument][L3] Tokens added to the inverted index for '978'.
[indexDocument][L2] Indexing document '1096'.
[indexDocument][L3] Opened file '1096'.
[preprocess][L2] Starting preprocessing for '1096'.
[preprocess][L3] 1096: Tokenized into 99 tokens.
[preprocess][L3] 1096: Removed stop words; 69 tokens remain.
[preprocess][L3] 1096: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1096'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1096'.
[indexDocument][L3] Recorded doc length 69 for '1096'.
[indexDocument][L3] Tokens added to the inverted index for '1096'.
[indexDocument][L2] Indexing document '144'.
[indexDocument][L3] Opened file '144'.
[preprocess][L2] Starting preprocessing for '144'.
[preprocess][L3] 144: Tokenized into 145 tokens.
[preprocess][L3] 144: Removed stop words; 92 tokens remain.
[preprocess][L3] 144: Stemming complete.
[indexDocument][L3] Preprocessing complete for '144'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '144'.
[indexDocument][L3] Recorded doc length 92 for '144'.
[indexDocument][L3] Tokens added to the inverted index for '144'.
[indexDocument][L2] Indexing document '376'.
[indexDocument][L3] Opened file '376'.
[preprocess][L2] Starting preprocessing for '376'.
[preprocess][L3] 376: Tokenized into 100 tokens.
[preprocess][L3] 376: Removed stop words; 58 tokens remain.
[preprocess][L3] 376: Stemming complete.
[indexDocument][L3] Preprocessing complete for '376'; 58 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '376'.
[indexDocument][L3] Recorded doc length 58 for '376'.
[indexDocument][L3] Tokens added to the inverted index for '376'.
[indexDocument][L2] Indexing document '349'.
[indexDocument][L3] Opened file '349'.
[preprocess][L2] Starting preprocessing for '349'.
[preprocess][L3] 349: Tokenized into 225 tokens.
[preprocess][L3] 349: Removed stop words; 146 tokens remain.
[preprocess][L3] 349: Stemming complete.
[indexDocument][L3] Preprocessing complete for '349'; 146 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '349'.
[indexDocument][L3] Recorded doc length 146 for '349'.
[indexDocument][L3] Tokens added to the inverted index for '349'.
[indexDocument][L2] Indexing document '947'.
[indexDocument][L3] Opened file '947'.
[preprocess][L2] Starting preprocessing for '947'.
[preprocess][L3] 947: Tokenized into 316 tokens.
[preprocess][L3] 947: Removed stop words; 192 tokens remain.
[preprocess][L3] 947: Stemming complete.
[indexDocument][L3] Preprocessing complete for '947'; 192 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '947'.
[indexDocument][L3] Recorded doc length 192 for '947'.
[indexDocument][L3] Tokens added to the inverted index for '947'.
[indexDocument][L2] Indexing document '924'.
[indexDocument][L3] Opened file '924'.
[preprocess][L2] Starting preprocessing for '924'.
[preprocess][L3] 924: Tokenized into 199 tokens.
[preprocess][L3] 924: Removed stop words; 128 tokens remain.
[preprocess][L3] 924: Stemming complete.
[indexDocument][L3] Preprocessing complete for '924'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '924'.
[indexDocument][L3] Recorded doc length 128 for '924'.
[indexDocument][L3] Tokens added to the inverted index for '924'.
[indexDocument][L2] Indexing document '118'.
[indexDocument][L3] Opened file '118'.
[preprocess][L2] Starting preprocessing for '118'.
[preprocess][L3] 118: Tokenized into 160 tokens.
[preprocess][L3] 118: Removed stop words; 103 tokens remain.
[preprocess][L3] 118: Stemming complete.
[indexDocument][L3] Preprocessing complete for '118'; 103 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '118'.
[indexDocument][L3] Recorded doc length 103 for '118'.
[indexDocument][L3] Tokens added to the inverted index for '118'.
[indexDocument][L2] Indexing document '588'.
[indexDocument][L3] Opened file '588'.
[preprocess][L2] Starting preprocessing for '588'.
[preprocess][L3] 588: Tokenized into 278 tokens.
[preprocess][L3] 588: Removed stop words; 168 tokens remain.
[preprocess][L3] 588: Stemming complete.
[indexDocument][L3] Preprocessing complete for '588'; 168 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '588'.
[indexDocument][L3] Recorded doc length 168 for '588'.
[indexDocument][L3] Tokens added to the inverted index for '588'.
[indexDocument][L2] Indexing document '127'.
[indexDocument][L3] Opened file '127'.
[preprocess][L2] Starting preprocessing for '127'.
[preprocess][L3] 127: Tokenized into 138 tokens.
[preprocess][L3] 127: Removed stop words; 84 tokens remain.
[preprocess][L3] 127: Stemming complete.
[indexDocument][L3] Preprocessing complete for '127'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '127'.
[indexDocument][L3] Recorded doc length 84 for '127'.
[indexDocument][L3] Tokens added to the inverted index for '127'.
[indexDocument][L2] Indexing document '315'.
[indexDocument][L3] Opened file '315'.
[preprocess][L2] Starting preprocessing for '315'.
[preprocess][L3] 315: Tokenized into 485 tokens.
[preprocess][L3] 315: Removed stop words; 274 tokens remain.
[preprocess][L3] 315: Stemming complete.
[indexDocument][L3] Preprocessing complete for '315'; 274 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '315'.
[indexDocument][L3] Recorded doc length 274 for '315'.
[indexDocument][L3] Tokens added to the inverted index for '315'.
[indexDocument][L2] Indexing document '771'.
[indexDocument][L3] Opened file '771'.
[preprocess][L2] Starting preprocessing for '771'.
[preprocess][L3] 771: Tokenized into 118 tokens.
[preprocess][L3] 771: Removed stop words; 70 tokens remain.
[preprocess][L3] 771: Stemming complete.
[indexDocument][L3] Preprocessing complete for '771'; 70 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '771'.
[indexDocument][L3] Recorded doc length 70 for '771'.
[indexDocument][L3] Tokens added to the inverted index for '771'.
[indexDocument][L2] Indexing document '543'.
[indexDocument][L3] Opened file '543'.
[preprocess][L2] Starting preprocessing for '543'.
[preprocess][L3] 543: Tokenized into 87 tokens.
[preprocess][L3] 543: Removed stop words; 53 tokens remain.
[preprocess][L3] 543: Stemming complete.
[indexDocument][L3] Preprocessing complete for '543'; 53 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '543'.
[indexDocument][L3] Recorded doc length 53 for '543'.
[indexDocument][L3] Tokens added to the inverted index for '543'.
[indexDocument][L2] Indexing document '785'.
[indexDocument][L3] Opened file '785'.
[preprocess][L2] Starting preprocessing for '785'.
[preprocess][L3] 785: Tokenized into 169 tokens.
[preprocess][L3] 785: Removed stop words; 96 tokens remain.
[preprocess][L3] 785: Stemming complete.
[indexDocument][L3] Preprocessing complete for '785'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '785'.
[indexDocument][L3] Recorded doc length 96 for '785'.
[indexDocument][L3] Tokens added to the inverted index for '785'.
[indexDocument][L2] Indexing document '1001'.
[indexDocument][L3] Opened file '1001'.
[preprocess][L2] Starting preprocessing for '1001'.
[preprocess][L3] 1001: Tokenized into 133 tokens.
[preprocess][L3] 1001: Removed stop words; 84 tokens remain.
[preprocess][L3] 1001: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1001'; 84 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1001'.
[indexDocument][L3] Recorded doc length 84 for '1001'.
[indexDocument][L3] Tokens added to the inverted index for '1001'.
[indexDocument][L2] Indexing document '1233'.
[indexDocument][L3] Opened file '1233'.
[preprocess][L2] Starting preprocessing for '1233'.
[preprocess][L3] 1233: Tokenized into 116 tokens.
[preprocess][L3] 1233: Removed stop words; 71 tokens remain.
[preprocess][L3] 1233: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1233'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1233'.
[indexDocument][L3] Recorded doc length 71 for '1233'.
[indexDocument][L3] Tokens added to the inverted index for '1233'.
[indexDocument][L2] Indexing document '1039'.
[indexDocument][L3] Opened file '1039'.
[preprocess][L2] Starting preprocessing for '1039'.
[preprocess][L3] 1039: Tokenized into 288 tokens.
[preprocess][L3] 1039: Removed stop words; 163 tokens remain.
[preprocess][L3] 1039: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1039'; 163 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1039'.
[indexDocument][L3] Recorded doc length 163 for '1039'.
[indexDocument][L3] Tokens added to the inverted index for '1039'.
[indexDocument][L2] Indexing document '923'.
[indexDocument][L3] Opened file '923'.
[preprocess][L2] Starting preprocessing for '923'.
[preprocess][L3] 923: Tokenized into 163 tokens.
[preprocess][L3] 923: Removed stop words; 90 tokens remain.
[preprocess][L3] 923: Stemming complete.
[indexDocument][L3] Preprocessing complete for '923'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '923'.
[indexDocument][L3] Recorded doc length 90 for '923'.
[indexDocument][L3] Tokens added to the inverted index for '923'.
[indexDocument][L2] Indexing document '749'.
[indexDocument][L3] Opened file '749'.
[preprocess][L2] Starting preprocessing for '749'.
[preprocess][L3] 749: Tokenized into 156 tokens.
[preprocess][L3] 749: Removed stop words; 92 tokens remain.
[preprocess][L3] 749: Stemming complete.
[indexDocument][L3] Preprocessing complete for '749'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '749'.
[indexDocument][L3] Recorded doc length 92 for '749'.
[indexDocument][L3] Tokens added to the inverted index for '749'.
[indexDocument][L2] Indexing document '1234'.
[indexDocument][L3] Opened file '1234'.
[preprocess][L2] Starting preprocessing for '1234'.
[preprocess][L3] 1234: Tokenized into 84 tokens.
[preprocess][L3] 1234: Removed stop words; 62 tokens remain.
[preprocess][L3] 1234: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1234'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1234'.
[indexDocument][L3] Recorded doc length 62 for '1234'.
[indexDocument][L3] Tokens added to the inverted index for '1234'.
[indexDocument][L2] Indexing document '1006'.
[indexDocument][L3] Opened file '1006'.
[preprocess][L2] Starting preprocessing for '1006'.
[preprocess][L3] 1006: Tokenized into 85 tokens.
[preprocess][L3] 1006: Removed stop words; 51 tokens remain.
[preprocess][L3] 1006: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1006'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1006'.
[indexDocument][L3] Recorded doc length 51 for '1006'.
[indexDocument][L3] Tokens added to the inverted index for '1006'.
[indexDocument][L2] Indexing document '782'.
[indexDocument][L3] Opened file '782'.
[preprocess][L2] Starting preprocessing for '782'.
[preprocess][L3] 782: Tokenized into 228 tokens.
[preprocess][L3] 782: Removed stop words; 137 tokens remain.
[preprocess][L3] 782: Stemming complete.
[indexDocument][L3] Preprocessing complete for '782'; 137 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '782'.
[indexDocument][L3] Recorded doc length 137 for '782'.
[indexDocument][L3] Tokens added to the inverted index for '782'.
[indexDocument][L2] Indexing document '544'.
[indexDocument][L3] Opened file '544'.
[preprocess][L2] Starting preprocessing for '544'.
[preprocess][L3] 544: Tokenized into 98 tokens.
[preprocess][L3] 544: Removed stop words; 62 tokens remain.
[preprocess][L3] 544: Stemming complete.
[indexDocument][L3] Preprocessing complete for '544'; 62 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '544'.
[indexDocument][L3] Recorded doc length 62 for '544'.
[indexDocument][L3] Tokens added to the inverted index for '544'.
[indexDocument][L2] Indexing document '776'.
[indexDocument][L3] Opened file '776'.
[preprocess][L2] Starting preprocessing for '776'.
[preprocess][L3] 776: Tokenized into 73 tokens.
[preprocess][L3] 776: Removed stop words; 49 tokens remain.
[preprocess][L3] 776: Stemming complete.
[indexDocument][L3] Preprocessing complete for '776'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '776'.
[indexDocument][L3] Recorded doc length 49 for '776'.
[indexDocument][L3] Tokens added to the inverted index for '776'.
[indexDocument][L2] Indexing document '312'.
[indexDocument][L3] Opened file '312'.
[preprocess][L2] Starting preprocessing for '312'.
[preprocess][L3] 312: Tokenized into 85 tokens.
[preprocess][L3] 312: Removed stop words; 60 tokens remain.
[preprocess][L3] 312: Stemming complete.
[indexDocument][L3] Preprocessing complete for '312'; 60 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '312'.
[indexDocument][L3] Recorded doc length 60 for '312'.
[indexDocument][L3] Tokens added to the inverted index for '312'.
[indexDocument][L2] Indexing document '120'.
[indexDocument][L3] Opened file '120'.
[preprocess][L2] Starting preprocessing for '120'.
[preprocess][L3] 120: Tokenized into 112 tokens.
[preprocess][L3] 120: Removed stop words; 71 tokens remain.
[preprocess][L3] 120: Stemming complete.
[indexDocument][L3] Preprocessing complete for '120'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '120'.
[indexDocument][L3] Recorded doc length 71 for '120'.
[indexDocument][L3] Tokens added to the inverted index for '120'.
[indexDocument][L2] Indexing document '915'.
[indexDocument][L3] Opened file '915'.
[preprocess][L2] Starting preprocessing for '915'.
[preprocess][L3] 915: Tokenized into 90 tokens.
[preprocess][L3] 915: Removed stop words; 57 tokens remain.
[preprocess][L3] 915: Stemming complete.
[indexDocument][L3] Preprocessing complete for '915'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '915'.
[indexDocument][L3] Recorded doc length 57 for '915'.
[indexDocument][L3] Tokens added to the inverted index for '915'.
[indexDocument][L2] Indexing document '129'.
[indexDocument][L3] Opened file '129'.
[preprocess][L2] Starting preprocessing for '129'.
[preprocess][L3] 129: Tokenized into 257 tokens.
[preprocess][L3] 129: Removed stop words; 155 tokens remain.
[preprocess][L3] 129: Stemming complete.
[indexDocument][L3] Preprocessing complete for '129'; 155 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '129'.
[indexDocument][L3] Recorded doc length 155 for '129'.
[indexDocument][L3] Tokens added to the inverted index for '129'.
[indexDocument][L2] Indexing document '586'.
[indexDocument][L3] Opened file '586'.
[preprocess][L2] Starting preprocessing for '586'.
[preprocess][L3] 586: Tokenized into 113 tokens.
[preprocess][L3] 586: Removed stop words; 72 tokens remain.
[preprocess][L3] 586: Stemming complete.
[indexDocument][L3] Preprocessing complete for '586'; 72 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '586'.
[indexDocument][L3] Recorded doc length 72 for '586'.
[indexDocument][L3] Tokens added to the inverted index for '586'.
[indexDocument][L2] Indexing document '1202'.
[indexDocument][L3] Opened file '1202'.
[preprocess][L2] Starting preprocessing for '1202'.
[preprocess][L3] 1202: Tokenized into 292 tokens.
[preprocess][L3] 1202: Removed stop words; 182 tokens remain.
[preprocess][L3] 1202: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1202'; 182 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1202'.
[indexDocument][L3] Recorded doc length 182 for '1202'.
[indexDocument][L3] Tokens added to the inverted index for '1202'.
[indexDocument][L2] Indexing document '1030'.
[indexDocument][L3] Opened file '1030'.
[preprocess][L2] Starting preprocessing for '1030'.
[preprocess][L3] 1030: Tokenized into 47 tokens.
[preprocess][L3] 1030: Removed stop words; 25 tokens remain.
[preprocess][L3] 1030: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1030'; 25 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1030'.
[indexDocument][L3] Recorded doc length 25 for '1030'.
[indexDocument][L3] Tokens added to the inverted index for '1030'.
[indexDocument][L2] Indexing document '324'.
[indexDocument][L3] Opened file '324'.
[preprocess][L2] Starting preprocessing for '324'.
[preprocess][L3] 324: Tokenized into 82 tokens.
[preprocess][L3] 324: Removed stop words; 53 tokens remain.
[preprocess][L3] 324: Stemming complete.
[indexDocument][L3] Preprocessing complete for '324'; 53 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '324'.
[indexDocument][L3] Recorded doc length 53 for '324'.
[indexDocument][L3] Tokens added to the inverted index for '324'.
[indexDocument][L2] Indexing document '116'.
[indexDocument][L3] Opened file '116'.
[preprocess][L2] Starting preprocessing for '116'.
[preprocess][L3] 116: Tokenized into 167 tokens.
[preprocess][L3] 116: Removed stop words; 100 tokens remain.
[preprocess][L3] 116: Stemming complete.
[indexDocument][L3] Preprocessing complete for '116'; 100 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '116'.
[indexDocument][L3] Recorded doc length 100 for '116'.
[indexDocument][L3] Tokens added to the inverted index for '116'.
[indexDocument][L2] Indexing document '572'.
[indexDocument][L3] Opened file '572'.
[preprocess][L2] Starting preprocessing for '572'.
[preprocess][L3] 572: Tokenized into 387 tokens.
[preprocess][L3] 572: Removed stop words; 252 tokens remain.
[preprocess][L3] 572: Stemming complete.
[indexDocument][L3] Preprocessing complete for '572'; 252 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '572'.
[indexDocument][L3] Recorded doc length 252 for '572'.
[indexDocument][L3] Tokens added to the inverted index for '572'.
[indexDocument][L2] Indexing document '740'.
[indexDocument][L3] Opened file '740'.
[preprocess][L2] Starting preprocessing for '740'.
[preprocess][L3] 740: Tokenized into 226 tokens.
[preprocess][L3] 740: Removed stop words; 120 tokens remain.
[preprocess][L3] 740: Stemming complete.
[indexDocument][L3] Preprocessing complete for '740'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '740'.
[indexDocument][L3] Recorded doc length 120 for '740'.
[indexDocument][L3] Tokens added to the inverted index for '740'.
[indexDocument][L2] Indexing document '912'.
[indexDocument][L3] Opened file '912'.
[preprocess][L2] Starting preprocessing for '912'.
[preprocess][L3] 912: Tokenized into 166 tokens.
[preprocess][L3] 912: Removed stop words; 105 tokens remain.
[preprocess][L3] 912: Stemming complete.
[indexDocument][L3] Preprocessing complete for '912'; 105 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '912'.
[indexDocument][L3] Recorded doc length 105 for '912'.
[indexDocument][L3] Tokens added to the inverted index for '912'.
[indexDocument][L2] Indexing document '778'.
[indexDocument][L3] Opened file '778'.
[preprocess][L2] Starting preprocessing for '778'.
[preprocess][L3] 778: Tokenized into 73 tokens.
[preprocess][L3] 778: Removed stop words; 46 tokens remain.
[preprocess][L3] 778: Stemming complete.
[indexDocument][L3] Preprocessing complete for '778'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '778'.
[indexDocument][L3] Recorded doc length 46 for '778'.
[indexDocument][L3] Tokens added to the inverted index for '778'.
[indexDocument][L2] Indexing document '1008'.
[indexDocument][L3] Opened file '1008'.
[preprocess][L2] Starting preprocessing for '1008'.
[preprocess][L3] 1008: Tokenized into 82 tokens.
[preprocess][L3] 1008: Removed stop words; 51 tokens remain.
[preprocess][L3] 1008: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1008'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1008'.
[indexDocument][L3] Recorded doc length 51 for '1008'.
[indexDocument][L3] Tokens added to the inverted index for '1008'.
[indexDocument][L2] Indexing document '747'.
[indexDocument][L3] Opened file '747'.
[preprocess][L2] Starting preprocessing for '747'.
[preprocess][L3] 747: Tokenized into 135 tokens.
[preprocess][L3] 747: Removed stop words; 78 tokens remain.
[preprocess][L3] 747: Stemming complete.
[indexDocument][L3] Preprocessing complete for '747'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '747'.
[indexDocument][L3] Recorded doc length 78 for '747'.
[indexDocument][L3] Tokens added to the inverted index for '747'.
[indexDocument][L2] Indexing document '575'.
[indexDocument][L3] Opened file '575'.
[preprocess][L2] Starting preprocessing for '575'.
[preprocess][L3] 575: Tokenized into 270 tokens.
[preprocess][L3] 575: Removed stop words; 156 tokens remain.
[preprocess][L3] 575: Stemming complete.
[indexDocument][L3] Preprocessing complete for '575'; 156 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '575'.
[indexDocument][L3] Recorded doc length 156 for '575'.
[indexDocument][L3] Tokens added to the inverted index for '575'.
[indexDocument][L2] Indexing document '111'.
[indexDocument][L3] Opened file '111'.
[preprocess][L2] Starting preprocessing for '111'.
[preprocess][L3] 111: Tokenized into 116 tokens.
[preprocess][L3] 111: Removed stop words; 67 tokens remain.
[preprocess][L3] 111: Stemming complete.
[indexDocument][L3] Preprocessing complete for '111'; 67 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '111'.
[indexDocument][L3] Recorded doc length 67 for '111'.
[indexDocument][L3] Tokens added to the inverted index for '111'.
[indexDocument][L2] Indexing document '323'.
[indexDocument][L3] Opened file '323'.
[preprocess][L2] Starting preprocessing for '323'.
[preprocess][L3] 323: Tokenized into 85 tokens.
[preprocess][L3] 323: Removed stop words; 50 tokens remain.
[preprocess][L3] 323: Stemming complete.
[indexDocument][L3] Preprocessing complete for '323'; 50 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '323'.
[indexDocument][L3] Recorded doc length 50 for '323'.
[indexDocument][L3] Tokens added to the inverted index for '323'.
[indexDocument][L2] Indexing document '1037'.
[indexDocument][L3] Opened file '1037'.
[preprocess][L2] Starting preprocessing for '1037'.
[preprocess][L3] 1037: Tokenized into 83 tokens.
[preprocess][L3] 1037: Removed stop words; 49 tokens remain.
[preprocess][L3] 1037: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1037'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1037'.
[indexDocument][L3] Recorded doc length 49 for '1037'.
[indexDocument][L3] Tokens added to the inverted index for '1037'.
[indexDocument][L2] Indexing document '1205'.
[indexDocument][L3] Opened file '1205'.
[preprocess][L2] Starting preprocessing for '1205'.
[preprocess][L3] 1205: Tokenized into 221 tokens.
[preprocess][L3] 1205: Removed stop words; 131 tokens remain.
[preprocess][L3] 1205: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1205'; 131 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1205'.
[indexDocument][L3] Recorded doc length 131 for '1205'.
[indexDocument][L3] Tokens added to the inverted index for '1205'.
[indexDocument][L2] Indexing document '581'.
[indexDocument][L3] Opened file '581'.
[preprocess][L2] Starting preprocessing for '581'.
[preprocess][L3] 581: Tokenized into 97 tokens.
[preprocess][L3] 581: Removed stop words; 56 tokens remain.
[preprocess][L3] 581: Stemming complete.
[indexDocument][L3] Preprocessing complete for '581'; 56 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '581'.
[indexDocument][L3] Recorded doc length 56 for '581'.
[indexDocument][L3] Tokens added to the inverted index for '581'.
[indexDocument][L2] Indexing document '521'.
[indexDocument][L3] Opened file '521'.
[preprocess][L2] Starting preprocessing for '521'.
[preprocess][L3] 521: Tokenized into 158 tokens.
[preprocess][L3] 521: Removed stop words; 92 tokens remain.
[preprocess][L3] 521: Stemming complete.
[indexDocument][L3] Preprocessing complete for '521'; 92 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '521'.
[indexDocument][L3] Recorded doc length 92 for '521'.
[indexDocument][L3] Tokens added to the inverted index for '521'.
[indexDocument][L2] Indexing document '713'.
[indexDocument][L3] Opened file '713'.
[preprocess][L2] Starting preprocessing for '713'.
[preprocess][L3] 713: Tokenized into 168 tokens.
[preprocess][L3] 713: Removed stop words; 110 tokens remain.
[preprocess][L3] 713: Stemming complete.
[indexDocument][L3] Preprocessing complete for '713'; 110 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '713'.
[indexDocument][L3] Recorded doc length 110 for '713'.
[indexDocument][L3] Tokens added to the inverted index for '713'.
[indexDocument][L2] Indexing document '377'.
[indexDocument][L3] Opened file '377'.
[preprocess][L2] Starting preprocessing for '377'.
[preprocess][L3] 377: Tokenized into 151 tokens.
[preprocess][L3] 377: Removed stop words; 88 tokens remain.
[preprocess][L3] 377: Stemming complete.
[indexDocument][L3] Preprocessing complete for '377'; 88 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '377'.
[indexDocument][L3] Recorded doc length 88 for '377'.
[indexDocument][L3] Tokens added to the inverted index for '377'.
[indexDocument][L2] Indexing document '1097'.
[indexDocument][L3] Opened file '1097'.
[preprocess][L2] Starting preprocessing for '1097'.
[preprocess][L3] 1097: Tokenized into 236 tokens.
[preprocess][L3] 1097: Removed stop words; 148 tokens remain.
[preprocess][L3] 1097: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1097'; 148 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1097'.
[indexDocument][L3] Recorded doc length 148 for '1097'.
[indexDocument][L3] Tokens added to the inverted index for '1097'.
[indexDocument][L2] Indexing document '145'.
[indexDocument][L3] Opened file '145'.
[preprocess][L2] Starting preprocessing for '145'.
[preprocess][L3] 145: Tokenized into 140 tokens.
[preprocess][L3] 145: Removed stop words; 76 tokens remain.
[preprocess][L3] 145: Stemming complete.
[indexDocument][L3] Preprocessing complete for '145'; 76 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '145'.
[indexDocument][L3] Recorded doc length 76 for '145'.
[indexDocument][L3] Tokens added to the inverted index for '145'.
[indexDocument][L2] Indexing document '979'.
[indexDocument][L3] Opened file '979'.
[preprocess][L2] Starting preprocessing for '979'.
[preprocess][L3] 979: Tokenized into 146 tokens.
[preprocess][L3] 979: Removed stop words; 90 tokens remain.
[preprocess][L3] 979: Stemming complete.
[indexDocument][L3] Preprocessing complete for '979'; 90 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '979'.
[indexDocument][L3] Recorded doc length 90 for '979'.
[indexDocument][L3] Tokens added to the inverted index for '979'.
[indexDocument][L2] Indexing document '1251'.
[indexDocument][L3] Opened file '1251'.
[preprocess][L2] Starting preprocessing for '1251'.
[preprocess][L3] 1251: Tokenized into 204 tokens.
[preprocess][L3] 1251: Removed stop words; 113 tokens remain.
[preprocess][L3] 1251: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1251'; 113 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1251'.
[indexDocument][L3] Recorded doc length 113 for '1251'.
[indexDocument][L3] Tokens added to the inverted index for '1251'.
[indexDocument][L2] Indexing document '383'.
[indexDocument][L3] Opened file '383'.
[preprocess][L2] Starting preprocessing for '383'.
[preprocess][L3] 383: Tokenized into 131 tokens.
[preprocess][L3] 383: Removed stop words; 78 tokens remain.
[preprocess][L3] 383: Stemming complete.
[indexDocument][L3] Preprocessing complete for '383'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '383'.
[indexDocument][L3] Recorded doc length 78 for '383'.
[indexDocument][L3] Tokens added to the inverted index for '383'.
[indexDocument][L2] Indexing document '1063'.
[indexDocument][L3] Opened file '1063'.
[preprocess][L2] Starting preprocessing for '1063'.
[preprocess][L3] 1063: Tokenized into 83 tokens.
[preprocess][L3] 1063: Removed stop words; 51 tokens remain.
[preprocess][L3] 1063: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1063'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1063'.
[indexDocument][L3] Recorded doc length 51 for '1063'.
[indexDocument][L3] Tokens added to the inverted index for '1063'.
[indexDocument][L2] Indexing document '946'.
[indexDocument][L3] Opened file '946'.
[preprocess][L2] Starting preprocessing for '946'.
[preprocess][L3] 946: Tokenized into 338 tokens.
[preprocess][L3] 946: Removed stop words; 205 tokens remain.
[preprocess][L3] 946: Stemming complete.
[indexDocument][L3] Preprocessing complete for '946'; 205 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '946'.
[indexDocument][L3] Recorded doc length 205 for '946'.
[indexDocument][L3] Tokens added to the inverted index for '946'.
[indexDocument][L2] Indexing document '348'.
[indexDocument][L3] Opened file '348'.
[preprocess][L2] Starting preprocessing for '348'.
[preprocess][L3] 348: Tokenized into 142 tokens.
[preprocess][L3] 348: Removed stop words; 96 tokens remain.
[preprocess][L3] 348: Stemming complete.
[indexDocument][L3] Preprocessing complete for '348'; 96 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '348'.
[indexDocument][L3] Recorded doc length 96 for '348'.
[indexDocument][L3] Tokens added to the inverted index for '348'.
[indexDocument][L2] Indexing document '1400'.
[indexDocument][L3] Opened file '1400'.
[preprocess][L2] Starting preprocessing for '1400'.
[preprocess][L3] 1400: Tokenized into 107 tokens.
[preprocess][L3] 1400: Removed stop words; 68 tokens remain.
[preprocess][L3] 1400: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1400'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1400'.
[indexDocument][L3] Recorded doc length 68 for '1400'.
[indexDocument][L3] Tokens added to the inverted index for '1400'.
[indexDocument][L2] Indexing document '1064'.
[indexDocument][L3] Opened file '1064'.
[preprocess][L2] Starting preprocessing for '1064'.
[preprocess][L3] 1064: Tokenized into 177 tokens.
[preprocess][L3] 1064: Removed stop words; 116 tokens remain.
[preprocess][L3] 1064: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1064'; 116 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1064'.
[indexDocument][L3] Recorded doc length 116 for '1064'.
[indexDocument][L3] Tokens added to the inverted index for '1064'.
[indexDocument][L2] Indexing document '1256'.
[indexDocument][L3] Opened file '1256'.
[preprocess][L2] Starting preprocessing for '1256'.
[preprocess][L3] 1256: Tokenized into 73 tokens.
[preprocess][L3] 1256: Removed stop words; 45 tokens remain.
[preprocess][L3] 1256: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1256'; 45 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1256'.
[indexDocument][L3] Recorded doc length 45 for '1256'.
[indexDocument][L3] Tokens added to the inverted index for '1256'.
[indexDocument][L2] Indexing document '384'.
[indexDocument][L3] Opened file '384'.
[preprocess][L2] Starting preprocessing for '384'.
[preprocess][L3] 384: Tokenized into 65 tokens.
[preprocess][L3] 384: Removed stop words; 50 tokens remain.
[preprocess][L3] 384: Stemming complete.
[indexDocument][L3] Preprocessing complete for '384'; 50 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '384'.
[indexDocument][L3] Recorded doc length 50 for '384'.
[indexDocument][L3] Tokens added to the inverted index for '384'.
[indexDocument][L2] Indexing document '1090'.
[indexDocument][L3] Opened file '1090'.
[preprocess][L2] Starting preprocessing for '1090'.
[preprocess][L3] 1090: Tokenized into 72 tokens.
[preprocess][L3] 1090: Removed stop words; 48 tokens remain.
[preprocess][L3] 1090: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1090'; 48 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1090'.
[indexDocument][L3] Recorded doc length 48 for '1090'.
[indexDocument][L3] Tokens added to the inverted index for '1090'.
[indexDocument][L2] Indexing document '142'.
[indexDocument][L3] Opened file '142'.
[preprocess][L2] Starting preprocessing for '142'.
[preprocess][L3] 142: Tokenized into 106 tokens.
[preprocess][L3] 142: Removed stop words; 73 tokens remain.
[preprocess][L3] 142: Stemming complete.
[indexDocument][L3] Preprocessing complete for '142'; 73 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '142'.
[indexDocument][L3] Recorded doc length 73 for '142'.
[indexDocument][L3] Tokens added to the inverted index for '142'.
[indexDocument][L2] Indexing document '370'.
[indexDocument][L3] Opened file '370'.
[preprocess][L2] Starting preprocessing for '370'.
[preprocess][L3] 370: Tokenized into 197 tokens.
[preprocess][L3] 370: Removed stop words; 109 tokens remain.
[preprocess][L3] 370: Stemming complete.
[indexDocument][L3] Preprocessing complete for '370'; 109 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '370'.
[indexDocument][L3] Recorded doc length 109 for '370'.
[indexDocument][L3] Tokens added to the inverted index for '370'.
[indexDocument][L2] Indexing document '714'.
[indexDocument][L3] Opened file '714'.
[preprocess][L2] Starting preprocessing for '714'.
[preprocess][L3] 714: Tokenized into 120 tokens.
[preprocess][L3] 714: Removed stop words; 65 tokens remain.
[preprocess][L3] 714: Stemming complete.
[indexDocument][L3] Preprocessing complete for '714'; 65 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '714'.
[indexDocument][L3] Recorded doc length 65 for '714'.
[indexDocument][L3] Tokens added to the inverted index for '714'.
[indexDocument][L2] Indexing document '526'.
[indexDocument][L3] Opened file '526'.
[preprocess][L2] Starting preprocessing for '526'.
[preprocess][L3] 526: Tokenized into 116 tokens.
[preprocess][L3] 526: Removed stop words; 75 tokens remain.
[preprocess][L3] 526: Stemming complete.
[indexDocument][L3] Preprocessing complete for '526'; 75 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '526'.
[indexDocument][L3] Recorded doc length 75 for '526'.
[indexDocument][L3] Tokens added to the inverted index for '526'.
[indexDocument][L2] Indexing document '1269'.
[indexDocument][L3] Opened file '1269'.
[preprocess][L2] Starting preprocessing for '1269'.
[preprocess][L3] 1269: Tokenized into 74 tokens.
[preprocess][L3] 1269: Removed stop words; 51 tokens remain.
[preprocess][L3] 1269: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1269'; 51 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1269'.
[indexDocument][L3] Recorded doc length 51 for '1269'.
[indexDocument][L3] Tokens added to the inverted index for '1269'.
[indexDocument][L2] Indexing document '189'.
[indexDocument][L3] Opened file '189'.
[preprocess][L2] Starting preprocessing for '189'.
[preprocess][L3] 189: Tokenized into 365 tokens.
[preprocess][L3] 189: Removed stop words; 234 tokens remain.
[preprocess][L3] 189: Stemming complete.
[indexDocument][L3] Preprocessing complete for '189'; 234 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '189'.
[indexDocument][L3] Recorded doc length 234 for '189'.
[indexDocument][L3] Tokens added to the inverted index for '189'.
[indexDocument][L2] Indexing document '519'.
[indexDocument][L3] Opened file '519'.
[preprocess][L2] Starting preprocessing for '519'.
[preprocess][L3] 519: Tokenized into 164 tokens.
[preprocess][L3] 519: Removed stop words; 110 tokens remain.
[preprocess][L3] 519: Stemming complete.
[indexDocument][L3] Preprocessing complete for '519'; 110 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '519'.
[indexDocument][L3] Recorded doc length 110 for '519'.
[indexDocument][L3] Tokens added to the inverted index for '519'.
[indexDocument][L2] Indexing document '941'.
[indexDocument][L3] Opened file '941'.
[preprocess][L2] Starting preprocessing for '941'.
[preprocess][L3] 941: Tokenized into 102 tokens.
[preprocess][L3] 941: Removed stop words; 64 tokens remain.
[preprocess][L3] 941: Stemming complete.
[indexDocument][L3] Preprocessing complete for '941'; 64 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '941'.
[indexDocument][L3] Recorded doc length 64 for '941'.
[indexDocument][L3] Tokens added to the inverted index for '941'.
[indexDocument][L2] Indexing document '180'.
[indexDocument][L3] Opened file '180'.
[preprocess][L2] Starting preprocessing for '180'.
[preprocess][L3] 180: Tokenized into 70 tokens.
[preprocess][L3] 180: Removed stop words; 44 tokens remain.
[preprocess][L3] 180: Stemming complete.
[indexDocument][L3] Preprocessing complete for '180'; 44 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '180'.
[indexDocument][L3] Recorded doc length 44 for '180'.
[indexDocument][L3] Tokens added to the inverted index for '180'.
[indexDocument][L2] Indexing document '1052'.
[indexDocument][L3] Opened file '1052'.
[preprocess][L2] Starting preprocessing for '1052'.
[preprocess][L3] 1052: Tokenized into 113 tokens.
[preprocess][L3] 1052: Removed stop words; 69 tokens remain.
[preprocess][L3] 1052: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1052'; 69 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1052'.
[indexDocument][L3] Recorded doc length 69 for '1052'.
[indexDocument][L3] Tokens added to the inverted index for '1052'.
[indexDocument][L2] Indexing document '1260'.
[indexDocument][L3] Opened file '1260'.
[preprocess][L2] Starting preprocessing for '1260'.
[preprocess][L3] 1260: Tokenized into 115 tokens.
[preprocess][L3] 1260: Removed stop words; 71 tokens remain.
[preprocess][L3] 1260: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1260'; 71 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1260'.
[indexDocument][L3] Recorded doc length 71 for '1260'.
[indexDocument][L3] Tokens added to the inverted index for '1260'.
[indexDocument][L2] Indexing document '722'.
[indexDocument][L3] Opened file '722'.
[preprocess][L2] Starting preprocessing for '722'.
[preprocess][L3] 722: Tokenized into 185 tokens.
[preprocess][L3] 722: Removed stop words; 102 tokens remain.
[preprocess][L3] 722: Stemming complete.
[indexDocument][L3] Preprocessing complete for '722'; 102 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '722'.
[indexDocument][L3] Recorded doc length 102 for '722'.
[indexDocument][L3] Tokens added to the inverted index for '722'.
[indexDocument][L2] Indexing document '510'.
[indexDocument][L3] Opened file '510'.
[preprocess][L2] Starting preprocessing for '510'.
[preprocess][L3] 510: Tokenized into 75 tokens.
[preprocess][L3] 510: Removed stop words; 47 tokens remain.
[preprocess][L3] 510: Stemming complete.
[indexDocument][L3] Preprocessing complete for '510'; 47 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '510'.
[indexDocument][L3] Recorded doc length 47 for '510'.
[indexDocument][L3] Tokens added to the inverted index for '510'.
[indexDocument][L2] Indexing document '174'.
[indexDocument][L3] Opened file '174'.
[preprocess][L2] Starting preprocessing for '174'.
[preprocess][L3] 174: Tokenized into 294 tokens.
[preprocess][L3] 174: Removed stop words; 183 tokens remain.
[preprocess][L3] 174: Stemming complete.
[indexDocument][L3] Preprocessing complete for '174'; 183 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '174'.
[indexDocument][L3] Recorded doc length 183 for '174'.
[indexDocument][L3] Tokens added to the inverted index for '174'.
[indexDocument][L2] Indexing document '948'.
[indexDocument][L3] Opened file '948'.
[preprocess][L2] Starting preprocessing for '948'.
[preprocess][L3] 948: Tokenized into 185 tokens.
[preprocess][L3] 948: Removed stop words; 117 tokens remain.
[preprocess][L3] 948: Stemming complete.
[indexDocument][L3] Preprocessing complete for '948'; 117 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '948'.
[indexDocument][L3] Recorded doc length 117 for '948'.
[indexDocument][L3] Tokens added to the inverted index for '948'.
[indexDocument][L2] Indexing document '346'.
[indexDocument][L3] Opened file '346'.
[preprocess][L2] Starting preprocessing for '346'.
[preprocess][L3] 346: Tokenized into 186 tokens.
[preprocess][L3] 346: Removed stop words; 111 tokens remain.
[preprocess][L3] 346: Stemming complete.
[indexDocument][L3] Preprocessing complete for '346'; 111 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '346'.
[indexDocument][L3] Recorded doc length 111 for '346'.
[indexDocument][L3] Tokens added to the inverted index for '346'.
[indexDocument][L2] Indexing document '1294'.
[indexDocument][L3] Opened file '1294'.
[preprocess][L2] Starting preprocessing for '1294'.
[preprocess][L3] 1294: Tokenized into 194 tokens.
[preprocess][L3] 1294: Removed stop words; 116 tokens remain.
[preprocess][L3] 1294: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1294'; 116 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1294'.
[indexDocument][L3] Recorded doc length 116 for '1294'.
[indexDocument][L3] Tokens added to the inverted index for '1294'.
[indexDocument][L2] Indexing document '983'.
[indexDocument][L3] Opened file '983'.
[preprocess][L2] Starting preprocessing for '983'.
[preprocess][L3] 983: Tokenized into 100 tokens.
[preprocess][L3] 983: Removed stop words; 68 tokens remain.
[preprocess][L3] 983: Stemming complete.
[indexDocument][L3] Preprocessing complete for '983'; 68 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '983'.
[indexDocument][L3] Recorded doc length 68 for '983'.
[indexDocument][L3] Tokens added to the inverted index for '983'.
[indexDocument][L2] Indexing document '379'.
[indexDocument][L3] Opened file '379'.
[preprocess][L2] Starting preprocessing for '379'.
[preprocess][L3] 379: Tokenized into 144 tokens.
[preprocess][L3] 379: Removed stop words; 91 tokens remain.
[preprocess][L3] 379: Stemming complete.
[indexDocument][L3] Preprocessing complete for '379'; 91 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '379'.
[indexDocument][L3] Recorded doc length 91 for '379'.
[indexDocument][L3] Tokens added to the inverted index for '379'.
[indexDocument][L2] Indexing document '1099'.
[indexDocument][L3] Opened file '1099'.
[preprocess][L2] Starting preprocessing for '1099'.
[preprocess][L3] 1099: Tokenized into 77 tokens.
[preprocess][L3] 1099: Removed stop words; 49 tokens remain.
[preprocess][L3] 1099: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1099'; 49 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1099'.
[indexDocument][L3] Recorded doc length 49 for '1099'.
[indexDocument][L3] Tokens added to the inverted index for '1099'.
[indexDocument][L2] Indexing document '977'.
[indexDocument][L3] Opened file '977'.
[preprocess][L2] Starting preprocessing for '977'.
[preprocess][L3] 977: Tokenized into 244 tokens.
[preprocess][L3] 977: Removed stop words; 128 tokens remain.
[preprocess][L3] 977: Stemming complete.
[indexDocument][L3] Preprocessing complete for '977'; 128 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '977'.
[indexDocument][L3] Recorded doc length 128 for '977'.
[indexDocument][L3] Tokens added to the inverted index for '977'.
[indexDocument][L2] Indexing document '341'.
[indexDocument][L3] Opened file '341'.
[preprocess][L2] Starting preprocessing for '341'.
[preprocess][L3] 341: Tokenized into 247 tokens.
[preprocess][L3] 341: Removed stop words; 133 tokens remain.
[preprocess][L3] 341: Stemming complete.
[indexDocument][L3] Preprocessing complete for '341'; 133 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '341'.
[indexDocument][L3] Recorded doc length 133 for '341'.
[indexDocument][L3] Tokens added to the inverted index for '341'.
[indexDocument][L2] Indexing document '1293'.
[indexDocument][L3] Opened file '1293'.
[preprocess][L2] Starting preprocessing for '1293'.
[preprocess][L3] 1293: Tokenized into 58 tokens.
[preprocess][L3] 1293: Removed stop words; 39 tokens remain.
[preprocess][L3] 1293: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1293'; 39 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1293'.
[indexDocument][L3] Recorded doc length 39 for '1293'.
[indexDocument][L3] Tokens added to the inverted index for '1293'.
[indexDocument][L2] Indexing document '173'.
[indexDocument][L3] Opened file '173'.
[preprocess][L2] Starting preprocessing for '173'.
[preprocess][L3] 173: Tokenized into 314 tokens.
[preprocess][L3] 173: Removed stop words; 197 tokens remain.
[preprocess][L3] 173: Stemming complete.
[indexDocument][L3] Preprocessing complete for '173'; 197 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '173'.
[indexDocument][L3] Recorded doc length 197 for '173'.
[indexDocument][L3] Tokens added to the inverted index for '173'.
[indexDocument][L2] Indexing document '517'.
[indexDocument][L3] Opened file '517'.
[preprocess][L2] Starting preprocessing for '517'.
[preprocess][L3] 517: Tokenized into 80 tokens.
[preprocess][L3] 517: Removed stop words; 57 tokens remain.
[preprocess][L3] 517: Stemming complete.
[indexDocument][L3] Preprocessing complete for '517'; 57 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '517'.
[indexDocument][L3] Recorded doc length 57 for '517'.
[indexDocument][L3] Tokens added to the inverted index for '517'.
[indexDocument][L2] Indexing document '725'.
[indexDocument][L3] Opened file '725'.
[preprocess][L2] Starting preprocessing for '725'.
[preprocess][L3] 725: Tokenized into 110 tokens.
[preprocess][L3] 725: Removed stop words; 70 tokens remain.
[preprocess][L3] 725: Stemming complete.
[indexDocument][L3] Preprocessing complete for '725'; 70 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '725'.
[indexDocument][L3] Recorded doc length 70 for '725'.
[indexDocument][L3] Tokens added to the inverted index for '725'.
[indexDocument][L2] Indexing document '1267'.
[indexDocument][L3] Opened file '1267'.
[preprocess][L2] Starting preprocessing for '1267'.
[preprocess][L3] 1267: Tokenized into 71 tokens.
[preprocess][L3] 1267: Removed stop words; 46 tokens remain.
[preprocess][L3] 1267: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1267'; 46 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1267'.
[indexDocument][L3] Recorded doc length 46 for '1267'.
[indexDocument][L3] Tokens added to the inverted index for '1267'.
[indexDocument][L2] Indexing document '187'.
[indexDocument][L3] Opened file '187'.
[preprocess][L2] Starting preprocessing for '187'.
[preprocess][L3] 187: Tokenized into 279 tokens.
[preprocess][L3] 187: Removed stop words; 178 tokens remain.
[preprocess][L3] 187: Stemming complete.
[indexDocument][L3] Preprocessing complete for '187'; 178 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '187'.
[indexDocument][L3] Recorded doc length 178 for '187'.
[indexDocument][L3] Tokens added to the inverted index for '187'.
[indexDocument][L2] Indexing document '1055'.
[indexDocument][L3] Opened file '1055'.
[preprocess][L2] Starting preprocessing for '1055'.
[preprocess][L3] 1055: Tokenized into 120 tokens.
[preprocess][L3] 1055: Removed stop words; 83 tokens remain.
[preprocess][L3] 1055: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1055'; 83 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1055'.
[indexDocument][L3] Recorded doc length 83 for '1055'.
[indexDocument][L3] Tokens added to the inverted index for '1055'.
[indexDocument][L2] Indexing document '528'.
[indexDocument][L3] Opened file '528'.
[preprocess][L2] Starting preprocessing for '528'.
[preprocess][L3] 528: Tokenized into 145 tokens.
[preprocess][L3] 528: Removed stop words; 93 tokens remain.
[preprocess][L3] 528: Stemming complete.
[indexDocument][L3] Preprocessing complete for '528'; 93 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '528'.
[indexDocument][L3] Recorded doc length 93 for '528'.
[indexDocument][L3] Tokens added to the inverted index for '528'.
[indexDocument][L2] Indexing document '970'.
[indexDocument][L3] Opened file '970'.
[preprocess][L2] Starting preprocessing for '970'.
[preprocess][L3] 970: Tokenized into 186 tokens.
[preprocess][L3] 970: Removed stop words; 120 tokens remain.
[preprocess][L3] 970: Stemming complete.
[indexDocument][L3] Preprocessing complete for '970'; 120 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '970'.
[indexDocument][L3] Recorded doc length 120 for '970'.
[indexDocument][L3] Tokens added to the inverted index for '970'.
[indexDocument][L2] Indexing document '984'.
[indexDocument][L3] Opened file '984'.
[preprocess][L2] Starting preprocessing for '984'.
[preprocess][L3] 984: Tokenized into 130 tokens.
[preprocess][L3] 984: Removed stop words; 78 tokens remain.
[preprocess][L3] 984: Stemming complete.
[indexDocument][L3] Preprocessing complete for '984'; 78 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '984'.
[indexDocument][L3] Recorded doc length 78 for '984'.
[indexDocument][L3] Tokens added to the inverted index for '984'.
[indexDocument][L2] Indexing document '1258'.
[indexDocument][L3] Opened file '1258'.
[preprocess][L2] Starting preprocessing for '1258'.
[preprocess][L3] 1258: Tokenized into 193 tokens.
[preprocess][L3] 1258: Removed stop words; 131 tokens remain.
[preprocess][L3] 1258: Stemming complete.
[indexDocument][L3] Preprocessing complete for '1258'; 131 tokens obtained.
[add_tf_idf][L2] Adding tokens for document '1258'.
[indexDocument][L3] Recorded doc length 131 for '1258'.
[indexDocument][L3] Tokens added to the inverted index for '1258'.
[main][L1] Indexing complete. Vocabulary size = 9415
[construct_weights][L2] Constructing BM25 weights with k1=1.5, b=0.75, avgdl=100.79.
[construct_weights][L3] Term '1223': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inviscid-incompressible-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'theori': DF = 452, IDF = 1.1302
[construct_weights][L3] Term 'static': DF = 84, IDF = 2.8082
[construct_weights][L3] Term 'two-dimension': DF = 155, IDF = 2.1983
[construct_weights][L3] Term 'solid': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'jet': DF = 96, IDF = 2.6754
[construct_weights][L3] Term 'proxim': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'ground': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'strand': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 't': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'j': DF = 500, IDF = 1.0293
[construct_weights][L3] Term 'ae': DF = 306, IDF = 1.5197
[construct_weights][L3] Term 'sc': DF = 343, IDF = 1.4058
[construct_weights][L3] Term '1962': DF = 218, IDF = 1.8582
[construct_weights][L3] Term '170': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'imping': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'orthogon': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'present': DF = 508, IDF = 1.0135
[construct_weights][L3] Term 'us': DF = 516, IDF = 0.9979
[construct_weights][L3] Term 'conform': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'map': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'method': DF = 454, IDF = 1.1257
[construct_weights][L3] Term 'shown': DF = 285, IDF = 1.5907
[construct_weights][L3] Term 'thrust': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'constant': DF = 171, IDF = 2.1004
[construct_weights][L3] Term 'power': DF = 83, IDF = 2.8201
[construct_weights][L3] Term 'initi': DF = 109, IDF = 2.5490
[construct_weights][L3] Term 'decreas': DF = 89, IDF = 2.7507
[construct_weights][L3] Term 'approach': DF = 96, IDF = 2.6754
[construct_weights][L3] Term 'magnitud': DF = 68, IDF = 3.0181
[construct_weights][L3] Term 'effect': DF = 539, IDF = 0.9543
[construct_weights][L3] Term 'regain': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'low': DF = 112, IDF = 2.5220
[construct_weights][L3] Term 'height-to-jet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'width': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'ratio': DF = 288, IDF = 1.5802
[construct_weights][L3] Term 'approxim': DF = 377, IDF = 1.3114
[construct_weights][L3] Term '0': DF = 217, IDF = 1.8627
[construct_weights][L3] Term '55': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'maximuin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6': DF = 106, IDF = 2.5768
[construct_weights][L3] Term 'percent': DF = 60, IDF = 3.1423
[construct_weights][L3] Term 'thu': DF = 77, IDF = 2.8947
[construct_weights][L3] Term 'larg': DF = 201, IDF = 1.9392
[construct_weights][L3] Term 'unfavor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1011': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'free-flight': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'measur': DF = 252, IDF = 1.7135
[construct_weights][L3] Term 'dynam': DF = 77, IDF = 2.8947
[construct_weights][L3] Term 'chart': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'prepar': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'relat': DF = 150, IDF = 2.2310
[construct_weights][L3] Term 'thermodynam': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'properti': DF = 109, IDF = 2.5490
[construct_weights][L3] Term 'air': DF = 180, IDF = 2.0492
[construct_weights][L3] Term 'chemic': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'equilibrium': DF = 79, IDF = 2.8692
[construct_weights][L3] Term 'temperatur': DF = 257, IDF = 1.6939
[construct_weights][L3] Term '15': DF = 54, IDF = 3.2467
[construct_weights][L3] Term '000': DF = 50, IDF = 3.3230
[construct_weights][L3] Term 'k': DF = 49, IDF = 3.3430
[construct_weights][L3] Term 'pressur': DF = 533, IDF = 0.9655
[construct_weights][L3] Term '10': DF = 135, IDF = 2.3360
[construct_weights][L3] Term 'atmospher': DF = 76, IDF = 2.9077
[construct_weights][L3] Term 'also': DF = 308, IDF = 1.5132
[construct_weights][L3] Term 'includ': DF = 225, IDF = 1.8266
[construct_weights][L3] Term 'show': DF = 202, IDF = 1.9342
[construct_weights][L3] Term 'composit': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'isentrop': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'expon': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'speed': DF = 247, IDF = 1.7335
[construct_weights][L3] Term 'sound': DF = 36, IDF = 3.6476
[construct_weights][L3] Term 'base': DF = 229, IDF = 1.8090
[construct_weights][L3] Term 'data': DF = 217, IDF = 1.8627
[construct_weights][L3] Term 'calcul': DF = 328, IDF = 1.4504
[construct_weights][L3] Term 'nation': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'bureau': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'standard': DF = 21, IDF = 4.1769
[construct_weights][L3] Term '795': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'oper': DF = 67, IDF = 3.0328
[construct_weights][L3] Term 'npl': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '18in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x': DF = 78, IDF = 2.8818
[construct_weights][L3] Term '14in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wind': DF = 98, IDF = 2.6549
[construct_weights][L3] Term 'tunnel': DF = 151, IDF = 2.2244
[construct_weights][L3] Term 'transon': DF = 62, IDF = 3.1098
[construct_weights][L3] Term 'rang': DF = 295, IDF = 1.5563
[construct_weights][L3] Term 'hall': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'i.m.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'a.r.c.': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'c.p.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '338': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'januari': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '1957': DF = 83, IDF = 2.8201
[construct_weights][L3] Term 'brief': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'descript': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'slot': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'liner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'given': DF = 386, IDF = 1.2878
[construct_weights][L3] Term 'togeth': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'requir': DF = 145, IDF = 2.2648
[construct_weights][L3] Term 'flow': DF = 715, IDF = 0.6720
[construct_weights][L3] Term 'survei': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'observ': DF = 97, IDF = 2.6651
[construct_weights][L3] Term 'made': DF = 352, IDF = 1.3799
[construct_weights][L3] Term 'wall': DF = 165, IDF = 2.1360
[construct_weights][L3] Term 'interfer': DF = 45, IDF = 3.4272
[construct_weights][L3] Term 'half-model': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'swept': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'wing': DF = 216, IDF = 1.8674
[construct_weights][L3] Term 'test': DF = 273, IDF = 1.6336
[construct_weights][L3] Term '553': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ablat': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'glassi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'materi': DF = 68, IDF = 3.0181
[construct_weights][L3] Term 'around': DF = 55, IDF = 3.2286
[construct_weights][L3] Term 'blunt': DF = 119, IDF = 2.4616
[construct_weights][L3] Term 'bodi': DF = 275, IDF = 1.6264
[construct_weights][L3] Term 'revolut': DF = 77, IDF = 2.8947
[construct_weights][L3] Term 'hidalgo': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'h': DF = 75, IDF = 2.9208
[construct_weights][L3] Term 'ar': DF = 23, IDF = 4.0879
[construct_weights][L3] Term '30': DF = 41, IDF = 3.5192
[construct_weights][L3] Term '1960': DF = 146, IDF = 2.2579
[construct_weights][L3] Term 'steady-st': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'equat': DF = 401, IDF = 1.2497
[construct_weights][L3] Term 'motion': DF = 119, IDF = 2.4616
[construct_weights][L3] Term 'thin': DF = 119, IDF = 2.4616
[construct_weights][L3] Term 'layer': DF = 358, IDF = 1.3630
[construct_weights][L3] Term 'incompress': DF = 128, IDF = 2.3890
[construct_weights][L3] Term 'surfac': DF = 321, IDF = 1.4719
[construct_weights][L3] Term 'radiat': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'reduc': DF = 127, IDF = 2.3968
[construct_weights][L3] Term 'first-ord': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'ordinari': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'differenti': DF = 105, IDF = 2.5862
[construct_weights][L3] Term 'integr': DF = 144, IDF = 2.2717
[construct_weights][L3] Term 'numer': DF = 200, IDF = 1.9441
[construct_weights][L3] Term 'solut': DF = 407, IDF = 1.2349
[construct_weights][L3] Term 'coupl': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'boundari': DF = 413, IDF = 1.2203
[construct_weights][L3] Term 'laminar': DF = 231, IDF = 1.8004
[construct_weights][L3] Term 'turbul': DF = 139, IDF = 2.3069
[construct_weights][L3] Term 'heat': DF = 280, IDF = 1.6084
[construct_weights][L3] Term 'transfer': DF = 181, IDF = 2.0437
[construct_weights][L3] Term 'without': DF = 76, IDF = 2.9077
[construct_weights][L3] Term 'mass': DF = 65, IDF = 3.0629
[construct_weights][L3] Term 'vapor': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'distribut': DF = 359, IDF = 1.3602
[construct_weights][L3] Term 'energi': DF = 93, IDF = 2.7070
[construct_weights][L3] Term 'obtain': DF = 464, IDF = 1.1040
[construct_weights][L3] Term 'cone': DF = 107, IDF = 2.5675
[construct_weights][L3] Term 'cylind': DF = 153, IDF = 2.2112
[construct_weights][L3] Term 'hemispher': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'cap': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 're-ent': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'hyperson': DF = 169, IDF = 2.1121
[construct_weights][L3] Term 'flight': DF = 124, IDF = 2.4206
[construct_weights][L3] Term 'quartz': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'found': DF = 322, IDF = 1.4688
[construct_weights][L3] Term 'process': DF = 71, IDF = 2.9752
[construct_weights][L3] Term 'effici': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'case': DF = 366, IDF = 1.3409
[construct_weights][L3] Term 'increas': DF = 206, IDF = 1.9146
[construct_weights][L3] Term 'stagnat': DF = 121, IDF = 2.4450
[construct_weights][L3] Term 'point': DF = 221, IDF = 1.8445
[construct_weights][L3] Term 'verifi': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'experi': DF = 152, IDF = 2.2178
[construct_weights][L3] Term 'state': DF = 60, IDF = 3.1423
[construct_weights][L3] Term 'develop': DF = 265, IDF = 1.6633
[construct_weights][L3] Term 'permit': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'experiment': DF = 337, IDF = 1.4234
[construct_weights][L3] Term 'investig': DF = 361, IDF = 1.3547
[construct_weights][L3] Term '761': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'buckl': DF = 125, IDF = 2.4126
[construct_weights][L3] Term 'sandwich': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'normal': DF = 117, IDF = 2.4785
[construct_weights][L3] Term 'yao': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.c.': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '29': DF = 93, IDF = 2.7070
[construct_weights][L3] Term '264': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'theoret': DF = 235, IDF = 1.7832
[construct_weights][L3] Term 'studi': DF = 239, IDF = 1.7664
[construct_weights][L3] Term 'sphere': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'compris': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'core': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'low-modulu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two': DF = 302, IDF = 1.5329
[construct_weights][L3] Term 'face': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'higher': DF = 102, IDF = 2.6151
[construct_weights][L3] Term 'modulu': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'resist': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'extern': DF = 73, IDF = 2.9477
[construct_weights][L3] Term 'linear': DF = 135, IDF = 2.3360
[construct_weights][L3] Term 'classic': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'monocoqu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'spheric': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'shell': DF = 85, IDF = 2.7964
[construct_weights][L3] Term 'critic': DF = 75, IDF = 2.9208
[construct_weights][L3] Term 'variou': DF = 160, IDF = 2.1666
[construct_weights][L3] Term 'radius-thick': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '305': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'strong': DF = 38, IDF = 3.5943
[construct_weights][L3] Term 'viscou': DF = 119, IDF = 2.4616
[construct_weights][L3] Term 'interact': DF = 87, IDF = 2.7733
[construct_weights][L3] Term 'flat': DF = 167, IDF = 2.1240
[construct_weights][L3] Term 'plate': DF = 216, IDF = 1.8674
[construct_weights][L3] Term 'li': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 't.y.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'gross': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'j.f.': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'fluid': DF = 183, IDF = 2.0327
[construct_weights][L3] Term 'mech': DF = 79, IDF = 2.8692
[construct_weights][L3] Term 'inst': DF = 12, IDF = 4.7192
[construct_weights][L3] Term '1961': DF = 130, IDF = 2.3736
[construct_weights][L3] Term '146': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'report': DF = 112, IDF = 2.5220
[construct_weights][L3] Term 'give': DF = 144, IDF = 2.2717
[construct_weights][L3] Term 'account': DF = 82, IDF = 2.8321
[construct_weights][L3] Term 'problem': DF = 323, IDF = 1.4657
[construct_weights][L3] Term 'mass-transf': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'disturb': DF = 55, IDF = 3.2286
[construct_weights][L3] Term 'region': DF = 173, IDF = 2.0888
[construct_weights][L3] Term 'divid': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'inviscid': DF = 72, IDF = 2.9614
[construct_weights][L3] Term 'small': DF = 222, IDF = 1.8400
[construct_weights][L3] Term 'perturb': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'appli': DF = 207, IDF = 1.9098
[construct_weights][L3] Term 'similar': DF = 146, IDF = 2.2579
[construct_weights][L3] Term 'compress': DF = 194, IDF = 1.9745
[construct_weights][L3] Term 'treatment': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'law': DF = 50, IDF = 3.3230
[construct_weights][L3] Term 'deriv': DF = 212, IDF = 1.8860
[construct_weights][L3] Term 'veloc': DF = 310, IDF = 1.5068
[construct_weights][L3] Term 'match': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'formula': DF = 102, IDF = 2.6151
[construct_weights][L3] Term 'induc': DF = 56, IDF = 3.2107
[construct_weights][L3] Term 'thick': DF = 175, IDF = 2.0773
[construct_weights][L3] Term 'skin': DF = 72, IDF = 2.9614
[construct_weights][L3] Term 'friction': DF = 68, IDF = 3.0181
[construct_weights][L3] Term 'coeffici': DF = 223, IDF = 1.8355
[construct_weights][L3] Term 'result': DF = 692, IDF = 0.7046
[construct_weights][L3] Term 'signific': DF = 77, IDF = 2.8947
[construct_weights][L3] Term 'discuss': DF = 271, IDF = 1.6410
[construct_weights][L3] Term 'futur': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'improv': DF = 44, IDF = 3.4495
[construct_weights][L3] Term 'indic': DF = 214, IDF = 1.8766
[construct_weights][L3] Term '137': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'gener': DF = 331, IDF = 1.4413
[construct_weights][L3] Term 'aerodynam': DF = 176, IDF = 2.0716
[construct_weights][L3] Term 'mean': DF = 144, IDF = 2.2717
[construct_weights][L3] Term 'curl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'n': DF = 47, IDF = 3.3842
[construct_weights][L3] Term 'j.roy.ae.s.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '65': DF = 14, IDF = 4.5708
[construct_weights][L3] Term '724': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'summari': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'import': DF = 110, IDF = 2.5399
[construct_weights][L3] Term 'nois': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'cold': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'subson': DF = 112, IDF = 2.5220
[construct_weights][L3] Term 'predict': DF = 193, IDF = 1.9797
[construct_weights][L3] Term 'lighthil': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 's': DF = 230, IDF = 1.8047
[construct_weights][L3] Term '598': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'new': DF = 80, IDF = 2.8567
[construct_weights][L3] Term 'techniqu': DF = 100, IDF = 2.6348
[construct_weights][L3] Term 'hyperveloc': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'stalmach': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'c.j.': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'cooksei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.m.': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'aerospac': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'engin': DF = 53, IDF = 3.2653
[construct_weights][L3] Term 'vol': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '21': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'march': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'rocket': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'exhaust': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'vehicl': DF = 63, IDF = 3.0939
[construct_weights][L3] Term 'stabil': DF = 120, IDF = 2.4533
[construct_weights][L3] Term 'damp': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'arc-discharg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'type': DF = 160, IDF = 2.1666
[construct_weights][L3] Term 'sampl': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'qualiti': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'self-lumin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shadowgraph': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'photograph': DF = 21, IDF = 4.1769
[construct_weights][L3] Term '934': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cylindr': DF = 94, IDF = 2.6963
[construct_weights][L3] Term 'conic': DF = 66, IDF = 3.0477
[construct_weights][L3] Term 'circular': DF = 134, IDF = 2.3434
[construct_weights][L3] Term 'cross': DF = 48, IDF = 3.3634
[construct_weights][L3] Term 'section': DF = 132, IDF = 2.3584
[construct_weights][L3] Term 'simultan': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'action': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'axial': DF = 136, IDF = 2.3286
[construct_weights][L3] Term 'mushtari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'k.m.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sachenkov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.v.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'naca': DF = 190, IDF = 1.9953
[construct_weights][L3] Term 'tm': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '1433': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1958': DF = 89, IDF = 2.7507
[construct_weights][L3] Term 'consid': DF = 249, IDF = 1.7255
[construct_weights][L3] Term 'determin': DF = 299, IDF = 1.5428
[construct_weights][L3] Term 'upper': DF = 36, IDF = 3.6476
[construct_weights][L3] Term 'limit': DF = 134, IDF = 2.3434
[construct_weights][L3] Term 'load': DF = 193, IDF = 1.9797
[construct_weights][L3] Term 'forc': DF = 160, IDF = 2.1666
[construct_weights][L3] Term 'uniformli': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'plane': DF = 90, IDF = 2.7396
[construct_weights][L3] Term 'isotrop': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'start': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'neutral': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'ref': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '1': DF = 318, IDF = 1.4813
[construct_weights][L3] Term 'torsion': DF = 29, IDF = 3.8606
[construct_weights][L3] Term '2': DF = 222, IDF = 1.8400
[construct_weights][L3] Term 'upon': DF = 74, IDF = 2.9341
[construct_weights][L3] Term 'possibl': DF = 154, IDF = 2.2047
[construct_weights][L3] Term 'satisfi': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'condit': DF = 334, IDF = 1.4323
[construct_weights][L3] Term 'contrast': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '3': DF = 161, IDF = 2.1604
[construct_weights][L3] Term 'attent': DF = 36, IDF = 3.6476
[construct_weights][L3] Term 'paid': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'fulfil': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '4': DF = 121, IDF = 2.4450
[construct_weights][L3] Term 'part': DF = 105, IDF = 2.5862
[construct_weights][L3] Term 'accord': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'galerkin': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'longitudin': DF = 47, IDF = 3.3842
[construct_weights][L3] Term 'let': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'note': DF = 122, IDF = 2.4368
[construct_weights][L3] Term 'suggest': DF = 73, IDF = 2.9477
[construct_weights][L3] Term 'refer': DF = 57, IDF = 3.1932
[construct_weights][L3] Term '5': DF = 127, IDF = 2.3968
[construct_weights][L3] Term 'well': DF = 149, IDF = 2.2376
[construct_weights][L3] Term 'mai': DF = 242, IDF = 1.7539
[construct_weights][L3] Term 'lead': DF = 162, IDF = 2.1543
[construct_weights][L3] Term 'number': DF = 568, IDF = 0.9019
[construct_weights][L3] Term 'substanti': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'mistak': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '108': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'confluent': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hypergeometr': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'function': DF = 187, IDF = 2.0112
[construct_weights][L3] Term 'd': DF = 43, IDF = 3.4722
[construct_weights][L3] Term 'macdonald': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prove': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'mani': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'branch': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'physic': DF = 68, IDF = 3.0181
[construct_weights][L3] Term 'involv': DF = 101, IDF = 2.6249
[construct_weights][L3] Term 'diffus': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'sediment': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'isotop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'separ': DF = 150, IDF = 2.2310
[construct_weights][L3] Term 'protein': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'molecular': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'weight': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'ultracentrifug': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'electron': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'high': DF = 183, IDF = 2.0327
[construct_weights][L3] Term 'frequenc': DF = 50, IDF = 3.3230
[construct_weights][L3] Term 'ga': DF = 130, IDF = 2.3736
[construct_weights][L3] Term 'discharg': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'frequent': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'express': DF = 134, IDF = 2.3434
[construct_weights][L3] Term 'term': DF = 158, IDF = 2.1792
[construct_weights][L3] Term 'breakdown': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'electr': DF = 42, IDF = 3.4954
[construct_weights][L3] Term 'field': DF = 166, IDF = 2.1299
[construct_weights][L3] Term 'gase': DF = 38, IDF = 3.5943
[construct_weights][L3] Term 'kinet': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'six-figur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tabl': DF = 37, IDF = 3.6206
[construct_weights][L3] Term '130': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'behaviour': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'non-linear': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'system': DF = 86, IDF = 2.7848
[construct_weights][L3] Term 'clauser': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.h.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.aero.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '23': DF = 28, IDF = 3.8950
[construct_weights][L3] Term '411': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'phenomena': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'occur': DF = 124, IDF = 2.4206
[construct_weights][L3] Term 'world': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'govern': DF = 44, IDF = 3.4495
[construct_weights][L3] Term 'nonlinear': DF = 44, IDF = 3.4495
[construct_weights][L3] Term 'relationship': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'mathemat': DF = 42, IDF = 3.4954
[construct_weights][L3] Term 'scienc': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'difficulti': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'analysi': DF = 276, IDF = 1.6227
[construct_weights][L3] Term 'hinder': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'formul': DF = 52, IDF = 3.2841
[construct_weights][L3] Term 'concept': DF = 52, IDF = 3.2841
[construct_weights][L3] Term 'understand': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'articl': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'progress': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'behavior': DF = 61, IDF = 3.1259
[construct_weights][L3] Term 'review': DF = 52, IDF = 3.2841
[construct_weights][L3] Term 'attempt': DF = 36, IDF = 3.6476
[construct_weights][L3] Term 'wai': DF = 36, IDF = 3.6476
[construct_weights][L3] Term '302': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'transport': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'hansen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'c': DF = 50, IDF = 3.3230
[construct_weights][L3] Term 'f': DF = 44, IDF = 3.4495
[construct_weights][L3] Term 'nasa': DF = 145, IDF = 2.2648
[construct_weights][L3] Term 'tr': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'r': DF = 121, IDF = 2.4450
[construct_weights][L3] Term '50': DF = 29, IDF = 3.8606
[construct_weights][L3] Term '1959': DF = 130, IDF = 2.3736
[construct_weights][L3] Term 'high-temperatur': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'close': DF = 82, IDF = 2.8321
[construct_weights][L3] Term 'form': DF = 226, IDF = 1.8222
[construct_weights][L3] Term 'partit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'major': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'compon': DF = 60, IDF = 3.1423
[construct_weights][L3] Term 'neglect': DF = 45, IDF = 3.4272
[construct_weights][L3] Term 'minor': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'enthalpi': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'entropi': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'specif': DF = 61, IDF = 3.1259
[construct_weights][L3] Term 'viscos': DF = 59, IDF = 3.1590
[construct_weights][L3] Term 'thermal': DF = 85, IDF = 2.7964
[construct_weights][L3] Term 'conduct': DF = 153, IDF = 2.2112
[construct_weights][L3] Term 'prandtl': DF = 53, IDF = 3.2653
[construct_weights][L3] Term 'tabul': DF = 26, IDF = 3.9678
[construct_weights][L3] Term '500degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '000degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '0001': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '100': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'mol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fraction': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'can': DF = 289, IDF = 1.5768
[construct_weights][L3] Term 'valu': DF = 306, IDF = 1.5197
[construct_weights][L3] Term 'fulli': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'ioniz': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'complet': DF = 85, IDF = 2.7964
[construct_weights][L3] Term 'will': DF = 145, IDF = 2.2648
[construct_weights][L3] Term 'becom': DF = 52, IDF = 3.2841
[construct_weights][L3] Term 'compar': DF = 247, IDF = 1.7335
[construct_weights][L3] Term 'uniti': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'order': DF = 150, IDF = 2.2310
[construct_weights][L3] Term 'transpar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flux': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '766': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mach': DF = 386, IDF = 1.2878
[construct_weights][L3] Term 'stress': DF = 148, IDF = 2.2444
[construct_weights][L3] Term 'flutter': DF = 56, IDF = 3.2107
[construct_weights][L3] Term 'characterist': DF = 184, IDF = 2.0273
[construct_weights][L3] Term 'single-bai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'panel': DF = 45, IDF = 3.4272
[construct_weights][L3] Term 'length-width': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '96': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'dixon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 's.c.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'tn': DF = 217, IDF = 1.8627
[construct_weights][L3] Term 'd1485': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stiffen': DF = 29, IDF = 3.8606
[construct_weights][L3] Term '500': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '300': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'support': DF = 66, IDF = 3.0477
[construct_weights][L3] Term 'structur': DF = 107, IDF = 2.5675
[construct_weights][L3] Term 'allow': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'partial': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'expans': DF = 63, IDF = 3.0939
[construct_weights][L3] Term 'later': DF = 46, IDF = 3.4055
[construct_weights][L3] Term 'direct': DF = 120, IDF = 2.4533
[construct_weights][L3] Term 'vari': DF = 144, IDF = 2.2717
[construct_weights][L3] Term 'fair': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'consist': DF = 76, IDF = 2.9077
[construct_weights][L3] Term 'flat-panel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'portion': DF = 56, IDF = 3.2107
[construct_weights][L3] Term 'buckled-panel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'transit': DF = 93, IDF = 2.7070
[construct_weights][L3] Term 'intersect': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'suscept': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'fairli': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'distinct': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'large-amplitud': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'small-amplitud': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trend': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'revers': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'modifi': DF = 74, IDF = 2.9341
[construct_weights][L3] Term 'paramet': DF = 153, IDF = 2.2112
[construct_weights][L3] Term 'variat': DF = 139, IDF = 2.3069
[construct_weights][L3] Term 'scatter': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'exact': DF = 106, IDF = 2.5768
[construct_weights][L3] Term 'clamp': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'zero': DF = 126, IDF = 2.4047
[construct_weights][L3] Term 'midplan': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'addit': DF = 96, IDF = 2.6754
[construct_weights][L3] Term 'twomod': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transtabl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '554': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'graph': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'detra': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'r.w.': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'kidalgo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'util': DF = 36, IDF = 3.6476
[construct_weights][L3] Term 'research': DF = 74, IDF = 2.9341
[construct_weights][L3] Term 'previous': DF = 56, IDF = 3.2107
[construct_weights][L3] Term 'radi': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'dissoci': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'momentum': DF = 52, IDF = 3.2841
[construct_weights][L3] Term 'reynold': DF = 198, IDF = 1.9542
[construct_weights][L3] Term 'axisymmetr': DF = 53, IDF = 3.2653
[construct_weights][L3] Term 'correl': DF = 85, IDF = 2.7964
[construct_weights][L3] Term 'valid': DF = 70, IDF = 2.9893
[construct_weights][L3] Term '6000': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '26': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'fp': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'altitud': DF = 43, IDF = 3.4722
[construct_weights][L3] Term '250': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'ft': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'cover': DF = 55, IDF = 3.2286
[construct_weights][L3] Term 're-entri': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'regim': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'practic': DF = 100, IDF = 2.6348
[construct_weights][L3] Term 'trajectori': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'interest': DF = 65, IDF = 3.0629
[construct_weights][L3] Term 'todai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'last': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'special': DF = 75, IDF = 2.9208
[construct_weights][L3] Term 'icbm': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nose': DF = 87, IDF = 2.7733
[construct_weights][L3] Term 'applic': DF = 219, IDF = 1.8536
[construct_weights][L3] Term 'make': DF = 50, IDF = 3.3230
[construct_weights][L3] Term 'rapid': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'estim': DF = 106, IDF = 2.5768
[construct_weights][L3] Term 'preliminari': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'design': DF = 141, IDF = 2.2926
[construct_weights][L3] Term 'evalu': DF = 70, IDF = 2.9893
[construct_weights][L3] Term 'associ': DF = 71, IDF = 2.9752
[construct_weights][L3] Term 'earth': DF = 21, IDF = 4.1769
[construct_weights][L3] Term '792': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aircraft': DF = 76, IDF = 2.9077
[construct_weights][L3] Term 'spenc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'clean': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'royal': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'aero': DF = 89, IDF = 2.7507
[construct_weights][L3] Term 'soc': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'v': DF = 71, IDF = 2.9752
[construct_weights][L3] Term '66': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'april': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'pp': DF = 24, IDF = 4.0463
[construct_weights][L3] Term '211-225': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'first': DF = 156, IDF = 2.1919
[construct_weights][L3] Term 'paper': DF = 206, IDF = 1.9146
[construct_weights][L3] Term 'deal': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'shape': DF = 189, IDF = 2.0006
[construct_weights][L3] Term 'kuchemann': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'second': DF = 83, IDF = 2.8201
[construct_weights][L3] Term 'intern': DF = 44, IDF = 3.4495
[construct_weights][L3] Term 'congress': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'aeronaut': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'zurich': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'suitabl': DF = 45, IDF = 3.4272
[construct_weights][L3] Term 'achiev': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'superson': DF = 269, IDF = 1.6484
[construct_weights][L3] Term 'name': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'wingbodi': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'arrang': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'sweepback': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'angl': DF = 201, IDF = 1.9392
[construct_weights][L3] Term '55degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '60degre': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'streamwis': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'thickness-chord': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'per': DF = 82, IDF = 2.8321
[construct_weights][L3] Term 'cent': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'slender': DF = 87, IDF = 2.7733
[construct_weights][L3] Term 'near-triangular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sharp': DF = 42, IDF = 3.4954
[construct_weights][L3] Term 'edg': DF = 168, IDF = 2.1180
[construct_weights][L3] Term 'slew': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lift': DF = 142, IDF = 2.2856
[construct_weights][L3] Term 'variabl': DF = 78, IDF = 2.8818
[construct_weights][L3] Term 'geometri': DF = 47, IDF = 3.3842
[construct_weights][L3] Term 'briefli': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'need': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'avoid': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'demonstr': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'conclus': DF = 53, IDF = 3.2653
[construct_weights][L3] Term 'desir': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'flap': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'blow': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'suction': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'knee': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blown': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trail': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'describ': DF = 154, IDF = 2.2047
[construct_weights][L3] Term 'simplifi': DF = 59, IDF = 3.1590
[construct_weights][L3] Term 'model': DF = 178, IDF = 2.0604
[construct_weights][L3] Term 'control': DF = 58, IDF = 3.1759
[construct_weights][L3] Term 'mention': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'advers': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'maximum': DF = 111, IDF = 2.5309
[construct_weights][L3] Term 'steadi': DF = 86, IDF = 2.7848
[construct_weights][L3] Term 'therefor': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'view': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'buffet': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'plan': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'pitch-up': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'analys': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'short': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'avail': DF = 81, IDF = 2.8443
[construct_weights][L3] Term 'pitch': DF = 56, IDF = 3.2107
[construct_weights][L3] Term 'roll': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'moment': DF = 89, IDF = 2.7507
[construct_weights][L3] Term 'due': DF = 143, IDF = 2.2786
[construct_weights][L3] Term 'sideslip': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'rise': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'seriou': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'knowledg': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'yaw': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'rotari': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'final': DF = 61, IDF = 3.1259
[construct_weights][L3] Term 'summaris': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'concern': DF = 65, IDF = 3.0629
[construct_weights][L3] Term 'work': DF = 112, IDF = 2.5220
[construct_weights][L3] Term 'aim': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'clarifi': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'handl': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'much': DF = 66, IDF = 3.0477
[construct_weights][L3] Term 'forecast': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pilot': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'want': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'aspect': DF = 74, IDF = 2.9341
[construct_weights][L3] Term 'vertic': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'detail': DF = 105, IDF = 2.5862
[construct_weights][L3] Term 'glide': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'path': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'hold': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'avro': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '707a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'artifici': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'worsen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'perform': DF = 81, IDF = 2.8443
[construct_weights][L3] Term 'task': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'expens': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'effort': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'tent': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'level': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'phugoid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nevertheless': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'drawn': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'statu': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'mainli': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'american': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1016': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'principl': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'creep': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'weight-strength': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'time-depend': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'tangent-modulu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'conceiv': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'shanlei': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'actual': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'column': DF = 36, IDF = 3.6476
[construct_weights][L3] Term 'capac': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'interpret': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'conserv': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'tangentmodulu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'howev': DF = 110, IDF = 2.5399
[construct_weights][L3] Term 'either': DF = 71, IDF = 2.9752
[construct_weights][L3] Term 'nonconserv': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'imperfect': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'real': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'typic': DF = 38, IDF = 3.5943
[construct_weights][L3] Term 'alloi': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'cite': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1224': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'plk': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blunt-bodi': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'vaglio-laurin': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '185': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'portiou': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'asymmetr': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'configur': DF = 81, IDF = 2.8443
[construct_weights][L3] Term 'success': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'carri': DF = 74, IDF = 2.9341
[construct_weights][L3] Term 'systemat': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'fashion': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'appropri': DF = 44, IDF = 3.4495
[construct_weights][L3] Term 'converg': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'procedur': DF = 88, IDF = 2.7619
[construct_weights][L3] Term 'produc': DF = 76, IDF = 2.9077
[construct_weights][L3] Term 'refin': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'set': DF = 66, IDF = 3.0477
[construct_weights][L3] Term 'invers': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'follow': DF = 77, IDF = 2.8947
[construct_weights][L3] Term 'reason': DF = 72, IDF = 2.9614
[construct_weights][L3] Term 'shock': DF = 222, IDF = 1.8400
[construct_weights][L3] Term 'on': DF = 233, IDF = 1.7918
[construct_weights][L3] Term 'degre': DF = 46, IDF = 3.4055
[construct_weights][L3] Term 'qm': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stretch': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'coordin': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'spirit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'poincare-lighthill-kuo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transform': DF = 62, IDF = 3.1098
[construct_weights][L3] Term 'along': DF = 137, IDF = 2.3213
[construct_weights][L3] Term 'intermedi': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'line': DF = 69, IDF = 3.0036
[construct_weights][L3] Term 'annul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'local': DF = 121, IDF = 2.4450
[construct_weights][L3] Term 'b': DF = 50, IDF = 3.3230
[construct_weights][L3] Term 'strip': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'shift': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'justifi': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'consider': DF = 159, IDF = 2.1729
[construct_weights][L3] Term 'propos': DF = 46, IDF = 3.4055
[construct_weights][L3] Term 'assess': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'comparison': DF = 140, IDF = 2.2997
[construct_weights][L3] Term 'disk': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'low-termperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stream': DF = 153, IDF = 2.2112
[construct_weights][L3] Term 'm': DF = 146, IDF = 2.2579
[construct_weights][L3] Term '76': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'highli': DF = 26, IDF = 3.9678
[construct_weights][L3] Term '8': DF = 89, IDF = 2.7507
[construct_weights][L3] Term '759': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ogiv': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'flare': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'afterbodi': DF = 28, IDF = 3.8950
[construct_weights][L3] Term '85': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'coltran': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'l.c.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'd1506': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tip': DF = 22, IDF = 4.1314
[construct_weights][L3] Term '20': DF = 65, IDF = 3.0629
[construct_weights][L3] Term 'free': DF = 104, IDF = 2.5958
[construct_weights][L3] Term 'time': DF = 139, IDF = 2.3069
[construct_weights][L3] Term 'histori': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'plot': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'longitudinal-forc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'center': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'normal-force-curv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slope': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'center-of-grav': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'locat': DF = 80, IDF = 2.8567
[construct_weights][L3] Term 'stabl': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'throughout': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'averag': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'move': DF = 59, IDF = 3.1590
[construct_weights][L3] Term 'slightli': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'forward': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'remain': DF = 50, IDF = 3.3230
[construct_weights][L3] Term 'drag': DF = 130, IDF = 2.3736
[construct_weights][L3] Term 'area': DF = 52, IDF = 3.2841
[construct_weights][L3] Term '933': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rough': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'insect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'past': DF = 84, IDF = 2.8082
[construct_weights][L3] Term 'airfoil': DF = 65, IDF = 3.0629
[construct_weights][L3] Term 'coleman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.s.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'advanc': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'boundary-lay': DF = 149, IDF = 2.2376
[construct_weights][L3] Term 'mainten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'extens': DF = 77, IDF = 2.8947
[construct_weights][L3] Term 'irregular': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'rivet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'head': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'lap': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'joint': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'window': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'etc': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'kind': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'impact': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'natur': DF = 57, IDF = 3.1932
[construct_weights][L3] Term 'roughen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'though': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'subject': DF = 98, IDF = 2.6549
[construct_weights][L3] Term 'phenomenon': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'wind-tunnel': DF = 38, IDF = 3.5943
[construct_weights][L3] Term 'fruit': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'fly': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'drosophila': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'common': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'housefli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'former': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'treat': DF = 79, IDF = 2.8692
[construct_weights][L3] Term 'yet': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'publish': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'agreement': DF = 181, IDF = 2.0437
[construct_weights][L3] Term 'satisfactori': DF = 49, IDF = 3.3430
[construct_weights][L3] Term 'essenti': DF = 60, IDF = 3.1423
[construct_weights][L3] Term 'profil': DF = 108, IDF = 2.5582
[construct_weights][L3] Term 'princip': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'pronounc': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'peak': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'near': DF = 105, IDF = 2.5862
[construct_weights][L3] Term 'gradual': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'diminish': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'excresc': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'height': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'sever': DF = 131, IDF = 2.3659
[construct_weights][L3] Term 'leading-edg': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'elimin': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'downstream': DF = 70, IDF = 2.9893
[construct_weights][L3] Term 'caus': DF = 59, IDF = 3.1590
[construct_weights][L3] Term 'passag': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'i.e.': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'smooth': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'moreov': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'appear': DF = 95, IDF = 2.6858
[construct_weights][L3] Term 'defin': DF = 48, IDF = 3.3634
[construct_weights][L3] Term 'upstream': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'insignific': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'fundament': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'three-dimension': DF = 52, IDF = 3.2841
[construct_weights][L3] Term 'charact': DF = 20, IDF = 4.2245
[construct_weights][L3] Term '1029': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'theorem': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'undergo': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'beam': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'calculu': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'conjunct': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'illustr': DF = 72, IDF = 2.9614
[construct_weights][L3] Term '750': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'dimension': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'symmetr': DF = 75, IDF = 2.9208
[construct_weights][L3] Term 'nozzl': DF = 80, IDF = 2.8567
[construct_weights][L3] Term 'arc': DF = 76, IDF = 2.9077
[construct_weights][L3] Term '347': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'radiu': DF = 45, IDF = 3.4272
[construct_weights][L3] Term 'curvatur': DF = 48, IDF = 3.3634
[construct_weights][L3] Term 'throat': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'half-height': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'convergent-diverg': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'three': DF = 85, IDF = 2.7964
[construct_weights][L3] Term 'seri': DF = 89, IDF = 2.7507
[construct_weights][L3] Term 'axially-symmetr': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'accuraci': DF = 75, IDF = 2.9208
[construct_weights][L3] Term 'confirm': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'known': DF = 75, IDF = 2.9208
[construct_weights][L3] Term 'branchlin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '562': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'howarth': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'l': DF = 54, IDF = 3.2467
[construct_weights][L3] Term 'proc': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'roi': DF = 25, IDF = 4.0063
[construct_weights][L3] Term '194': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1948': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'assum': DF = 158, IDF = 2.1792
[construct_weights][L3] Term 'proport': DF = 36, IDF = 3.6476
[construct_weights][L3] Term 'absolut': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'assumpt': DF = 108, IDF = 2.5582
[construct_weights][L3] Term 'empir': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'cope': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ordin': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'multipl': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'factor': DF = 80, IDF = 2.8567
[construct_weights][L3] Term 'g': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'greater': DF = 68, IDF = 3.0181
[construct_weights][L3] Term 'tend': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'outsid': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'particular': DF = 109, IDF = 2.5490
[construct_weights][L3] Term 'acceler': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'linearli': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'retard': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'implic': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'qualit': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'mainstream': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'subsequ': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'fall': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'conclud': DF = 46, IDF = 3.4055
[construct_weights][L3] Term 'earlier': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'kinemat': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'correspond': DF = 110, IDF = 2.5399
[construct_weights][L3] Term '106': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transvers': DF = 60, IDF = 3.1423
[construct_weights][L3] Term 'potenti': DF = 56, IDF = 3.2107
[construct_weights][L3] Term 'campbel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'i.j.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'q.j.mech.app.math.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '9': DF = 52, IDF = 3.2841
[construct_weights][L3] Term '1956': DF = 73, IDF = 2.9477
[construct_weights][L3] Term '140': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'axi': DF = 44, IDF = 3.4495
[construct_weights][L3] Term 'right': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'perpendicular': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'meridian': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'azimuth': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'round': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'simpl': DF = 183, IDF = 2.0327
[construct_weights][L3] Term 'manner': DF = 58, IDF = 3.1759
[construct_weights][L3] Term 'entir': DF = 44, IDF = 3.4495
[construct_weights][L3] Term 'elementari': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '334': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'influenc': DF = 101, IDF = 2.6249
[construct_weights][L3] Term 'wave': DF = 194, IDF = 1.9745
[construct_weights][L3] Term 'lester': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'lee': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'california': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'institut': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'technolog': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'bring': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'semi-infinit': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'insul': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'streamlin': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'enter': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'curv': DF = 114, IDF = 2.5044
[construct_weights][L3] Term 'consequ': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'outer': DF = 42, IDF = 3.4954
[construct_weights][L3] Term 'appreci': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'free-stream': DF = 93, IDF = 2.7070
[construct_weights][L3] Term 'vortic': DF = 49, IDF = 3.3430
[construct_weights][L3] Term 'shock-wav': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'larger': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'usual': DF = 63, IDF = 3.0939
[construct_weights][L3] Term 'error': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'within': DF = 107, IDF = 2.5675
[construct_weights][L3] Term 'framework': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'hammitt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bogdonoff': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'helium': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'care': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '1020': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'short-tim': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'creepbuckl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'creep-bend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '111': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '600': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'strain': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'taken': DF = 59, IDF = 3.1590
[construct_weights][L3] Term 'electric-resist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gage': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'appendix': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'rapidli': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'deviat': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'straight': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'bend': DF = 66, IDF = 3.0477
[construct_weights][L3] Term 'rate': DF = 145, IDF = 2.2648
[construct_weights][L3] Term 'latter': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'semiempir': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'guid': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '1212': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'skin-frict': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'goddard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.e.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'program': DF = 23, IDF = 4.0879
[construct_weights][L3] Term '18-in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '20in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'propuls': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'laboratori': DF = 42, IDF = 3.4954
[construct_weights][L3] Term 'sand-grain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '98': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '54': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '70': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'size': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'quadrat': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'indirect': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'exactli': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'reduct': DF = 54, IDF = 3.2467
[construct_weights][L3] Term 'densiti': DF = 82, IDF = 2.8321
[construct_weights][L3] Term 'hydraul': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'equal': DF = 55, IDF = 3.2286
[construct_weights][L3] Term 'sublay': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'veociti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quantit': DF = 22, IDF = 4.1314
[construct_weights][L3] Term '596': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'flexur': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'pivot': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'wittrick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.h.': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'quart': DF = 18, IDF = 4.3272
[construct_weights][L3] Term '1950-1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rotat': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'stiff': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'radic': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'chang': DF = 128, IDF = 2.3890
[construct_weights][L3] Term 'simpli': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'torqu': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'extent': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'take': DF = 55, IDF = 3.2286
[construct_weights][L3] Term 'movement': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'centr': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'purpos': DF = 76, IDF = 2.9077
[construct_weights][L3] Term '905': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pitot-stat': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'tube': DF = 64, IDF = 3.0783
[construct_weights][L3] Term 'merriam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'k.g.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'spauld': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e.r.': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '546': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'seven': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'convent': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'open': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'inner-tub': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diamet': DF = 74, IDF = 2.9341
[construct_weights][L3] Term 'pressure-distribut': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'stem': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'submit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'auspic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'council': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '139': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pitot': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'mcmillan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.a.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '58': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '1954': DF = 40, IDF = 3.5436
[construct_weights][L3] Term '570': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'blunt-nos': DF = 26, IDF = 3.9678
[construct_weights][L3] Term '1000': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'p': DF = 68, IDF = 3.0181
[construct_weights][L3] Term 'undisturb': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'previou': DF = 51, IDF = 3.3034
[construct_weights][L3] Term 'disagr': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'hurd': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cheski': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shapiro': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'closer': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '591': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'choke': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'compressor': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'csanadi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g.t.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '637': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'august': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'pressure-ratio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'versu': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'inletmass-flow-coeffici': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'turbin': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'analog': DF = 51, IDF = 3.3034
[construct_weights][L3] Term 'laval': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'idea': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'extend': DF = 91, IDF = 2.7286
[construct_weights][L3] Term '1215': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'slip': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'particularli': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'cool': DF = 62, IDF = 3.1098
[construct_weights][L3] Term 'rott': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'lenard': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jump': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'oversimplifi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unrealist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shear': DF = 99, IDF = 2.6448
[construct_weights][L3] Term 'boundarylay': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'even': DF = 48, IDF = 3.3634
[construct_weights][L3] Term 'put': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'evid': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'agre': DF = 59, IDF = 3.1590
[construct_weights][L3] Term 'accur': DF = 75, IDF = 2.9208
[construct_weights][L3] Term 'neglig': DF = 36, IDF = 3.6476
[construct_weights][L3] Term 'low-reynolds-numb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'correct': DF = 67, IDF = 3.0328
[construct_weights][L3] Term '1027': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'phenomenolog': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'polycrystallin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'metal': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'equicohes': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'elast': DF = 79, IDF = 2.8692
[construct_weights][L3] Term 'provid': DF = 124, IDF = 2.4206
[construct_weights][L3] Term 'inform': DF = 44, IDF = 3.4495
[construct_weights][L3] Term 'ruptur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stress-strain': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'rapid-h': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7075-t6': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'aluminum-alloi': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'sheet': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'yield': DF = 74, IDF = 2.9341
[construct_weights][L3] Term 'four': DF = 41, IDF = 3.5192
[construct_weights][L3] Term '333': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'infinit': DF = 69, IDF = 3.0036
[construct_weights][L3] Term 'robert': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'whalen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'scientist': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'inc': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'buffalo': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'n.y.': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'combin': DF = 111, IDF = 2.5309
[construct_weights][L3] Term 'howorth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mangler': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'diagram': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'rel': DF = 84, IDF = 2.8082
[construct_weights][L3] Term 'secondari': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '101': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'ramo-wooldridg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'corpor': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'lo': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pasadena': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'reaction': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'regard': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'fast': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'across': DF = 45, IDF = 3.4272
[construct_weights][L3] Term 'rate-govern': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'volum': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'recombin': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'slow': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'just': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'less': DF = 81, IDF = 2.8443
[construct_weights][L3] Term 'sensit': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'gradient': DF = 110, IDF = 2.5399
[construct_weights][L3] Term 'familiar': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'moder': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'differ': DF = 191, IDF = 1.9901
[construct_weights][L3] Term 'fact': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'nondimension': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'repres': DF = 97, IDF = 2.6651
[construct_weights][L3] Term 'heat-transf': DF = 72, IDF = 2.9614
[construct_weights][L3] Term 'directli': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'unyaw': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'segment': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'opposit': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'rate-control': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'speci': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'low-spe': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'except': DF = 61, IDF = 3.1259
[construct_weights][L3] Term 'replac': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'schmidt': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'simplif': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'introduc': DF = 64, IDF = 3.0783
[construct_weights][L3] Term 'alon': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'atom-molecul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'collis': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'especi': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'clearli': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'accept': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '565': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'transpir': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'beckwith': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '4345': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compressible-boundary-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chordwis': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'falkner-skan': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'porou': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'nonpor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'viscositytemperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sutherland': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '7': DF = 67, IDF = 3.0328
[construct_weights][L3] Term 'viscosity-temperatur': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'percentag': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'amount': DF = 43, IDF = 3.4722
[construct_weights][L3] Term 'coolant': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'incompressible-boundary-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '757': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'half-w': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'crop': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'delta': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'planform': DF = 23, IDF = 4.0879
[construct_weights][L3] Term '41': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'roger': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'e.w.e.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'berri': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '3286': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'incid': DF = 59, IDF = 3.1590
[construct_weights][L3] Term 'oil-flow': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'pattern': DF = 51, IDF = 3.3034
[construct_weights][L3] Term 'growth': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'main': DF = 53, IDF = 3.2653
[construct_weights][L3] Term 'rear': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'littl': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'sweep': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'shock-induc': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'aerofoil': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'seem': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'rather': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'simultaneously-attain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'front': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'posit': DF = 82, IDF = 2.8321
[construct_weights][L3] Term 'reattach': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'certain': DF = 118, IDF = 2.4700
[construct_weights][L3] Term 'ahead': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'resembl': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'respect': DF = 95, IDF = 2.6858
[construct_weights][L3] Term 'text': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'affect': DF = 45, IDF = 3.4272
[construct_weights][L3] Term 'aft': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'stimul': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'lift-depend': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'stage': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'cropped-delta': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'noteworthi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'absenc': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'outboard': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'attribut': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'partli': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'unswept': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'true': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '1018': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'elevated-temperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'object': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'lifetim': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'aid': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'out-of-straight': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wherea': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'encount': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'explicitli': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '768': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fatigu': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'meter': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'life': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'phillip': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rae': DF = 61, IDF = 3.1259
[construct_weights][L3] Term 'struct': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '279': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'multipli': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'read': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'record': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'ad': DF = 12, IDF = 4.7192
[construct_weights][L3] Term '80': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '902': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'current': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'unsteadi': DF = 43, IDF = 3.4722
[construct_weights][L3] Term 'lambourn': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'n.c.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '844': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'emphas': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'recent': DF = 92, IDF = 2.7177
[construct_weights][L3] Term 'zbrozek': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'format': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'presenc': DF = 68, IDF = 3.0181
[construct_weights][L3] Term 'oscillatori': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'transient': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'mode': DF = 56, IDF = 3.2107
[construct_weights][L3] Term 'alreadi': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'featur': DF = 49, IDF = 3.3430
[construct_weights][L3] Term 'shed': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'distort': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'pass': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'might': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'expect': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'although': DF = 62, IDF = 3.1098
[construct_weights][L3] Term 'greatli': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'delai': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'significantli': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'attach': DF = 32, IDF = 3.7637
[construct_weights][L3] Term '358': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'mager': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '181': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'shock-separ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'adjust': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'shock-boundary-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'postul': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'overexpand': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '956': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'corrug': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'sand': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wich': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'harri': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'l.a.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'baker': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'e.g.': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'd1510': DF = 12, IDF = 4.7192
[construct_weights][L3] Term '331': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'instabl': DF = 38, IDF = 3.5943
[construct_weights][L3] Term 'orient': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'parallel': DF = 65, IDF = 3.0629
[construct_weights][L3] Term 'deflect': DF = 77, IDF = 2.8947
[construct_weights][L3] Term 'stein': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'mayer': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'deform': DF = 46, IDF = 3.4055
[construct_weights][L3] Term 'solv': DF = 82, IDF = 2.8321
[construct_weights][L3] Term 'remark': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'probabl': DF = 23, IDF = 4.0879
[construct_weights][L3] Term '1073': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heat-conduct': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'crank': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nicolson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cam': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'phil': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '43': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '1947': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'medium': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'subscript': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'q': DF = 25, IDF = 4.0063
[construct_weights][L3] Term '393': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uniform': DF = 113, IDF = 2.5131
[construct_weights][L3] Term 'sakurai': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'j.ae.scs.': DF = 24, IDF = 4.0463
[construct_weights][L3] Term '24': DF = 38, IDF = 3.5943
[construct_weights][L3] Term 'author': DF = 88, IDF = 2.7619
[construct_weights][L3] Term 'navier-stok': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'situat': DF = 22, IDF = 4.1314
[construct_weights][L3] Term '1241': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'activ': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'denison': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'm.r.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'turbulent-boundary-lay': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'extrapol': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'analyt': DF = 106, IDF = 2.5768
[construct_weights][L3] Term 'inject': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'combust': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'lewi': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'chemistri': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'insid': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'instanc': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'often': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'suffici': DF = 47, IDF = 3.3842
[construct_weights][L3] Term 'graphit': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '155': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'proc.roy.s.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '164': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1938': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '547': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'place': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'edgewis': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'distanc': DF = 115, IDF = 2.4957
[construct_weights][L3] Term 'superpos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'y': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'denot': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'eighth': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ninth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'roughli': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'unfortun': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'eight': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'prohibit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'circumscrib': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'polygon': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'infinitesim': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'side': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'preced': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'continu': DF = 56, IDF = 3.2107
[construct_weights][L3] Term 'vortex': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'therebi': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '1087': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'iter': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'frankel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 's.p.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'math': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'comp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1950': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'high-spe': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'digit': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'comput': DF = 125, IDF = 2.4126
[construct_weights][L3] Term 'feasibl': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'quit': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'labor': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'southwel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'relax': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'routin': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'automat': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'flexibl': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'depend': DF = 107, IDF = 2.5675
[construct_weights][L3] Term 'skill': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'practition': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '969': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'side-jet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'devic': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'liepman': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'h.p.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jnl': DF = 40, IDF = 3.5436
[construct_weights][L3] Term '453': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'issu': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'exist': DF = 117, IDF = 2.4785
[construct_weights][L3] Term 'sizabl': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'usabl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'attract': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'obvious': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bonu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '367': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'via': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'lyapunov': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kalman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e': DF = 70, IDF = 2.9893
[construct_weights][L3] Term 'bertram': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'tran': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'asm': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '82': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'june': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'rigor': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'exposit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stationari': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'nonslationari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'control-system': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'optim': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'relai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'servo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'self-contain': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'emphasi': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'thorough': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tool': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'companion': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '703': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kussner': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'h.g.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '979': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1941': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'simpler': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'possio': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'birnbaum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'span': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'period': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'downwash': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'oscil': DF = 58, IDF = 3.1759
[construct_weights][L3] Term 'lastli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '531': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ionospher': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'bird': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'g.a.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'particl': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'distant': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'charg': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'magnet': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'mark': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'concentr': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'vicin': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'reflect': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'reimping': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exampl': DF = 127, IDF = 2.3968
[construct_weights][L3] Term 'charged-particledens': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'contour': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'surround': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'disc': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'neutral-particl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '951': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unifi': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'stowel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'e.z.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thein': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wah': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '658': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hyperbolic-sin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'believ': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'among': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'ideal': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'sourc': DF = 33, IDF = 3.7334
[construct_weights][L3] Term '509': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'graphic': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'sublim': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'net': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'adam': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'w': DF = 39, IDF = 3.5686
[construct_weights][L3] Term '360-1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'act': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'space': DF = 44, IDF = 3.4495
[construct_weights][L3] Term '199': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'wing-aileron-tab': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wight': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'k.c.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '2934': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1955': DF = 46, IDF = 3.4055
[construct_weights][L3] Term 'ii': DF = 14, IDF = 4.5708
[construct_weights][L3] Term '3029': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aris': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'apparatu': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'tab': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'amplitud': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'aileron': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'vortex-sheet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lowest': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'smaller': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'aileron-tab': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'approx': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'seal': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'hing': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'gap': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'ailcron': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'minu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'deg': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'plu': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '12': DF = 38, IDF = 3.5943
[construct_weights][L3] Term 'equival': DF = 44, IDF = 3.4495
[construct_weights][L3] Term 'departur': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'flat-plat': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'suffix': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1279': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'environ': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'scala': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 's.m.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'priori': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'respons': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'aerotherm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'demand': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'properli': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'pertin': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'convect': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'exchang': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'analyz': DF = 62, IDF = 3.1098
[construct_weights][L3] Term 'refractori': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'environment': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'reentri': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'simplic': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'quantiti': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'call': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'absorb': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'block': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'independ': DF = 53, IDF = 3.2653
[construct_weights][L3] Term 'nonequilibrium': DF = 18, IDF = 4.3272
[construct_weights][L3] Term '536': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wake': DF = 45, IDF = 3.4272
[construct_weights][L3] Term 'taylor': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'r.l.': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'keck': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'detect': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'gun': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'facil': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'fire': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '22-in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '17': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'sec': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'projectil': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'ambient': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'optic': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'cm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lumin': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'enc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'streak': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'disappear': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'suddenli': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'argon': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'intens': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'alwai': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'fluctuat': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'drum-camera': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'drum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'camera': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'photomultipli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'abl': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'schlieren': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'local-mach-numb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '704': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'kernel': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'finit': DF = 78, IDF = 2.8818
[construct_weights][L3] Term 'watkin': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'c.e.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'woolston': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'd.s.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'cunningham': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'h.j.a.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r-48': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'all-mov': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'tail': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'modif': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'must': DF = 49, IDF = 3.3430
[construct_weights][L3] Term 'scheme': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'adopt': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'suit': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'machin': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'incorpor': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'ibm': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'total': DF = 80, IDF = 2.8567
[construct_weights][L3] Term 'five': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'minut': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'rectangular': DF = 55, IDF = 3.2286
[construct_weights][L3] Term 'attack': DF = 107, IDF = 2.5675
[construct_weights][L3] Term 'allmov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cantilev': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '04': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '360': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inclin': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'grimming': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'william': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'young': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'novemb': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'constitut': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'missil': DF = 34, IDF = 3.7040
[construct_weights][L3] Term 'wide': DF = 51, IDF = 3.3034
[construct_weights][L3] Term 'emploi': DF = 69, IDF = 3.0036
[construct_weights][L3] Term 'herein': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'newtonian': DF = 43, IDF = 3.4722
[construct_weights][L3] Term 'corpuscular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sinc': DF = 49, IDF = 3.3430
[construct_weights][L3] Term 'centrifug': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '152': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'obstacl': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'lord': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'rayleigh': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'o.m.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.r.s.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hydrodynam': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'frictionless': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exercis': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rigid': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'immers': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'neg': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'accompani': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'helmholtz': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'someth': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'behind': DF = 68, IDF = 3.0181
[construct_weights][L3] Term 'infin': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'assert': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hardli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'suppos': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'water': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'hand': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'exceed': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'alter': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'worth': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'examin': DF = 57, IDF = 3.1932
[construct_weights][L3] Term 'question': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'amen': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1080': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l40': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jenson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'v.g.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '249': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '346': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'outlin': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'polar': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'co-ordin': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '40': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'favour': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stoke': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'toward': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'circul': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'explain': DF = 23, IDF = 4.0879
[construct_weights][L3] Term '394': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stuart': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'teddington': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'middlesex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'england': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'controversi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'glauert': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'worthwhil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'draw': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'lend': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1246': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonplanar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kilakowski': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l.j.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'haskel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.n.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'restrict': DF = 43, IDF = 3.4722
[construct_weights][L3] Term 'beyond': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'necessari': DF = 59, IDF = 3.1590
[construct_weights][L3] Term 'represent': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'singl': DF = 43, IDF = 3.4722
[construct_weights][L3] Term 'doubl': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'fourier': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'termwis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'matrix': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'select': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'storag': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'complic': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'tank': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'taper': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'camber': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'twist': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'evidenc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'overal': DF = 22, IDF = 4.1314
[construct_weights][L3] Term '1074': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'second-ord': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'wing-bodi': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'landahl': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'drougg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bean': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'b.j.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '27': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'septemb': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '694-702': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm-2': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonuniform': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'non-lift': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'cone-cylind': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1089': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'propeller-driven': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'vtol': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'kirbi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.h.': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'd730': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'keep': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'fuselag': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'horizont': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'tilt-w': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'deflected-slipstream': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'turn': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'loss': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'incur': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'hover': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'wing-stal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pure': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'extensible-chord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'high-lift': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stall': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'cruis': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '967': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pipe': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'magnetogasdynam': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'martin': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'd.e.': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'd-855': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magnetofluidmechan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'entri': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'length': DF = 90, IDF = 2.7396
[construct_weights][L3] Term '369': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prescrib': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'arbitrari': DF = 87, IDF = 2.7733
[construct_weights][L3] Term 'traugott': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '361': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'belotserkovskii': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'surface-pressur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'good': DF = 136, IDF = 2.3286
[construct_weights][L3] Term 'successfulli': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'connect': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'surface-veloc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shock-detach': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'varieti': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'stagnation-point': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'stand-off': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'unknown': DF = 23, IDF = 4.0879
[construct_weights][L3] Term '993': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hous': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sonic': DF = 50, IDF = 3.3230
[construct_weights][L3] Term '90degre': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '180degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hayman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l.o.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mcdearmon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '90': DF = 21, IDF = 4.1769
[construct_weights][L3] Term '180': DF = 13, IDF = 4.6423
[construct_weights][L3] Term '91': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'freestream': DF = 18, IDF = 4.3272
[construct_weights][L3] Term '15x106': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '400': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'jet-exit': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'windward': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'halv': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'leeward': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1284': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tubul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '501': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'radii': DF = 12, IDF = 4.7192
[construct_weights][L3] Term '81': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'reach': DF = 36, IDF = 3.6476
[construct_weights][L3] Term 'instead': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'benefit': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'origin': DF = 48, IDF = 3.3634
[construct_weights][L3] Term '356': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'optimum': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'super-aerodynam': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tan': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'h.s.': DF = 12, IDF = 4.7192
[construct_weights][L3] Term '25': DF = 57, IDF = 3.1932
[construct_weights][L3] Term '56': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'minimum': DF = 42, IDF = 3.4954
[construct_weights][L3] Term 'molecul': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'carter': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'amr': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '11': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'rev': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'realiz': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'unnecessari': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'euler': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'written': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'prior': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'substitut': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'planetari': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'atospher': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chapman': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'd.r.': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'r-11': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pair': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'disregard': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'introdur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'graviti': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'precis': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'allen': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'egger': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'ballist': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'steep': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'descent': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'basic': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'truncat': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'sanger': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lift-drag': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'nonlift': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'univers': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'sens': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'dimens': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'deceler': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'venu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mar': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'jupit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'human': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'toler': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'minim': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'trim': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'establish': DF = 49, IDF = 3.3430
[construct_weights][L3] Term 'man': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'brake': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'escap': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'satellit': DF = 31, IDF = 3.7950
[construct_weights][L3] Term '958': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'scoop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'berner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'camac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'avco-everett': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 're': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'lab': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'collect': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'store': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'liquid': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'serv': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'fill': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'station': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'furnish': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'oxygen': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'spacecraft': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'altern': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'launch': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'orbit': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'econom': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'basi': DF = 68, IDF = 3.0181
[construct_weights][L3] Term 'save': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'year': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'longtim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'joul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magnetohydrodynam': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'free-convect': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'cramer': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'k.r.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '746': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'submerg': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'openend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constant-temperatur': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'retain': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '732': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'analogu': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'r.v.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'app': DF = 61, IDF = 3.1259
[construct_weights][L3] Term '257': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'displac': DF = 73, IDF = 2.9477
[construct_weights][L3] Term 'bent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'extension': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'ident': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'edgedisplac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'specifi': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'edge-tract': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'circumst': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'edge-displac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'propound': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inclus': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'statement': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'perfor': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'two-diagram': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prefer': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'mix': DF = 46, IDF = 3.4055
[construct_weights][L3] Term '1270': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inlet': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'fraiser': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h.r.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '429': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'duct': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'small-perturb': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pressure-recoveri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'calcualt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fixed-geometri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sugar-scoop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hypass': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'airflow': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'done': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'f8u-3': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'airplan': DF = 30, IDF = 3.8272
[construct_weights][L3] Term '1042': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vibrat': DF = 56, IDF = 3.2107
[construct_weights][L3] Term 'shallow': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'cabin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'high-altitud': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'met': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'vessel': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'frame': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'necessarili': DF = 13, IDF = 4.6423
[construct_weights][L3] Term '190': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kanwal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.math.mech.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '681': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'find': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'hydromagnet': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'write': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'full': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'help': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'thoma': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'everi': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'rearrang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'easili': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '1248': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shock-expans': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'waldman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g.d.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'probstein': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'r.f.': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '119': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'perfect': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'shockexpans': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'break': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'zero-ord': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'similitud': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'shock-shap': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mahoni': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weak': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'simple-wav': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'friedrich': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'expand': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'obliqu': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'famili': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'parabol': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'excel': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '994': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'retrocket': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'charczenko': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hennessi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'k.w.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'd-1016': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '60': DF = 31, IDF = 3.7950
[construct_weights][L3] Term '00': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'retrorocket': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exclud': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'jet-off': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'unstabl': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '960': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h.w.': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'laufer': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1257': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'spread': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'forth': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'scale': DF = 38, IDF = 3.5943
[construct_weights][L3] Term 'zone': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'mixing-length': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'balanc': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'tripl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '538': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conpress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'burgraf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'o.r.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '19': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'dorodnitzyn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'invari': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'proper': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'crocco': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'eckert': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'chosen': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'shearstress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'choic': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'referenceenthalpi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1045': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'strength': DF = 59, IDF = 3.1590
[construct_weights][L3] Term 'zender': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'g.w.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '362': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'membran': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '197': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'swihart': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'whitcomb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'c.f.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'rm': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'l53h04': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1953': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'langlei': DF = 27, IDF = 3.9308
[construct_weights][L3] Term '16-foot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sting': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'increment': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '09': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '39': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '53': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '05': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'normalforc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'subcrit': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'cross-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sting-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '120-inch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1277': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cancel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'schaffer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '193': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'anoth': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'spin': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'recoveri': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'restor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'geometr': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'filament': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '62': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'drop': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'recov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'impos': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'irrespect': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'irrevers': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'perhap': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'primari': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'opportun': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mechan': DF = 49, IDF = 3.3430
[construct_weights][L3] Term 'brought': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'plai': DF = 12, IDF = 4.7192
[construct_weights][L3] Term '735': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sectori': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'conwai': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'h.d.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'huang': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'm.k.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'superposit': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '507': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'goldstein': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'a.w.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'nearli': DF = 31, IDF = 3.7950
[construct_weights][L3] Term '163': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'corridor': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'guidanc': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'supercircular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r-55': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'planet': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'accomplish': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'maneuv': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'dimensionless': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'perige': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'character': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'conveni': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'ellipt': DF = 37, IDF = 3.6206
[construct_weights][L3] Term 'hyperbol': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'titan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fix': DF = 40, IDF = 3.5436
[construct_weights][L3] Term 'single-pass': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'twice': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'overshoot': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'undershoot': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'broaden': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'markedli': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '10-earth-g': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'singlepass': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '52': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '51': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'mile': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'termin': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'hit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'moon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fuel': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'heliocentr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'planetocentr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'convert': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'target': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'voyag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'impli': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'lift-off': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1283': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'coaxial': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'dunwoodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n.t.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '494': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'radial': DF = 36, IDF = 3.6476
[construct_weights][L3] Term 'withdraw': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'solubl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '351': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jeffrey-hamel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonparallel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'millsap': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pohlhausen': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '187-196': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'held': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'jacobian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jeffery-hamel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'introduct': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'diverg': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'channel': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '10degre': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'isol': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'plant': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'e.s.': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'brown': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'k.a.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'almost': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'trace': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'vibratori': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'transmit': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'adjac': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'propel': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'transmiss': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'unbalanc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mount': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'preponder': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'occasion': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'failur': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'shorten': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'psycholog': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'physiolog': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'attend': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'passeng': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crew': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'like': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'intim': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'drastic': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'symmetri': DF = 26, IDF = 3.9678
[construct_weights][L3] Term '332': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'real-ga': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'cheng': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'h.k.': DF = 12, IDF = 4.7192
[construct_weights][L3] Term '575': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'remov': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'inspect': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'wedg': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'rule': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'simul': DF = 35, IDF = 3.6754
[construct_weights][L3] Term '756': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'comment': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'matric': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'charl': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'samson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jr': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'professor': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'depart': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'civil': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'colleg': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'texa': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'klein': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'co-author': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'writer': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'large-ord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hopeless': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'anove-ment': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'disagre': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pessimist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'large-matrix': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'invert': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'ultim': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'arriv': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '564': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'gallagh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.j.': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'memo': DF = 14, IDF = 4.5708
[construct_weights][L3] Term '2-27-59l': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1x10': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4x10': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stagnation-lin': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'half': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'heattransf': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'monoton': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'quadratur': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '590': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'stone': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1273': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'axial-flow': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '1026': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'instantan': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'plastic': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'fail': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '1214': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'elong': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'robertson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'clark': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'm.e.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '842': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'oseen-typ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'creeping-mot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'million': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'hydroballist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'glycerin-wat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fin': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'ellipsoid': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'fineness-ratio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '000-fold': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'laminar-visc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'laminarturbul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'occurr': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '769': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'circumferenti': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'john': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'd.j.': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '267': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hoop': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'extrem': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'localis': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'discontinu': DF = 20, IDF = 4.2245
[construct_weights][L3] Term '903': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eckhau': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'm.i.t.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'group': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '59-3': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shockwav': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'hinge-axi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'whole': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'role': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'shock-boundari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1019': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'isochronousstress-strain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'as-receiv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '24s-t4': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aluminum': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'load-capac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'promis': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'recogn': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'columnmateri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'column-capac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'column-imperfect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'column-materi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exposur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1213': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wittcliff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wilson': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'aerothermodynam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cal': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '15-in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '13': DF = 28, IDF = 3.8950
[construct_weights][L3] Term 'transverse-curvatur': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'reshotko': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'laminar-boundary-lay': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'zero-yaw': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shock-lay': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '1021': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'collaps': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'long': DF = 54, IDF = 3.2467
[construct_weights][L3] Term '597': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pugh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p.g.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'woodgat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '012': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'scruton': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'et': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'al': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '75': DF = 14, IDF = 4.5708
[construct_weights][L3] Term '47': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '02': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'thinner': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '563': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'cole': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '191': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'mean-veloc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'well-known': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'hypothet': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'shearing-stress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'supposedli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'large-eddi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constraint': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'inertia': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'penetr': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'manifest': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'chiefli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'logarithm': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'usefulli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vector': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'scalar': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'least': DF = 30, IDF = 3.8272
[construct_weights][L3] Term '751': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'end': DF = 35, IDF = 3.6754
[construct_weights][L3] Term 'prevent': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'bluff': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'cowdrei': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1025': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'felt': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'merit': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '335': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'liepmann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'decemb': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '1946': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'strongli': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'distinctli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'discern': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '107': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ting': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'lu': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'j.math.phys.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '38': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '153': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'third': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'compat': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'commonli': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'blasiu': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '138': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pearson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mckenzi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.b.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '63': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '415': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'tendenc': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'non-uniform': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'fed': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'prevail': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'earli': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'blade': DF = 30, IDF = 3.8272
[construct_weights][L3] Term 'row': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'excit': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'implicit': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'think': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'peopl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'outlet': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'intak': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'cours': DF = 13, IDF = 4.6423
[construct_weights][L3] Term '904': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'calibr': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'kettl': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'tech': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '222': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1951': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'r.a.e.': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'slight': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'discrep': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'doubt': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'decid': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'check': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'instrument': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'octob': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '793': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sweptback': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '3271': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'juli': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '828': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'smoothli': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1225': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'georg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mclafferti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'barber': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unit': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'curved-surfac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'begin': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'creat': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'von': DF = 33, IDF = 3.7334
[construct_weights][L3] Term 'karman': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'poor': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'high-energi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'maintain': DF = 22, IDF = 4.1314
[construct_weights][L3] Term '1017': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gerard': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '714': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'librov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'contain': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'time-independ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'conjectur': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'whether': DF = 12, IDF = 4.7192
[construct_weights][L3] Term '303': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'fenster': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 's.j.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'neyman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.j.': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'atom': DF = 13, IDF = 4.6423
[construct_weights][L3] Term '35': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'realist': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'frozen': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'cold-wal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '131': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pai': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 's.i.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'j.aero.scs.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '16': DF = 21, IDF = 4.1769
[construct_weights][L3] Term '1949': DF = 23, IDF = 4.0879
[construct_weights][L3] Term '463': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'whose': DF = 42, IDF = 3.4954
[construct_weights][L3] Term 'two-uniform': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'regardless': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'reichardt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'henc': DF = 25, IDF = 4.0063
[construct_weights][L3] Term '555': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'repli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exce': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'sonic-point': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quot': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '767': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'specimen': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'tension': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'bulk': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'biot': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'riccati': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bessel': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'iii': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'endur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cyclic': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'temperature-edg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wider': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'context': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'better': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'cycl': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'cater': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1028': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'technic': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '4000': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'elev': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'phase': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'appar': DF = 32, IDF = 3.7637
[construct_weights][L3] Term 'construct': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'gamma': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'iron': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '758': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lower': DF = 72, IDF = 2.9614
[construct_weights][L3] Term 'bound': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'attain': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'sonic-boom': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'over-pressur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'carlson': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'd1494': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'overpressur': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'narrow': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'cross-sect': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'lift-produc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '932': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lackman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'penzien': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '458': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'recommend': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'forego': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '760': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inelast': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'l.h.n.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '87': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'donnel': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'virtual': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'adapt': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'ten': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '3003-0': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'post-buckl': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'overestim': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'paradox': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'resolv': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '552': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wrai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'k.l.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'academ': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'press': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'york': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'equilibr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'translat': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'freedom': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'ion': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'complex': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'compil': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'understood': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'still': DF = 23, IDF = 4.0879
[construct_weights][L3] Term '136': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'roa': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g.v.r.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.r.s.jnl.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '31': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '1488': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bell-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'annular': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'plug': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'e-d': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'expansion-deflect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'belltyp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'outward': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'central': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'inward': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'back': DF = 13, IDF = 4.6423
[construct_weights][L3] Term '304': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'oguchi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'univ': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'tokyo': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r330': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'join': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'sai': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'ignor': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'erickson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1010': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'air-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'reservoir': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'becam': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'poind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'squar': DF = 48, IDF = 3.3634
[construct_weights][L3] Term 'inch': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'btu': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'pound': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '1222': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hain': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'f.d.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'holer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'y.a.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '143': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'inviscid-flow': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'dipol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inviscid-flow-field': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'continuum-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'order-of-magnitud': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'fourth-degre': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fifth-degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stagnation-enthalpi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'supersonic-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '794': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'swept-back': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'warren': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'chord': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'programm': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'spanwis': DF = 27, IDF = 3.9308
[construct_weights][L3] Term '14': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'band': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'strain-gaug': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '109': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'product': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'owen': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'p.r.': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'zienkiewicz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '521': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'insert': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'grid': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rod': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'grade': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'strictli': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'weakli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '45': DF = 22, IDF = 4.1314
[construct_weights][L3] Term 'gave': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'large-scal': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'danger': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inher': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'decai': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '935': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'multi-lay': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'rotation': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'radkowski': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'avco': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'manufactur': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'compact': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '599': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sack': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'a.h.': DF = 12, IDF = 4.7192
[construct_weights][L3] Term '3283': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slender-bodi': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '325': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'triangular': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'light': DF = 25, IDF = 4.0063
[construct_weights][L3] Term '961': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'korst': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'h.h.': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'page': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'illinoi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dept': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'eng': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'me-tn-392-1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'one-paramet': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bornel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'intrins': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'kernal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'asymptot': DF = 41, IDF = 3.5192
[construct_weights][L3] Term '539': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'infit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weiss': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '995': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'placehold': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1249': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'plasma': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'conductor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yoshihara': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '141': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'dens': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wavi': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'cusp': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'coulomb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vanish': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'fluid-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ackeret': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'reduced-plasma': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '506': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'havelock': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'shallow-wat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wave-resist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'brandmaier': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'focus': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ground-effect': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'contact': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'terrain': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'land': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'gravity-wav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cushion': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '650': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'digital-comput': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '734': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sector': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'hass': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '271': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'deflexion': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semicircl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weinstein': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'biharmon': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'z': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'harmon': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'six': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'judg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'connexion': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1282': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bush': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'w.b.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '49': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'exhibit': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'hysteresi': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'magnetoaerodynam': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'couett': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'furthermor': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '350': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'releas': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'maryland': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'isotherm': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'isovel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '162': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'descend': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'g.m.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'r-3': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonrot': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'miss': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'misalin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'largest': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'contribut': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'retroveloc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '34': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '120': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '150': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'flight-path': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1276': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exert': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'ehler': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'shoemak': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'e.m.': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'acoust': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'strike': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '1044': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reissner': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'anniv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '231': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'undeform': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'advantag': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'nth': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'apex': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '196': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r.g.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'cp213': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pictur': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'lotz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'adequ': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'onset': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'cavit': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '992': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'love': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'l52119a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1952': DF = 31, IDF = 3.7950
[construct_weights][L3] Term 'forebodi': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '20-percent-bodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'abruptli': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'gardless': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'somewhat': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'forward-exhaust': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'favor': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'rearward-exhaust': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rearward': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '368': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'best': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'j.roy.ae.soc.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '64': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'polar-control': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'manoeuvr': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'ax': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'aeroplan': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'home': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'cartesian': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'twodimension': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '966': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'maslen': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 's.h.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'r-34': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'readili': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'constantproperti': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'quasi-incompress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'freeconvect': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wherein': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'root': DF = 19, IDF = 4.2745
[construct_weights][L3] Term '1088': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'amer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '92': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'sort': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'o': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'rewrit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'superscript': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'over-relax': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ser': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '210': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '307-357': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1910': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'immedi': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'newly-comput': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'la': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gauss-seidel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dirichlet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1043': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'discret': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'multi-sect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'arbitrarili': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'meridion': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'semi-algebra': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'languag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'm.g.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r.aero.': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '2644': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spiral': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'admit': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'inner': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'sub-cor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tabular': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1271': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ordwai': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hale': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '437': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hub': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'lightli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thin-w': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'constant-strength': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'travel': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'helic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'evvard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'krasilshchikova': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'upwash': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'curvilinear': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'nonorthogon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'coincid': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'noncommun': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'highspe': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '959': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'larson': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '731': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3792': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'scope': DF = 14, IDF = 4.5708
[construct_weights][L3] Term '165': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'smith': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'd.w.': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'walker': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'r-26': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'surface-shear': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'floating-el': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'skinfrict': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'obtainec': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '32': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'schultz-grunow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kempf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schoenherr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'univer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'appraoch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uncorrect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'preston': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'inexpens': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slighlti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1285': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'peckham': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd.h.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'tn.aero.': DF = 22, IDF = 4.1314
[construct_weights][L3] Term '2863': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1963': DF = 41, IDF = 3.5192
[construct_weights][L3] Term 'apex-angl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '357': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.j.': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '527': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'superaerodynam': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'specular': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'specular-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'absorpt': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'random': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'emiss': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'likewis': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'fine': DF = 18, IDF = 4.3272
[construct_weights][L3] Term '733': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'carrier': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'g.f.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1944': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '134': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ring': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'reinforc': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'piston': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'integral-equ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boundary-condit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'detach': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'ridyard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gravalo': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'edelfelt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'emmon': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'van': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'dyke': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'gordon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'geiger': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'serbin': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1278': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lochtenberg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'b.h.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'step': DF = 26, IDF = 3.9678
[construct_weights][L3] Term 'hot-wir': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'anemomet': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'tollmienschlicht': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'burst': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'amplif': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'bubbl': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'foil': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '198': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cowl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'spinner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nichol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'keith': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.l.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'r950': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'propellerresearch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cowling-spinn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1-seri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'open-nos': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lip': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oval': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'shank': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'near-maximum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compressible-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inlet-veloc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '950': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jahsman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.e.': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '431': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'creep-buckl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hypothes': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'rabotnov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shesterikov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '508': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nose-blunt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'after-bodi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'greenberg': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'r.a.': DF = 18, IDF = 4.3272
[construct_weights][L3] Term '359': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'hise': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nose-bluntness-induc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nose-afterbodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'junction': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'blast-wav': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'chernyi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1075': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wilbi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sweden': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'ffa': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '0degre': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'deterior': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '395': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.a.': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'temperature-depend': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'dissip': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'lagrangian': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'simplest': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'depth': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'onedimension': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'slab': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'elabor': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1247': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boom': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'ryhm': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'i.l.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'whitham': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'far-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bow': DF = 23, IDF = 4.0879
[construct_weights][L3] Term 'constrain': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'body-volum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'said': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'primarili': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '705': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'runyal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h.l.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'rep': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '1234': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'singular': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'nonsingular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'elsewher': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'see': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'memorandum': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'reproduc': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'fourth': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '537': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stoddard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.j.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1138': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'boundary-valu': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'closedform': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'millikan': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'c.b.': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'phil.mag.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1929': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '641': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'discov': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'belong': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'class': DF = 28, IDF = 3.8950
[construct_weights][L3] Term '1081': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kawaguti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'phy': DF = 25, IDF = 4.0063
[construct_weights][L3] Term 'japan': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '747': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'goe': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kirchhoff': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tomotika': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'yosinobu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'math.phys.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'repeat': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'pierci': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'winni': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'london': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1933': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'perfectli': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '957': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'snap': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'newman': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'reiss': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'e.l.': DF = 12, IDF = 4.7192
[construct_weights][L3] Term '451': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'bellevil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spring': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'bifurc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '18': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'now': DF = 24, IDF = 4.0463
[construct_weights][L3] Term 'infer': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'taylor-maccol': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'kopal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '366': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fox': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'libbi': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'p.a.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '921': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'twopoint': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'eigenvalu': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '968': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'interplanetari': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sutton': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'g.p.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'nuclear': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'magnetoplasma': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'solar': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'sail': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mission': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'criteria': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'reliabl': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'capabl': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'superior': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'long-tim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'none': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'reject': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '154': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'a.a.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hislop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'proc.cam.phil.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '345': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'abstract': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '954': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'low-turbul': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'experienc': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'truli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1086': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wood': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '176': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'old': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '530': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chu': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'wen-hwa': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '781': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'oseen': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'discontinuous-wak': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slowli': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'trailing-edg': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '702': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sink': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'mazelski': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'drischler': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'j.a.': DF = 19, IDF = 4.2745
[construct_weights][L3] Term '2739': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'indici': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'sharp-edg': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'gust': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'exponenti': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '392': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'restrain': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'jogarao': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lakshmikantham': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heavier': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'member': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'restraint': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'nomenclatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dana': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1240': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonsimilar': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'upstream-transpir': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pallon': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '449': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'injectioncool': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compressible-laminarboundary-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'domain': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'integrodifferenti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'integrand': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'polynomi': DF = 14, IDF = 4.5708
[construct_weights][L3] Term 'runge-kutta': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1072': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ignit': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'marbl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'adamson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't.c.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'prop': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'flame': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'propag': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'quench': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'accordingli': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'mixtur': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'hot': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'come': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'decomposit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'arrheniu': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'answer': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'far': DF = 39, IDF = 3.5686
[construct_weights][L3] Term 'enorm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1332': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diederich': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'f.w.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'zlotnick': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1228': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nineteen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'angle-of-attack': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'weissing': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'semispan': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'pertain': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'definit': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'stipul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'untwist': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'aeroelast': DF = 17, IDF = 4.3827
[construct_weights][L3] Term '1100': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rashi': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'hopko': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'x-300': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'enabl': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'teflon': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '800': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'lb': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'nonabl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inconel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'piec': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '684': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heuman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '127-206': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'legendr': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '442': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd.c.': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'castil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g.e.': DF = 21, IDF = 4.1769
[construct_weights][L3] Term '2558': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'assort': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'relative-dens': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'supplement': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'flutter-spe': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '670': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ferri': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'zakkai': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '882': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'inconsist': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '214': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'cascad': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'staniforth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ngte': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'r212': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'facilit': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'a.j.': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '700': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'inviscid-visc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'led': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '489': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'morduchow': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '996': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thenc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'worker': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'feel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unhappi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eq': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'justif': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'ber': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sure': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fuller': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'warrant': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'typographi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'confus': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'typograph': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'victor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'polytechn': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'brooklyn': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'freeport': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '825': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'onat': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'e.t.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'drucker': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'troublesom': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bear': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'peculiar': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'impress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'excess': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'overli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unavoid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'insensit': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'version': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'cruciform': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'incorrect': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'stephen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cleveland': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'ohio': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'dash': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'impuls': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '213': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stratford': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'b.s.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'sansom': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3273': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rotor': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'overrid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'predominantli': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'closed-jet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'straight-sid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'behav': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'travers': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'convers': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'four-hol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '677': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2884': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2353': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '439': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'devis': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'interpol': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'trigonometr': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'get': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'predigest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fig': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '1-6': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'proposit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2-5': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '8-30': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'appendic': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'i-vi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'carefulli': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'salient': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'vii': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'intend': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'instruct': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '445': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mathieu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'explicit': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'formal': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '683': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.h.b.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2584': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uncamb': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'droop': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'downward': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'forward-fac': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1107': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tendeland': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd689': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '8x10': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'foot': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'highest': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'sharp-plat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1335': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'freon-12': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'huber': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p.w.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1024': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'one-half': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'room': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'condens': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'adiabat': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'lag': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '648': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '275': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'prismat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bar': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'equilateral-triangl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hexagon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '822': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l.h.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'wan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'c.c.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'tsien': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'precipit': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'turbanc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'large-deflect': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'thicker': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lower-yield': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uneven': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fabric': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'strut': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'toroid': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'beskin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'morlei': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'l.s.d.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r.struct.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '277': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'minimis': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'gaussian': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'traction': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '473': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'freeman': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'chester': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'newtonian-plus-centrifug': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mise': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '7-113': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eddi': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'paul': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'divis': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'compani': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'lack': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'ration': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '225': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'jorgensen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tn4045': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '97': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '67': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'bank': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'zero-lift': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'visual-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'crossflow': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'inhibit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'zerolift': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '962': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1131': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'singer': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '212': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nonaxisymmetr': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'inadequaci': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1303': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g.i.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'maccol': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'j.w.': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '278': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'irrot': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'semi-vert': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bullet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '46': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'moor': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'spindle-shap': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'increasingli': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'truth': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '487': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'base-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crocco-le': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '963': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'incom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'basetyp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'backward-fac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'recompress': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'reynolds-number-depend': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'base-pressur': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '28': DF = 29, IDF = 3.8606
[construct_weights][L3] Term 'explos': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'sedov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '77': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'hay': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'constant-energi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inquir': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'einbind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'difficult': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '814': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tobak': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'wehrend': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'w.r.': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '3788': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tangenc': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'closed-form': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'semivertex': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '480': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'spald': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'imperi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'titl': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'burn': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '1304': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'colston': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'symposium': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'giraud': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.p.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'bristol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'circular-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'devot': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '1136': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'torispher': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'toricon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shield': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'r.t.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '292': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hydrostat': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'knuckl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unfir': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'surprisingli': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'np': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'safeti': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '06': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '08': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'toriconic1a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'enough': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'complement': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'marsden': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'simpson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rainbird': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'coa': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'r114': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ascertain': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'smoke': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'visual': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'top': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'chambr': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'p.l.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'schaaf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 's.a.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'continuum': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '474': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dean': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'high-veloc': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'rest': DF = 18, IDF = 4.3272
[construct_weights][L3] Term '76-power': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'proportion': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'hightemperatur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '646': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tewfik': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'o.e.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'e.r.g.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'shirtliff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mech.inst.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'seattl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-inch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'woven': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wire': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'align': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vice': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'versa': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1109': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'yang': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'k-t': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '653': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '679': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'weber': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'brebner': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'g.g.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '2374': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mostli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'isobar': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '813': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'body-axi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nelson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3737': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'asymmetri': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rolling-missil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pulse-rocket': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'watson': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'e.j.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2176': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'notat': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '2n': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'circl': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'thwait': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '847': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'gottenberg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.g.': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'america': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1002': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lineal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nodal': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'timoshenko': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1162': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'force-test': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'take-off': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'tosti': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'l.p.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd44': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'verticaltake-off-and-land': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'long-w': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6-blade': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dual-rot': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'interconnect': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'tilt': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'normal-forwardflight': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'forward-flight': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1350': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'billow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'missile-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'salmi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd284': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2by': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2-foot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nitrogen': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '282': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'baughman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l.e.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kochendorf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e57e06': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'afterbody-jet-nozzl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boattail': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'base-to-jet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exit': DF = 27, IDF = 3.9308
[construct_weights][L3] Term 'child': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '878': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'equip': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'molyneux': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'you.k.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'clearanc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1196': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'murphi': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'c.h.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'dickinson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aiaa': DF = 28, IDF = 3.8950
[construct_weights][L3] Term '339': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '8000': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'calib': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'imposs': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'quasi-steadi': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'lees-hroma': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '44': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'tip-blunt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'm.h.': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '898': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'characteristi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'standpoint': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'contempl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'benefici': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'receiv': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'tangent': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'blunter': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'gupta': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'x-direct': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'semi-incompress': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'obviou': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'electromagnet': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '276': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'keenan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.h.': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '773': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'whenev': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'exhaust-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '612': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'squir': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '2838': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '420': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'jaszlic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trill': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '544': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'soon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'persist': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '840': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wrinkl': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'hedgepeth': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'd813': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stretched-membran': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '418': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'morkovin': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'm.v.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1121': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '88': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '736': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'liquid-met': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'parallel-pl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hartmann': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'grashof': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'buoyant': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'open-end': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'volumetr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1368': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'steiger': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'bloom': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '233': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'osoen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '427': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lattic': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'deich': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1393': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'chap': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'book': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'gasdynam': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '1869': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'topic': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'list': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'paragraph': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '7-1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-2': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-3': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'electro-hydrodynam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-4': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'joukowski': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '7-5': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-6': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-7': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-8': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-9': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-10': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-11': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-12': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-13': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-14': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'overhang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7-15': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '615': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'contract': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'eccentr': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'king-hel': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'd.g.': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'mat': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'co-work': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'insulated-wal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1191': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hemisphere-cylind': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'hickman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.s.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'giedt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '665': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hemispherecylind': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'rarefi': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'individu': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '1730': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uncool': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'rarefact': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'potter': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'j.l.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'whitfield': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.d.': DF = 13, IDF = 4.6423
[construct_weights][L3] Term '663': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wall-toambi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'argu': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'transition-revers': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nomin': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'criterion': DF = 21, IDF = 4.1769
[construct_weights][L3] Term 'eter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'element': DF = 25, IDF = 4.0063
[construct_weights][L3] Term '1357': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'supplort': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'seid': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '1825': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '285': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'arisen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rais': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'matter': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1165': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'helicopt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bryan': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd977': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dynamic-pressur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'single-rotor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dual-propel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slipstream': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'mere': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'small-diamet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dual': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'upflow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slower': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '876': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3054': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'run': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'envisag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uncertainti': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1198': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blunt-leading-edg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inviscid-hypersonic-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'entropy-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wall-to-stagn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chapman-rubesin': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'linearis': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'robinson': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'entail': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'overcom': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'hadamard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'counterpart': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'bullen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n.i.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cp': DF = 15, IDF = 4.5041
[construct_weights][L3] Term '324': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'count': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'acceleromet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'summar': DF = 16, IDF = 4.4416
[construct_weights][L3] Term '247': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2993': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'untap': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'plan-form': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'biconvex': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'centre-sect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quickli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1395': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wittliff': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thermomet': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'vorticity-interact': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'viscous-lay': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'incipient-merged-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '849': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tobia': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ph.d.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ow': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'lagrang': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'hess': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'n.w.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4050': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'input': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'powerpl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'expos': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'discrete-typ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'herberl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gooderum': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'p.b.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '2000': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fore': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '623': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1009': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'constitu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'migrat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spontan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'warmer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'colder': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'soret': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dufour': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1856': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1873': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multicompon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'public': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'laminar-boundary-layer-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1361': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'turner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'step-bystep': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'force-deform': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'step-by-step': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'ostrach': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'thornton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4320': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reexamin': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'inaccuraci': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'nusselt': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '1153': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spreiter': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'j.r.': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'hyett': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r-73': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unbound': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inde': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'unusu': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1359': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'z-section': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stringer': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'peterson': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'dow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'm.b.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '2-12-59l': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'orthotrop': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'dispar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unstiffen': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '885': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hoff': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'n.j.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '405': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'perimet': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '871': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'disloc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'climb': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'weertman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.app.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'immobil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pool': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2678': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'philosophi': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '86': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'inviscid-incompress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'peripher': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'aerial-ground': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'insofar': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'augment': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1154': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'berndt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 's.b.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '71': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'stockholm': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'thicken': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1366': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cohen': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1294': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stewartson': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'yeild': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uniqu': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'interdepend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '624': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'channel-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'air-cushion': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'expenditur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fan': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'ground-height': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vehicle-length': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lower-surfac': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'aerodynamic-cent': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'identifi': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'mound': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'upper-surfac': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'induced-drag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'penalti': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'sealing-air': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '416': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'postpon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'allevi': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'pearcei': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'c.m.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'smf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fund': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.f.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '22': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'separation-fre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'margin': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'impair': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'emphasis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'embrac': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'highveloc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vane': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'air-jet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'neighbour': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'imag': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'counter-rot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'leav': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'lose': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'co-rot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'windtunnel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'disadvantag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '72': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'mirel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '3712': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'karman-pohlhausen': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'seventhpow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thermal-boundary-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '240': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'technot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'enclos': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'low-veloc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dead': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'transfn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1392': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hopkin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2234': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1945': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'middl': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'aeolotrop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'determinant': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'intuit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gaug': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'napolitano': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '389': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '223': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stanton': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'gadd': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '3147': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '647': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'other': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'j.app.mech.': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '475': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lock': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'r.c.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '42': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'non-zero': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '481': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'laminari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'foreign': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'schneider': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'haydai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hydrogen': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'admixtur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reliev': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1137': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonhomogen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'complementari': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'langer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trans.amer.math.soc.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '33': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'customari': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'wissler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dissert': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1916': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jointli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1305': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2730': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compromis': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'enforc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'built': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '678': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2429': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'endplat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '812': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l58a16': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fineness-ratio-2': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fineness-ratiobeen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'normal-forc': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '1108': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vandyk': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.d.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1071': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'busemann': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'insight': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'discoveri': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'corner': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'secondord': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1302': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2738': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'insuffici': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1130': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'handbook': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pt': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'vi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'becker': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '3786': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'comprehens': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '486': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aerothermoelast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dugundji': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'interior': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'stress-deflect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'high-stagnation-temperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conflict': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'look': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'incomplet': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'facet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '472': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chapter': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'shall': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'fit': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '640': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'clarkson': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'know': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'tne': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lectur': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'dicsov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'board': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prototyp': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'proof': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'deisgn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rib': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rib-skin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reson': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 's-n': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'consier': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uncertain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'db': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'guarante': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '224': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quasi-cylindr': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'jone': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'j.g.': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '2769': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ly': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'quasi': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'circularcylindr': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kei': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'self-preserv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '815': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'onic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fisher': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l.r.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dicamillo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1-21-59l': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '74': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '78': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'full-skirt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'short-skirt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'greatest': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'lift-curv': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'clamshell-shap': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'airstream': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'dissimilar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'isakson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '611': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'builtup': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wide-flang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'i-beam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flang': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '682': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lanc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g.n.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'generalis': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'deduc': DF = 20, IDF = 4.2245
[construct_weights][L3] Term 'compos': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'antisymmetr': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'w-a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ux': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sgn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'anti-symmetr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'homogen': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'cubic': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'tumbl': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1334': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3476': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '61': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'angleof-attack': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3014': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1106': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mcconnel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd278': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hemisphere-cone-cylind': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unblunt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hypothesi': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'decrement': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3275': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'output': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'inter-blad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'concurr': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'steam-turbin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'suffer': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'concav': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'convex': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'steam': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'free-vortex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'low-energi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'liquidinject': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '952': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'area-mean': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'generalised-newtonian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '314': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '2601': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pointednos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sin': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'max': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'revert': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'generalized-newtonian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'figur': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'd1': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'engag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hardwar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '444': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.b.t.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'perp': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kutta': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'one-degree-offreedom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '676': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2935': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2476': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '633': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1139': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'entranc': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'pond': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '917': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'randomli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'free-molecul': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'satelit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '649': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hovercraft': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'maritim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eggington': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'naval': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'arch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'british': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'project': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'search': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'underli': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'commerci': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'servic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'craft': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'saunders-ro': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sr-n1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'defenc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'person': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cost': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'undertaken': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'outstand': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '823': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'c.s.': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '241': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '671': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'baglei': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '2886': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6x10': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2x10': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'aerodynamic-centr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '443': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'midspan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4412': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pinkerton': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1936': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'variable-dens': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'orific': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'n.a.c.a.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '20degre': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '30degre': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'potential-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '448': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'aerodynamieist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'faster': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'small-disturb': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'noted-nam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '215': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'johnston': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'i.h.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'dransfield': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r235': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-stag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'single-stag': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'build': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1101': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sensor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'winter': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'c.w.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bracalent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd-800': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'variable-capacit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ablation-r': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'polym': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ethylene-h': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '400degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '800degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'telemet': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rocket-propel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1333': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.p.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '2117': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sudden': DF = 13, IDF = 4.6423
[construct_weights][L3] Term '1943': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'grow': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '685': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hanson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd984': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'all-movable-control-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'double-wedg': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'trailingedg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bending-to-tors': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'single-wedg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'destabil': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'prone': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'piston-theori': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'coupled-mod': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uncoupled-mod': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tapered-planform': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'square-planform': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'uncoupl': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'unconserv': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '824': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '543': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'interrel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'realli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'relev': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'thought': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'rhyme': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '798': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'impel': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'wheel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '488': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reaction-r': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'react': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'leonard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'higher-ord': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'announc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'co': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'msvo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'et.al.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'non-equilibrium': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'ia': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '62-67': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '30th': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'annual': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'meet': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '870': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rheolog': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'freudenth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.m.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'j.app.phys.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'fractur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'reactor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unreal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'viscoelast': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'harden': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'uniaxi': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'residu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '428': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quasi-cylind': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'portnoi': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '387': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'possess': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'centre-bodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inadmiss': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'invalid': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'ring-w': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nullifi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wave-drag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '884': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'damag': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'chilver': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tn.struct.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '125': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'spectrum': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'interv': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'suppli': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '1358': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'budianski': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1557': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '417': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lin': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '273': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'cf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '117-142': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '218-234': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '226': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'proce': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'heisenberg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'so-cal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inflect': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'flex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'syng': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'poiseuil': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '5906': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '502': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tollmien': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'schlicht': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '5314': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '625': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'freez': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'envelop': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'power-law': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'sonic-wedg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'noncatalyt': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'nash': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '22245': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sibulkin': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'm.j.': DF = 15, IDF = 4.5041
[construct_weights][L3] Term 'sharpnos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '73': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1115': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'chief': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'great': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'tietjen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'gain': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'sign': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rayleigh-tollmien': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1-1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1367': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'agenc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'provok': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pau-chang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'graduat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'assist': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '1155': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'petersohn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e.g.m.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n.r.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '883': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heywood': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.b.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '227': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'log': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'aluminium': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'tensil': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'undoubtedli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'predomin': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'reveal': DF = 13, IDF = 4.6423
[construct_weights][L3] Term 'broad': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'classif': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'hole': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'semicircular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ryna': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'b.m.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'dougla': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sm22627': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semi-circular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lomax': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'heaslet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1199': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'foreign-ga': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'freedman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'radbil': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kay': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'iodin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gibbs-dalton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dilut': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n.b.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kinetic-theori': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lennard-jon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6-12': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '8-percent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1-percent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'binari': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '877': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mansfield': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'e.h.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '3115': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'middle-surfac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'braslow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd53': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4-foot': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '01': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'granular-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transition-trigg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'threedimension': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'spot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'promot': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'unaffect': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '1152': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1360': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.r.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'rand': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'turbulen': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'coundari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p.n.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4243': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'interferometr': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '848': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'breath': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'pressuris': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '288': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fluid-cylind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '246': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'falkner': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'v.m.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2279': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trefitz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unequ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'theta': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1394': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'neic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 's.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rutkowski': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'k.k.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'standoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'longer': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'hoshizaki': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'correspondingli': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'fai': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'riddel': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'recover': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '622': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hugh': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'h-p': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dy': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '200': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'km': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'small-eccentr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '200-450': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dai': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'night': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 's.d.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5-10': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'declin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'supposit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1959-61': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shortlifetim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '410': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'f.g.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'edelflet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '9th': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'int': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'astro': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thermochem': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1369': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'globul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cloud': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '89': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'charwat': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'a.f.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '457': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'blunt-bas': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'caviti': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'cutout': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'free-shear': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'depress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mutual': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'protrus': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'laminar-turbul': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'oncom': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'notch': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'layer-fre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'floor-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mach-numb': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'supersonic-wak': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '841': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weingarten': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'v.i.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'batdorf': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'contrari': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '419': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'web': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'rockei': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hitherto': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'singleand': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'double-sid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1164': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'newson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'w.a.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '4124': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'four-engin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vertical-takeoff-and-land': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'transport-airplan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tuft': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wingpropel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nose-down': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'halfwai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1356': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'embed': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'seiff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1304': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ramp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thing': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bluntnos': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'largeangl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'protrud': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pitching-mo': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '284': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'curling-up': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'poisson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '614': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oblat': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'cook': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'd.m.c.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'gw': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'no.g.w.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '533': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'spheroid': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '025': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dealt': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'worst': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sphericalatmospher': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '426': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stator': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'l9g06': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'variable-geometri': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1190': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'zier': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '661': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'chapman-enskog': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'half-rang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'single-pl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hard': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'maxwellian': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'bakanov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deryagin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gyroscop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'scanlan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'truman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ask': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'whirl': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'engine-mount': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tical': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'studies-experiment': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'engine-propel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '270': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mori': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'devel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magneto-hydrodynam': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'gershuni': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'zhukhovitskii': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tao': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '846': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fung': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'y.c.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'sechler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e.e.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'kaplan': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'spectra': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'thin-wal': DF = 19, IDF = 4.2745
[construct_weights][L3] Term 'node': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'regular': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '248': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ohman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'saab': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'supercrit': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'hauser': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'plohr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sonder': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e9k25': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conservation-of-momentum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'staticpressur': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'static-pressur': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'static-to-tot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tangenti': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'took': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1197': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yate': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'j.e.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'zeydel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e.f.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '513': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sparkschlieren': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'length-to-depth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thermal-tomomentum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '879': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'targoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'white': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'r.p.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '706': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '421': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r78': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inducedpressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '613': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1351': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exploratori': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'plume': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'falanga': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd1000': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conecylinder-flar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '11-inch': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'unitari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '317': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '582': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'half-angl': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'solid-propel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'motor': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'flare-cylind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'junctur': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '283': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kemp': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'n.h.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'rose': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'p.h.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'j.ae.sc.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'underestim': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1163': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '11-3-58l': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3-blade': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'single-rot': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'forwardacceler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '631': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'split': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '568': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '403': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'helger': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'astrophi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '117': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '177': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'de': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'hoffman-tel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rankin': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'hugoniot': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'interstellar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'amplifi': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'compression': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'promin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gauss': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ascend': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '4275': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'skip': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'recur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'distinguish': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '255': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lillei': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'r134': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'townsend': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'our': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dispos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1387': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'leggett': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'd.m.a.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1991': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spar': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'midwai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'easi': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '93': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '485': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'plausibl': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'medium-s': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1141': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'zeisberg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 's.l.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1344': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'time-wis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'defect': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1373': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'low-subson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inadequ': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'admiss': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lowsubson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hot-bodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'warhead': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cold-bodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'minimal-surfac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'amick': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd753': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'onetenth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '864': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'advisori': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'committe': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '890': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'david': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'cranfield': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'zuk': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'word': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bulk-head': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'awai': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '1374': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reactiv': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '388': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '94': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'elliott': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ae.scs.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '206': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'zeroth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1146': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'melvin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'anderson': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '252': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'hara': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.r.a.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sweptw': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'blockag': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'open-area': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1380': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'clinton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'franci': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mclean': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'grand': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unrestrict': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'certainli': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'thermopropuls': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'strang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unorthodox': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'go': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'outweight': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bobbitt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p.j.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'malvestuto': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.s.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '2955': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slender-tail': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conformaltransform': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'often-us': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wing-tail': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '404': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kuo': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2356': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'necess': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'incompressible-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hodograph': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '636': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'abraham': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'leiss': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'administr': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'd-1507': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'beneath': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'rocket-exit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'total-pressur': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'rocket-on': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rocket-off': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '48': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1179': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'swigart': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1034': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'garabedian': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'paraboloid': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'wet': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'counter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '897': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'postbuckl': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'kempner': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '173': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'noncircular': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'majorminor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '299': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'meksyn': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '609': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'peckman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2812': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '863': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r40': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1342': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'horsesho': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sum': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'layout': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '84': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '290': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bolz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.e.': DF = 18, IDF = 4.3272
[construct_weights][L3] Term 'coast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'intention': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unintention': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cant': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magnu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'misalign': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1170': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ward': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'd729': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'structural-load': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'airfram': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'vz-2': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '432': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g.j.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dugan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r1088': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slender-w': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'assign': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'body-diameter-maximum-span': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'planar': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'denc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oppos': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'ross': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2647': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'attached-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'part-span': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'anhedr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semi-empir': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'damping-in-yaw': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sidewash': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'non-symmetr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'con': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'app.mech.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'burger': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1184': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '776': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wakelik': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jetlik': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'algebra': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'frozenflow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ehret': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd.m.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '2250': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'similarli': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '69': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'kaattari': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'd860': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'normal-shock': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'high-drag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'capsul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'realga': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '855': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pohl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'f.v.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'nardo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 's.w.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'unless': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'otherwis': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '1389': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '36': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '192': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'cauchi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'seek': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'y1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'y2': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1183': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'expansion-conduct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lykoudi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'p.s.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '772': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'conductionexpans': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sullivan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4115': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'abil': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'withstand': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'correctli': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'angular': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '263': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'meteor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '607': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lundgen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 't.s.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'z.angew.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '100-114': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shercliff': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '435': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n.b.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nada': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd625': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1177': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pride': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4051': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multiweb': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'ring-stiffen': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '899': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'f.k.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '6th': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.a.aero.conf.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'manocuvr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'counterclockwis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1345': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gm-tr-0165-00519': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'modest': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'return': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'multiple-pass': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '297': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'magneto-aerodynam': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mccune': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'resler': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'fluiddynam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prandtl-glauert': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'magnetoacoust': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'anisotrop': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'puls': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '638': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spencer': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'd1482': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '10-foot': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'upper-and': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '852': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'non': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'romano': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pibal': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'median-surfac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'doubli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'apart': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'major-minor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1148': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'knudsen': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'capillari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'demarcu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hopper': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'carbid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'carbon': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'k-25': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'post': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'offic': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'box': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'oak': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ridg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tennesse': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'claus': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reinvestig': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '806': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'feet': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'lina': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'maglieri': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'd235': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'track': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'fighter': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'bomber': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'd-48': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'refract': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'cutoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '208': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sato': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.phys.soc.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1427': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tensor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ohm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'anisotropi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '698': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1940': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unsteady-lift': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'sharpedg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '237': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'code': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'dimmock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'commun': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'deputi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ministri': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'aviat': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'aircraft-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multi-stag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'outcom': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'endors': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sub-committe': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'collabor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wherebi': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'whilst': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '839': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'suer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'skene': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.t.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'benjamin': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'statist': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'unpressur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'best-fit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'one-dimension': DF = 17, IDF = 4.3827
[construct_weights][L3] Term 'double-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wasserman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '924': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '461': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'skan': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'gortler': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'atabeck': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1006': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nonconduct': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'convolut': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '495': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'blast': DF = 10, IDF = 4.8936
[construct_weights][L3] Term 'borcher': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e.f.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '694': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'newton-busemann': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1311': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'self-induc': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'blackstock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't.a.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd798': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'leadingedg': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1123': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tsu-tao': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'loo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'renssela': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'troi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'succeed': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tediou': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1329': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'non-stationari': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sear': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '104': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'vertical-gust': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sinusoid': DF = 16, IDF = 4.4416
[construct_weights][L3] Term 'quarter-chord': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'pre-rot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '801': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '4233': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'winglik': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '459': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tani': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '149': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'offer': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'necessit': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'recours': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1921': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'adequaci': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'du': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dx': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1124': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1316': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'spectrum-lin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'clouston': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gaydon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.g.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'hurl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a252': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p143': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sodium-lin': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'oscillograph': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'filter': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'spectrograph': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'background': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'mirror': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'interpos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bright': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sodium': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '20degreec': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'disequilibrium': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'single-beam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'indium': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blue': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3600degreek': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'resolut': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '492': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ogive-forebodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'earl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'keener': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aerodynamicist': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'edward': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'calif': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '654': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sakuri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '662': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'you-r': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'distance-tim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wkb': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'imai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '466': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vapour': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'screen': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '3ft': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bedford': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'mcgregor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'visualis': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'humid': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'fog': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'latent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'evapor': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'ting-yili': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'emit': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'novel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '230': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tailplan': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'maskel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'e.c.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '2441': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wing-body-tailplan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weapon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'roll-up': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'para': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '239': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'semi-ellipsoid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2247': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '100-230': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'square-edg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '837': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'padlog': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'huff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.d.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'hollowai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wadc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '60-271': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'accumul': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'stress-strain-temperature-tim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multiaxi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'time-vari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inplan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'three-bar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eventu': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'perman': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thermal-stress-fatigu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'test-theori': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'creep-ruptur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '696': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'falangan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jano': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd-893': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nozzle-throat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rocket-chamb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'incrementalnormal-forc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rocket-jet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'farther': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '1112': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bond': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'packard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'b.b.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd859': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pitchingmo': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1320': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hancock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'secondli': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'thirdli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rayleigh-ritz': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '808': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd161': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bypass': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boom-produc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'far-field': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magneto-gasdynam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kerrebrock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nontrivi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sought': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'gaseou': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'cp115': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aerodynamich': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stine': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'h.a.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'wanlass': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3344': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3x10': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'foreknowledg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wedge-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'recovery-temperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shoulder': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '450': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fujihiko': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sakao': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '830': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h.j.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'keller': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'h.b.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'chien': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stoker': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aec': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'univac': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'vs': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '468': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hosowaka': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oswatitsch': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'maeder': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'circular-arc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1318': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stolleri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '22854': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 's.l.r.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1400degreek': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3000degreek': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flash': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mech.app.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hartre': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'ceas': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'end-point': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sychev': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'v.v.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'adjoin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'self-similar': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'supplementari': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '201': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'r1183': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'enlarg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'actur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kink': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stratagem': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prospect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '235': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'seed': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'poiscuil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quasi-one-dimension': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'constant-area': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'maximum-acceler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'accru': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unsolv': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'electrod': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'provis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1327': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'index': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'finite-span': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'rainei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'midchord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '10-percentthick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3-percent-thick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '691': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hydrocarbon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ethylene-air': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'methane-air': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd-914': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compound': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'methan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ethylen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '200degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'modul': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'katzen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'e.d.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'levi': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'l.l.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd1145': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unmodul': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '10g': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '296': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '465': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'confin': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'alfven': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'low-beta': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unidirection': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hall-curr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'high-beta': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'electrostat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'above-ment': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unidirect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'v-shape': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'anomal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'radar': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'echo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '401': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'state-of-the-art': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shortcom': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1176': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3735': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'twenty-f': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heavi': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '262': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '159': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'secur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'told': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bomb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fission': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unaccompani': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'declassifi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'supersed': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'omiss': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'till': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'atm': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'degrad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blast-produc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'born': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1182': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kosson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'grumman': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bethpag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inaccur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'heimenz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ellips': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schubauer': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'driest': DF = 8, IDF = 5.1049
[construct_weights][L3] Term '2597': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wall-to-free-stream': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '434': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spahr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4146': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '03': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'cruciform-w': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ogive-cylind': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '606': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'monaghan': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2407': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1149': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dragutin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stojanov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'beograd': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jugoslavia': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonsteadi': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'tempratur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '639': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1549': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'arrest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fifth': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'painlev': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transcend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '853': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flugg': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '601': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2593': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sustain': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'choos': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'norman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'michael': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '433': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '3227': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'trailing-vortex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'line-vortex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stepwis': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'triangular-w': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cylindrical-bodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semi-span': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'indetermin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crossflow-plan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rolled-up': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'low-aspect-ratio': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'tail-bodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1185': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'culick': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'f.e.c.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '783': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'binary-mixtur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'undissoci': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '57': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rossow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'v.j.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2399': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '265': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '348': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'n.p.l.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '1171': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'soft': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'circular-cylindr': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1343': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1050': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rectilinear': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lie': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'linearizedtheori': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trailing-edge-tip': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '291': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stalker': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1388': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gill': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fourthord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'regist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '854': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '68': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'air-helium': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd49': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'imperfect-ga': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nonvisc': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '3971': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '253': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'spillman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n103': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'whitham-walkden': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sq.ft.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'employ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'afterburn': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1381': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'carro': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a56b15': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fin-stabil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shelter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sheltered-sid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pressure-ris': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'joseph': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd-1521': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'modern': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'hilsenrath': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chalr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'beckett': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bendict': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'liila': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fano': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'harold': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hoge': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'masi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ralph': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nuttal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yeram': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'touloukian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'woollei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dioxid': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'monoxid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '95': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'oliv': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'torda': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1147': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jackson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stalder': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jukoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '944': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'nocturn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'daytim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kept': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'insur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'event': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'skin-cool': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lost': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'radiant': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'accommod': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '1375': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gdalia': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kleinstein': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'modified-oseen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'labori': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'resort': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '608': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sept': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mass-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'waves-dash': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'measurements-dashth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'soundsourc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '862': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r39': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '298': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yen': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'k.t.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '896': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'economi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'high-g': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hybrid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1178': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.s.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'card': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.f.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1513': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thirteen': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'stainless-steel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'circumfer': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1372': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'farmingdal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heterogen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'eddy-viscos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eddyviscos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'redund': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'chrichlow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'haggenmach': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '595': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'urgent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hopelessli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'desk': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'organ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'subroutin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'minimum-s': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cut': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'plane-load': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wing-fuselage-sh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'intersection-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1140': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shock-standoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chaudhuri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '745': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'usaf-sponsor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'shockstandoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'apropri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'specific-heat': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'hai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '402': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hoffman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'teller': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '693': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'relativist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unrelativist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'degener': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '630': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.i.a.a.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'pr': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'merg': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'shocklay': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'valuabl': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '254': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'craven': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'r136': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'imperm': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1386': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1245': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'skin-stiffen': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'griffith': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'miltonberg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g.h.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '3609': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '891': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'almroth': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'b.o.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bruch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd.o.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '573': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'steel': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'pneumat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'encircl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mid-length': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bandwidth': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '865': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nickel-bas': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'glenni': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.inst.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thermal-fatigu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fluidiz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bed': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'media': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'durat': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'domin': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'thermalfatigu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crack': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'intercrystallin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oxid': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'intergranular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '59': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'laminar-boundari': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'w.d.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'donough': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2479': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'main-stream': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'picard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pressure-gradi': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'unheat': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'ture': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'weight-flow': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1319': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nagamatsu': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'h.t.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '083': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6000degreek': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'seen': DF = 10, IDF = 4.8936
[construct_weights][L3] Term '50degre': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'cone-hemispher': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '831': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '556': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '469': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p.f.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thommen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h.you.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'keun': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1114': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'space-vehicl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'payload': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'coe': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x503': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'centaur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'able-v': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spectral': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1326': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'newton': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'spaid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1203': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1300': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1500-lb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'side-forc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thrust-vector': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'zero-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ejector': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gascou': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nozzle-wal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'interfac': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'injector': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'axialthrust': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nozzle-expans': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '690': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'investigaion': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spiked-nos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crawford': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd118': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spike': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '664': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'murrai': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '309': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '456': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trella': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'calor': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'nonspher': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'parabola': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aforement': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'delimit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dore': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '628': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'ballistic-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'undamp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3060': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'full-span': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'constant-chord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hinge-mo': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'plung': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'low-frequ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'avalu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '836': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lingitudin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spragu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'p.c.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '55-350': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'load-temperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'verif': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'coupon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '238': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'ower': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'johansen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.c.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1437': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1931': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'enquiri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '207': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'g.b.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'skramstad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r909': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'german': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '809': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd881': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bow-shock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'liapunov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'park': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'routhhurwitz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'y.h.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '697': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bressett': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '57d19a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-effect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rocket-jet-exit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1321': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't.b.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p513': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'coat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tollmien-schlicht': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'orr-sommerfeld': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nondissip': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wavelength': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'permiss': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'sketchi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'coverag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1113': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'audio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'subaudio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'olsson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'c.o.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'orlik-ruckeman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cathode-rai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'decim': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'dampomet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '510': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pressure-plot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '211': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sidewal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'expansion-wav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spatial': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'logic': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1328': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'willmarth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.w.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'corp': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '2078': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'notic': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'hick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wedge-shap': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'turbojet-pow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'turbojetpow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '467': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'alksn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3970': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'everywher': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'pressure-correct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '655': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'henderson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '4301': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2744': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'manchest': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1317': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shock-tub': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'p184': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'attenu': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'explan': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '1125': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'windenburg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd.f.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1934': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '819': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thinwal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'basin': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'repair': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'navi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sequel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.s.m.e.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '493': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slug': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rubesin': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'johnson': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'perfectga': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'computing-machin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '699': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3639': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'corrobor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '209': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ribner': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'utia': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'r37': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wash': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rippl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'underneath': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'noise-fre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stand': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'link': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'provision': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '807': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hubbard': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd880': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ground-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gross-weight': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '83': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'plast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yashura': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '667': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'categori': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'local-similar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1122': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'perplex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'drew': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'aluminumalloi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1310': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crabtre': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'l.f.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '2695': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'piston-analog': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'next': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'tangent-wedg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tangent-con': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'twoand': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ogivecylind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'claim': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hope': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '728': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '838': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd360': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'smalldeflect': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '236': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rudin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quasi-equilibrium': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '652': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2625': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '460': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '720': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mead': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd.c.ae.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'date': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'mode-frequ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'honeycomb': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'uni-mod': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'meansquar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bond-stress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.m.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '512': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2815': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r129': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1875': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'under-expans': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1296': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eschenroed': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'a.q.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'boyer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'streamtub': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'gas-dynam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nitric': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shuffl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dissociation-recombin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nitrogen-atom': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'three-bodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deplet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '344': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'leadon': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'ingeni': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fd': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'terminolog': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'film': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'exterior': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'enjoi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'usag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'threaten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'seren': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'broken': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'frustrat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'excus': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'poetic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'licens': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eros': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'aerothermochem': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'effus': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'overstat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bibliographi': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'overlook': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'berger': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'etud': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'parietal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'doctor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thesi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'memori': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'poudr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'annex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'imprimeri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'national': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'corrugated-cor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'isotropic-cor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '182': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'blumer': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'agard': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r255': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '12-inch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trip': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'one-fourth': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1262': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'martellucci': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semidiamet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '975': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diatom': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'berthelot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'intermolecular': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'planck': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sea': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'proudman': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'j.r.a.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'no-slip': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uniform-stream': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '981': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bergl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.e.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '251-252': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yeh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1265': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-mix': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-flap': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'momentum-integr': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1057': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'almen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'j.o.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'laszlo': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'annular-disk': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'load-deflect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'versatil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'disclos': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'intent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compressibleflow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'toxic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'freon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'monatom': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'polyatom': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'nontox': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonflamm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inert': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'krypton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'xenon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gas-mixtur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'firing-rang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'concoct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1291': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1427': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'resultant-forc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deceleration-limit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'resultantforc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '343': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'scott': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '171': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sherman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3298': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'low-dens': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'two-third': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '515': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'self': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'neumark': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2839': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'self-sustain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vanishingli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '727': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'herr': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'may': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2024-t3': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'discretefrequ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'siren': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'root-mean-squar': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'shorter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'discrete-frequ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'satisfactorili': DF = 11, IDF = 4.8026
[construct_weights][L3] Term '986': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mixed-flow': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'viii': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shroud': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'redesign': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'osborn': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'w.m.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e56l07': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'skewedparabol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '900': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1500': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'incipi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'surg': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'temperature-ris': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'parabolic-blad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '07': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'circular-blad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'skewed-parabolic-blad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '737': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '805': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3399': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1068': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'herrmann': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'armenaka': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 're-examin': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '540': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'semiinfinit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '7157': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'obscur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'anywai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unlimit': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '718': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'franc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'onera': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mauric': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'twenty-second': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wright': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'brother': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'recherch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aeronautiqu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cosmonaut': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'forefront': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'occas': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'denomin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aerocosmonaut': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'countri': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'abundantli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'circumlunar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'content': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'akin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'illustri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'resourc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '972': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'perpendicularli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'romeo': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sterrett': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'd-743': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '001': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'forwardfac': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '381': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '188': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'seban': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'kelli': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'neither': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'displai': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1253': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wu': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'ching-sheng': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'localsimilar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1061': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vasilu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '19-28': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crank-nicolson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h2o': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h2': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'o2': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'co2': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n2': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'single-step': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '8n': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ibm-704': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '523': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rahman': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'acquir': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'frontal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ao': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shockdetach': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '711': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4degre': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '100degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'delta-w': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'fold': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wingtip': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'x-288': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unfold': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'angle-ofattack': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'attitud': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '375': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'illingworth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'c.r.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'tsion': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cross-drift': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heat-insul': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'whatev': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '147': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'aero.quart.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '79': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'diamond': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unlik': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'serious': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1095': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'large-chord': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'kuhn': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'draper': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3364': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'largechord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mph': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'upward': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'auxiliari': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'dive': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'slotted-flap': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'plain': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mclellan': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'd1476': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'obei': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '178': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dispers': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'w.c.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'kenni': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '286': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1298': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hammerl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tear': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kivel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '422': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'vibration': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'utilis': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'non-dimension': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fell': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'remot': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1092': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wing-nacelle-propel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'herrnstein': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '569': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'full-scal': DF = 11, IDF = 4.8026
[construct_weights][L3] Term 'nacelle-propel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reciproc': DF = 9, IDF = 4.9936
[construct_weights][L3] Term 'nacel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'monoplan': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '15-foot-span': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tractor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5-foot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '30-foot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pressuredistribut': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'propeller-slipstream': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'concur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '372': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'galcit': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '716': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sommer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3-2-59a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'defici': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '524': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rozycki': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'peng': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pindroh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '988': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pump': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'blade-to-blad': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kramer': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'd-1108': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prewhirl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'zero-angle-of-attack': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'drive': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1066': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'free-fre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'doggett': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1391': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hammerhead': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reflex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'power-input': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reducedfrequ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bond-packard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wind-on': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '386': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'porous-wal': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '121': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'couette-typ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1254': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '901': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'acrothermochemistri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '729': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'y.k.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'powel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'modal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dictat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'preval': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '943': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'baum': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '342': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1059': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wempner': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '81e': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'violat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2468': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'easiest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'broke': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'recognis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '319': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gibson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'variant': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'telegraph': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '789': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bourn': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'davi': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'wardl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ovalu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '326': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'forst-ord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 's.f.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'solomon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '114': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'randon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dyer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.acous.s.am.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '922': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lyon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hysteret': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '928': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'brass': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'mild-steel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'duralumin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'proposel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'explor': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'hundr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lundquist': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '742': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '329': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'er': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'michielsen': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '584': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jaeger': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'cam.phil.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '634': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1200': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sweat-cool': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tien': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'c.l.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'gee': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tangentwedg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1032': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bodner': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r.i.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nongyroscop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1238': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonmagnet': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ponderomot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '910': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ayr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jacobson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'l.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '391': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'network': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '548': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'iv': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'extra': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constant-h': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unchang': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'e-versus-tim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1035': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crinolin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'skirt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'loos': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'drape': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'three-wav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fourwav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nearer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magnifi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1207': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '99': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'acrodynam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shockless': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sesir': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'consumpt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '583': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mario': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cardullo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'you.s.': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'lake': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'denmark': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dover': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fluid-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rosenbrock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mercuri': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '577': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '105': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'notion': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lengthwis': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '113': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'signal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'm.w.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'lambert': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '858': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'signal-to-nois': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mask': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'autocorrel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'crosscorrel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'scan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '321': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'three-point': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'christian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constant-pressur': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'single-point': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '328': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'her': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't.r.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'homann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'zamm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oversimplif': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '926': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cylinder': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'small-deflect': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '787': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hasimoto': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '611-619': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'onto': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5772': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1003': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ideal-ga': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1231': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chapki': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pappa': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'okuno': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '919': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.t.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'f.f.a.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ko': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semi-span-to-chord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'low-aspect-ratio-w': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unsteady-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rankine-hugoniot': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'green': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'nall': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '689': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'transparent-wal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spark': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'photographi': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'dry': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'permeabl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stainless': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pore': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'micron': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'psig': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonperm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stagnation-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stream-sid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'noninject': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fluid-solid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '541': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inger': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'g.r.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sharp-nos': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'affin': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'literatur': DF = 12, IDF = 4.7192
[construct_weights][L3] Term 'theory-review': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.c.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'februari': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'munk': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'source-sink': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'breadth': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'not-so-slend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'supersonic-airfoil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '579': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heat-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'dirac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1209': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'downwash-imping': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vidal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'coher': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'entrain': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'subsidiari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'impinging-jet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'woodlei': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2847': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'electroformed-nickel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'temperature-tim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '774': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sauer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '310': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'princeton': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'r195': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'keen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'interanl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'meyer': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hpyerson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '122': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'willi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jacob': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lockhe': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'georgia': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conical-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'abbrevi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'equivalent-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'newtonian-theori': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1236': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '610': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '536-537': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1004': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'contamin': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '2-inch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'centerlin': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'center-lin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '780': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transonic-bump': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'krumm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3502': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thickness-to-chord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'am': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '63a2xx': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '63a4xx': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'annulu': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'inman': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r.m.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '532': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tacitli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1255': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hunzik': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.r.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'maxwel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deby': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1067': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'eighth-ord': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'moderate-length': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '989': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'splitter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd-1186': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'impart': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'main-blad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'splitter-van': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '373': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'airspe': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'savin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'syvertson': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'c.a.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'geodes': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '14-in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'affirm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hamilton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hufton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '151': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rocket-born': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rocket-launch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'airborn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1093': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'buried-fan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spreemann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd731': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'buri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nose-up': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '525': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'pressure-interact': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pressureinteract': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '717': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'martian': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'v.l.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tn-d': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'probe': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'passiv': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '12degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '40degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'six-degree-offreedom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rigid-bodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '10-rpm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ampl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'encompass': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'flat-bas': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nose-forward': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'base-forward': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'heating-r': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '25degre': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'preserv': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1058': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3rd': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'nat': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'cong': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'appl': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'skin-panel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '942': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.r.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'shandor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transduc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vacuum': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'catalyt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'decompos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '710': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'smallest': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'a.m.o.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'clutter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '229-245': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '256': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sandpap': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '522': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cone-cylinder-flar': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'callahan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1403': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stagnation-to-wal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'f.p.r.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fraenkel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'typifi': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'spillag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1094': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'redirect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3629': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'large-diamet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'static-thrust': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nearest': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '374': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'zoom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'turbojet': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'euler-lagrang': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'blowout': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1060': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.b.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '380': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bending-tors': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dugundi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'theodorsen': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'diagon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'garrick': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'c.g.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 're-emphas': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1252': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dorranc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '161': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1299': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '520': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'decad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ago': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '179': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2137': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'axiallysymmetr': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inviscid-fluid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inviscidflow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '945': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stockman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n.o.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1562': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blade-surfac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1418': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'afterflow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1290': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n.s.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'foughner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.t.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'd1616': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sting-mount': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wall-mount': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stripanalysi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l57l10': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'j.ze.scs.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'detriment': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'influx': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '726': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '753': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'throw': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cumul': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '514': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2823': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1056': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'forrai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'forum': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'session': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 's.m.f.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ff-30': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'in-plan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'strain-displac': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'satisfact': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thermo-mechan': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'immov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tain': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '184': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thermo-aeroelast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '294': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thermoaeroelast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1264': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hartunian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'russo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'marron': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'p.v.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '587': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'turbulent-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '719': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'remmler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '92-95': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'librat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'commenc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'massachusett': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blowdown': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '973': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vinson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '12-5-58w': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stagnation-to-free-stream': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-tobodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1069': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '010': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '987': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'turbo-machin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'axial-radial-and': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2604': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'turbomachin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'z-plane': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'somewher': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'off-design': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1263': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonslend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1051': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '183': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'patterson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r52': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'arm': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'britton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2821': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '721': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'callaghan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'how': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'w.l.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'mull': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1338': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'over-al': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'jet-nozzl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'near-field': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sound-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'free-field': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-engine-exhaust': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'turbulent-veloc': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'cold-air': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nozzle-exit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'strouhal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cross-correl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'microphon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nozzleexit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thereaft': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'frequency-band': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1297': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'electron-ion': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd-c': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-bodi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'deioniz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'never': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sheath': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'barri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'neumann': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'e.p.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '229': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'shock-boundarylay': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stimulu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constantdens': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'interferomet': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'johannesen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'director-gener': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'scientif': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'fully-expand': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shadow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'half-jet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'error-integr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '980': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'adiabatic-wal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hill': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'triangl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diversifi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thick-wal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '148': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '383': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'external-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '974': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'journal': DF = 6, IDF = 5.3731
[construct_weights][L3] Term '1677-1685': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'layer-shock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1208': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'motionless': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'laplac': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '920': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'camb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p307': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '578': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '32pp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '285-287': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stage-stack': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'lowspe': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'multiple-valu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'intermediate-spe': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'part-spe': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'high-pressure-ratio': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1005': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'design-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1237': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ness': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '645': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'light-ga': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'influence-coeffici': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sewal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd-515': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'liftingsurfac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nine': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '775': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'caret': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'catheral': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2835': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'undersurfac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '123': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cresci': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'j.aer.scs.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'subtend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'machnumber-8': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boundary-layer-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '311': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2763': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'warn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '927': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mello': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'body-w': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'feed': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'closed-contour': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'velocity-perimet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '316': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3109': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '17901': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '918': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'single-degree-offreedom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '124': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'morri': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd.n.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'apprais': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'forb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dewei': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jnr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '698-702': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '738': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '786': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'batchelor': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'g.k.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '179-192': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'steadili': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'crosssect': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'linder': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'overlap': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'ambigu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1230': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'workman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sheer': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '833': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'high-pressur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1400': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'psia': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'va': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'sci': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '65-74': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flat-fac': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'hemispherically-nos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hemispherical-nos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '911': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'b.l.': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'ford': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'you.s.a.a.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '128': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'southampton': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'caravel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'air-lin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'takeoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stringer-twist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mode-shap': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '549': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vortex-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hartnett': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pursu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'minnesota': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'vortex-tub': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1239': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'supersonic-aircraft': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reverse-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'implement': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'scarshaack': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diamond-profil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '576': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vibrationally-frozen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'highly-cool': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'complete-compressor-stal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'abrupt-stal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'multistag': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'stage-match': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bleed': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '744': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'end-shorten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ab': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '320': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'leigh': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '112': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'phil.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1206': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cumberbatch': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1476': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wave-front': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wavefront': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mhd': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-lob': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uncontrol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'melt': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'goodman': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'shea': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heat-bal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '788': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'varlei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '601-614': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'atlarg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 're-entr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '318': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'moeckel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'round-nos': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'flat-nos': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '916': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't.n.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'non-steadi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '585': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1683': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'volterra': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stefan-boltzmann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'adsorpt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chemisorpt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1033': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1201': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'miel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '168': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'centripet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonneg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'erdmann-weierstrass': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'switch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'subarc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '929': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'marguerr': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'decis': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ellipselik': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unsymmetr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oval-sh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '115': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lubric': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'constantinescu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'v.n.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'proc.inst.mech.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '881': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'proceed': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '327': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'invok': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flatplat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '743': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thielemann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deutsch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'versuchsanstalt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'luftfahrt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mulheim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ruhr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'germani': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'twenti': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'dvl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rsult': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '571': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ultra-high': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2550': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6500': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '826': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'noninsul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slip-flow': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'serpico': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pressure-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'frank': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'assoc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'muller': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'neighborhood': DF = 7, IDF = 5.2300
[construct_weights][L3] Term '1103': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1103a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transcendent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1048': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1008': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'elastic-behavior': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '707': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heat-sustain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hanawalt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bless': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '257-263': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'manner-dash': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'long-rang': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'glider': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'steadyst': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'redistribut': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'enhanc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '535': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dunlap': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'astronaut': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'ann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'arbor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mich': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1083': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thom': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1194': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1928': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'occupi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mean-squar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'time-deriv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '363': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'abramson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'h.n.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'kutta-joukowski': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'fictiti': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'potentialflow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1077': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'dihedr': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'trimpi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r-63': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '65degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '75degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ardc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '999': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'treon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1327': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lesser': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sednei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '189': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '397': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'turcott': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd.l.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '169': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'b.sc.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.f.r.ae.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '955': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mccomb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mikula': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.m.': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1289': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sobson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'murrow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pratt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd1501': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lifting-surfac': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'in-phas': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reduced-frequ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1242': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ordnanc': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'momentari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'instant': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transverse-wav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'longitudinalvortic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'overshadow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fale': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reserv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '390': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'panel-flutt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'advoc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'clamped-edg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'rayleigh-typ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1070': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '364': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pa': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'radom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sixth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'integral-differenti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1084': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '651': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'arithmet': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '156': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't.h.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1922': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '499': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ship': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'deep': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'treatis': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'quotat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trochoid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weitbrecht': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pitch-yaw': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '874': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'popular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'you.s.s.r.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'doublet': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1079': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'niclei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'q.j.mech.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p35': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '997': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r-6': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '399': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '187-198': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'agraw': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h.c.': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '709': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '99degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'olstad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x-610': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wing-tip': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'duce': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '49degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1274': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p241': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '000degreek': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r833': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1046': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '504': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '289': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'nonzero': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'redefinit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'woinowsky-krieg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bisector': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '352': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'murduchow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dorodnitsyn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1280': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ginzel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multhopp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cumbersom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sigular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '160': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r-15': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonvanish': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hypersonic-slender-bodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'riolat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '158': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'induct': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'heisler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.p.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.s.m.e.trans.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'histor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'trial-and-error': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '964': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flowmet': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'venturi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'riva': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '489-497': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rounded-entr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tional': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'length-diamet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'long-radiu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tap': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'untest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'precaut': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'instal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '990': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stanitz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'v.d.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2421': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mixedflow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'impeller-tip': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '167': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'oxygen-lik': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '355': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sinclair': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'philadelphia': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1287': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gregori': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '602': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inconclus': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diaz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schwarz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inequ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'add': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'you-v': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sequenc': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '503': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sinnott': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'transonic-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'drag-ris': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'onset-of-separation-effect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'krqu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'analytici': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '10-percent-thick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slender-airfoil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'verfi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5000': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1041': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wenk': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'love-meissn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kelvin': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'argument': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'clear': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'geckel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l.n.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'complimentari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '301': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sharp-corn': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nack': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sunderland': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'consult': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'elec': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '133': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.s.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '557': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'k.h.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'feldman': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'by-characterist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '765': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vafako': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1347': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '791': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oge': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '2658': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1227': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'naleid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thompson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '940': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'impact-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1015': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'concret': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sometim': DF = 9, IDF = 4.9936
[construct_weights][L3] Term '930': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dill': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd826': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fourth-ord': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1218': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semiballistic-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'half-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cone-seg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sphere-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'balsa': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'isofoam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'liter': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'free-flown': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'millisecond': DF = 5, IDF = 5.5402
[construct_weights][L3] Term '1012': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mcgraw-hil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'marin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1220': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'komoda': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '440': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'ribbon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inevit': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '796': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'carborundum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'grain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '762': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'gatewood': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'gehr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'north': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'strain-analysi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'load-strain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'room-temperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bending-mo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'beyween': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '550': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sparrow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '61-wa-165': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '908': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crandal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '739': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stochast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exagger': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hash': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'firm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dramat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pose': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spectacular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'malfunct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'antonio': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '306': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2818': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bartl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '937': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'frustum': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'synthesi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '592': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'howel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deflection-tim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1216': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'step-induc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'separated-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jet-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dead-air': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '102': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sobei': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '330': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'b.c.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'methanol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tetrachlorid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wave-numb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cut-off': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'third-ord': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'overst': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'competit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '754': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '566': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'goodwin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'creager': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.o.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'winkler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a55h31': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dropoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1229': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hypersoul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'summat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'subtract': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'avov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '40-50': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cosin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '559': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3513': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3430': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'force-fre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '561': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'graham': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'e.w.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '771': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mountain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x-y': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'y-axi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'radon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kernel-funct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gravitz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'laidlaw': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bryce': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cooper': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'quasi-unsteadi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multi-degree-of-freedom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kernelfunct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'parameter-vari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sophist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '337': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mass-inject': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '939': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constant-temp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magnetoga': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'berlot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '748': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nose-tail': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'asid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hypersonic-similar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'speak': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'q.app.math.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1211': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'speedsthree-dimension': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '12-in': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1023': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'elaps': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'east': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pennelegion': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'barrel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oversw': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'microwav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chamber': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'driven': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'ms': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '906': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'folson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '308': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'yasuhara': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'holder': DF = 8, IDF = 5.1049
[construct_weights][L3] Term 'reader': DF = 6, IDF = 5.3731
[construct_weights][L3] Term 'hero': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aerfoil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'small-scal': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'repercuss': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'steady-mot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'roar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inter-rel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'scarc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '991': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wing-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prolat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lopatoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l51e09': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pressure-drag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compens': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'unexpectedli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trial': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '965': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'simmon': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3449': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dusinberr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1272': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rodden': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'revel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'supersonic-hyperson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'strip-theori': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1040': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stepdown': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'judici': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pressure-reynold': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'free-mix': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'large-step': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'addendum': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'prandtl-mey': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'sharpli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '354': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eratur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'speric': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1286': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shyperson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bernstein': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'reflected-shock': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'statistical-mechan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constantli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shock-process': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '166': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r117': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonheat-conduct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'amongst': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'convergentdiverg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'concis': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '752': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '730': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'rock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ritz': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'render': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'deduct': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'mitig': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stoni': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'meteorit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 're-solidif': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'afford': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '708': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ladson': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '15degre': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '398': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ranni': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1939': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'emeri': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd618': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'plateau': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'staticpressure-ris': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '014': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1078': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'apelt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3175': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'navierstok': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'laplacian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'large-deflexion': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '505': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'navy-sponsor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aeroballist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r-1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '353': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sunnyval': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'protect': DF = 7, IDF = 5.2300
[construct_weights][L3] Term 'lightweight': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1281': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '195': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'photo-thermoelast': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'tramposch': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'photothermoelast': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'thermal-stress': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'duplic': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fring': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '1047': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'russian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inaccess': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'grossman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lyskov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'specialis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'attaindd': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'indefinit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'labour': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1275': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unsteadili': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gregg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'respond': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1288': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'broadwel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'freestream-inject': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deficit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'morgan': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'spera': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd.a.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '455': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'cylinderhemispher': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cone-spher': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'toru': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mismatch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonconcurr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'closur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n102': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stagnant': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'de-excit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eucken-valu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hirschfeld': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cell': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'eucken': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1085': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3061': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mesh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '157': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'roy.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '323': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'x-axi': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'triple-valued': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'single-valu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '365': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-point': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '701': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2562': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'circulatori': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'noncirculatori': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'apparentmass': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'apparent-mass': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'acquisit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ambrosio': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wortman': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mescal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prebuckl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1243': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yoler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '313': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'suppress': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thin-plat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1049': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'libov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hubka': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2289': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fasten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '953': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'belief': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1244': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sin-i': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'broad-spectrum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quadrupol': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sound-emit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schemat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crucial': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'shear-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'downstream-propag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'silenc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'teeth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '396': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'convection-fundament': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'onsag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '998': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'staff': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1135': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'revis': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1076': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2773': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '534': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1014': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'low-density-nonun': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'som': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonstationari': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '629': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'wagner': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'notabl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'senior': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'southwest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '8500': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'culebra': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'road': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'san': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'largeaspect-ratio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1082': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1-dash': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2-dash': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.g.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'o.you.e.l.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '070': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.m.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2983': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'orlfic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hemispheroid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'proc.roy.s.a.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'steepest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1022': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'forty-thre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '40-inch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '16-inch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5052-0': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oven': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1210': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '594': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bratt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'not-so-thin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2660': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rhombic': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '560': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transpiration-cool': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inouy': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3969': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reev': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'a.r.s.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '517': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '938': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '336': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '799': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'paramount': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'risk': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '907': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hunter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rous': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mcnown': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'iowa': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'citi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'undertook': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'variable-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'request': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'out-growth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bellman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pennington': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sine': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '103': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd.b.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '763': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'butler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'extinct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'premix': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heimel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '567': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'penland': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3861': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '755': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2668': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '593': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'runyam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'huckel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonlinear-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1217': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'levinski': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unconstrain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crook': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'strain-tim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '75s-t6': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'low-alloi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constant-stress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '558': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'slatteri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'clai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '500-in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'motion-pictur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bryer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '477': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'brower': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sub-cas': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mixing-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compression-corn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '551': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'liveslei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.k.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'birchal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ms26': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deuc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '307': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nagakura': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'narus': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tangent-wedge-approxim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '135': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diaconi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ludwieg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ludweig': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '909': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'confid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1221': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1013': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unimport': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '797': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'j.e.g.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3270': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'undroop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sweptforward': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '936': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'donnell-typ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'median': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'wellknown': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'mil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'zero-inject': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '790': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1258': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'single-degree-of-freedom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'feedback': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'strain-gag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'circuitri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'short-period': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'palt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1226': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'faulder': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'density-viscos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '132': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ed': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.m.davies.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'c.you.p.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sir': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'geoffrei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exception': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'remaind': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hopf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 're-der': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'union': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'non-plan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'finest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tribut': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inspir': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kippenhan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'orgin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'surpris': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '764': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lindholm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kana': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd.d.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1052': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'shallow-sh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bottom': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'liquid-slosh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'highfrequ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'beat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yarymovych': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1219': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bryson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'drag-modul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kellei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'calculus-of-vari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'optimum-program': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'acceleration-toler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kendal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '47-56': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5-dash': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '931': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'vlasov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prikl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'meh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '109placement': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '843': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1341': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '479': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'viewpoint': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1159': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6500-foot-per-second': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bernard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'samuel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'krau': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'shock-compress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3660': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '003': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4th': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '616': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'upper-atmospher': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'grove': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'g.v.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '16-27': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semi-major': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'p-28': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '59/0.15': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h-200': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5/0.028': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '013': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gram': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kilometr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bracket': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '424': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'holl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1937': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'boison': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'supply-stream': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'single-el': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magnified-schlieren': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1192': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shee-mang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thyson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n.a.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '672': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '272': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'polish': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pyrex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ellipse-cylind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'microinch': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stagnation-towal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '888': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1224': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1166': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd56': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vertical-take-off-andland': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exit-area': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dirt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sq': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'thoroughli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'soak': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'loose-dirt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sprai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'neater': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1354': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '11in': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2223': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'freeli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'arnold': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'warburton': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bell': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thickness-diamet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'evolv': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'aural': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'precess': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reed': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bland': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 's.r.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd659': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'precession-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aircraft-engine-propel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'power-pl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'backup': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1398': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a131': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '281': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'haag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1353': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-step': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2171': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1161': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stark': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.i.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'periment': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lunar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wong': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slye': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r-80': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mid-cours': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bergmann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lowaspect-ratio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slope-compat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'assembl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'electric-analog': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1195': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dosanjh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sheerlan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'underexpand': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'area-mach': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '423': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'machel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bryant': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'guggenheim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spherically-blunt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '064': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nose-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spherical-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'foredrag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2025': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'enthalpy-veloc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '725': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '886': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'air-fram': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tubular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unrestrain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1168': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multipropel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stol': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'gravel-cov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pegg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd535': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gravel': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'inadvert': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'taxiing-turn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'macadam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crush': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chase': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deposit': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'punctur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'high-aspectratio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rolling-up': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '872': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'temperature-field': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'peclet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '618': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h.f.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'plenum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '255-310': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'equatori': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'belt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'anomali': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'avenu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1365': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1949-1950': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'velocityprofil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'velocitycompon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1157': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.v.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '332-340': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'driver': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'devreas': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'left': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'trails-i': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'expansion-control': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conduction-control': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conductioncontrol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'widen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2nd': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'congr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'industri': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'shockinduc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unpublish': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '627': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rattayya': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'allround': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flutter-mod': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'five-term': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1391': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '185-194': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'continuum-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'delin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '243': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'brevoort': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r963': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3401': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '875': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'templeton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'omit': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'viz': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'gravit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'veiociti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'surface-temperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'surfacetemperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inappropri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'entitl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'miner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a159': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'expend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'algranti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd280': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'forward-fuselag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'turbulence-intens': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wall-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'forward-fuselagesect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unduli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1396': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'simply-support': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'i.t.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fralich': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '244': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sweet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2023': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'visibl': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'societi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'denser': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oil': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'prematur': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'bluish': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'colour': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'noncorros': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'non-irrit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'smell': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'troubl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wax': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'floccul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bore': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'educ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '620': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'priester': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kramp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '200-204': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '200-700': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'consolid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kallmann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h180': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '20-cm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ultra-violet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fl-layer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x-rai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diurnal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'season': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '660': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'sputnik': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vanguard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1958-octob': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'physcal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fllayer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f2-layer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '412': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'houbolt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'decidedli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1150': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mcisaac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stergi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cambridg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '63-24': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'singli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flown': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '370': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'morn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hour': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'glass-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'irbm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd564': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'melting-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'opaqu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nondecompos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'glass': DF = 4, IDF = 5.7409
[construct_weights][L3] Term '635': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'corollari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'light-weight': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1362': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'warner': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lans': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'irv': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ratner': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'warp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r820': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1909': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1917': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inflow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'fixedpitch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'advance-diamet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nd': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'one-third': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'lever': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cross-coupl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'therefrom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'scheu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '126': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '400-401': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '446': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'electrohydrodynam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '674': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'beaslei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2620': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '680': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'heaslett': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2497': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quasi-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'single-integr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1336': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vondoenhoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3000': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lowturbul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1104': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stonei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l58': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e05a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'creas': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yy': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1932': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '821': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '261': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'engess': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reduced-modulu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reducedmodulu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1309': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'origint': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1027-42': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '219': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1331': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flight-test': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bennett': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'random-process': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '687': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '402-406': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '673': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wallac': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1935': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'parasol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ld': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hingeaxi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'counteract': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '441': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stall-flutt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'halfman': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'halei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2533': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reaffirm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'translatori': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'time-averag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'happen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kubota': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magasarian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'o.l.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'despit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '217': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'converging-diverg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rothstein': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '228': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'i-de': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1832': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sandwich-typ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '483': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prism': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rigidli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unflang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1307': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd962': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1428': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'near-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yawed-cylind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heat-flux': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'box-w': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '221': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'axi-symmetr': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ensur': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'baron': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2206-61': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gather': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prime': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heretofor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'co-curr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'keulegan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sextic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f40umer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quartic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '45degre': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'unangst': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l55k30': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '26-inch': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sweptback-w': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'solidconstruct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '65a004': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'centerof-grav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magnetic-field-fre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wyatt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'geophi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inapplic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jastrow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pears': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sole': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '810': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3-4-59l': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '470': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'miyai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '10th': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'iii-4': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '642': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2151': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aeroelastician': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ashlei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'zartarian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'point-funct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bendingtors': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'control-surfac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wherev': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'analyses-': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'panels-i': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aerodynamic-thermoelast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1912': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'non-rotation': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1312': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hollow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'outer-edg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hollowcylind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1132': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'galletli': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '259': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'machined-stiffen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kendrick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'embryon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lobe': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'threshold': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weaken': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '484': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '889': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'destal': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'smallshear': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'uniform-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '817': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hankelman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prager': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'meant': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r51': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slice': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'muffler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multiple-nozzl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quiet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'frequency-shift': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'emerg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sound-attenu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mccaulei': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1323': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nonisotherm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '14x10': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '880': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mccarthi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'willett': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'duncan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1425': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'worthi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mckinnon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1363': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ashwel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1151': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.p.l.prog.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '20-372': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'proton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sinchtel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'c.d.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd671': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'geophys': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'igi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'igc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mev': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'three-year': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'began': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'terrestri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'identif': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'hazard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'be': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1397': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1851': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '245': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'huggett': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '713': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '4046': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unpow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'present-dai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4047': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'offset': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'artific': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '413': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r82': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nyholm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r847': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '100x10': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'float': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '621': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'latitud': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '280': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'discover': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '212-216': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '220': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'night-tim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'polar-region': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '873': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'surface-heat-transf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '619': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'v184': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1267-1270': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'day-to-night': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r1296': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'equal-span': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4-vortex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'centroid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1169': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hangl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reeder': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd735': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'accentu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'level-flight': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rudimentari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'encourag': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'undesir': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '887': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cooler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bulkhead': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '626': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'semiapex': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'overexpans': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'underexpans': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'touch': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'genuin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'influenti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '414': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5th': DF = 5, IDF = 5.5402
[construct_weights][L3] Term 'volta': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'const': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sake': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'nitzburg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1813': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'drag-diverg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'crest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'divers': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1390': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'liebersten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'h.m.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reformul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '242': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kirk': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'f.n.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2377': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'asimplifi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boat-tail': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1156': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4072': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'time-histori': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shock-waveveloc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'millimet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'helium-driven': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hydrogen-driven': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'efficaci': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1364': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'regan': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '227-253': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thrown': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1399': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't.k.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a269': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ing': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jack': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '4094': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'finish': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '15x10': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1250': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '845': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'freely-support': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'end-plat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'air-receiv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'convair': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diego': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'abovement': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sarason': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weitzner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unperturb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hyperlipt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'superimpos': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'grad': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '274': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weigh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '8640': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reenter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'molten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gersten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'braunschweig': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'leissa': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'niedenfuhr': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '116': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'pli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'intract': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1160': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deton': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'oppenheim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.k.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chapman-jouguet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kingdom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stanbrook': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2712': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1352': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3709': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1158': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tailored-interfac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'merl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hertzberg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'cornel': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'gollnick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'absent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '287': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r969': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1355': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r22': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'insulated-pl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boundary-layer-induc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1167': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'twin-propel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1239': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'snow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'obliter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vision': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'recircul': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'propeller-blad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'superfici': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fiber': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'erod': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'eas': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '425': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'macneal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dynamic-analog': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'circuit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'set-up': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'variable-thick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '617': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '28-34': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'air-dens': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sub-perige': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'capiaux': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'karchmar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '61-201-1904': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ias-ar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1193': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ehrich': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '675': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'synthes': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-paramet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'locu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1133': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'washington': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1301': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '9740': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hantzsch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wendt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reid': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '643': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'wing-aileron': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gaukrog': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'curran': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.k.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'torsion-control': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'under-massbalanc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '471': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '829': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'item': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'experimental-failur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'theoretical-buckl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ascrib': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '145': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'unbuckl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'randal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cp394': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'straight-edg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '688': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'armstrong': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'w.o.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '127': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '85degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'real-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '218': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'laurenc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r1292': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5-inchdiamet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'correlogram': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'easier': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '816': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'low-fineness-ratio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'letko': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1-18-159l': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'allcock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.m.i.e.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.m.brit.i.r.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tanner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.sc.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'i.e.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mclachlan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.m.brit.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'i.r.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ampli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aerelast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bisplinghoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'art': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'acrothermoelast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '476': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '397-408': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'u1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'u2': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'erfc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jmin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'recurr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dashn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jmjn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '644': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'finite-differ': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bernoulli-eul': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'plane-strain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gridwork': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'antielast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '482': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4170': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1306': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'holt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'blacki': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fort': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'halstead': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1134': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'caution': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'edit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'proof-test': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diameter-thick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'psi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'undergon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'safe': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'interim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd704': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'socal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '811': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'walkden': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'whichev': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1339': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'steady-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'center-ofgrav': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lindsei': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'w.f.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'landrum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4204': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '37': DF = 4, IDF = 5.7409
[construct_weights][L3] Term 'various': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unsepar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pankhurst': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pitman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'octagon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '216': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'kantrowitz': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'r974': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'acr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l5d20': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ineffici': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'singlestag': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '818': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hodg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prandtl-reuss': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hencki': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1330': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wilei': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1925': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hankel': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1102': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'five-stag': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'swanson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3-6-59': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'solid-fuel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sounding-rocket': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boost': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nautic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '686': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'miller': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4-8-59l': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '15-percent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bevel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shaft': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'elastic-axi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '827': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3212': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dome': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'surpass': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'finite-displac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2471': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stagger': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'angularli': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'sideforc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'haslet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.b.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1054': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1105': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd791': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'access': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1337': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'barmbi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'length-chord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '715': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f-35': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'garner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'walsh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '982': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3244': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '9x10': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '447': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yoshira': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'self-consist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'collision-fre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boltzmann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dielectr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1308': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rilei': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unsatisfactori': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inconveni': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'climin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '478': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1913': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5d': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '23849': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cuss': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '820': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wellestablish': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hamper': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonexist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'polyaxi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'legitim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'contradict': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'allud': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1118': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '802': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'valueless': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'neighbourhood': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '668': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1315': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'argon-fre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hydrogen-oxygen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diaphragm': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'uniform-sect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1127': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mautner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fifty-on': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unload': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'v-groov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'papreg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cellular': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'cellulos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'acet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '00675': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '02025': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '066': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '741': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'bulg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'skew': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '491': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'plane-couett': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'burton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'seventh-pow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'firmer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2508': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'diamond-shap': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'normalpressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '657': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gies': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'bergdolt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'v.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'interferogram': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2420': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'brenckman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boundary-layer-control': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd48': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fighter-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'radartrack': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tailwind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ground-reflect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'objection': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'plate-glass': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'near-son': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'multilay': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '340': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'wassermann': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '234': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deni': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3527': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'noninclin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mehtod': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'radian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'referenc': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'stanton-jon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '61-45': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '462': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gilbert': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.c.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'photoelast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'paraplex': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'p-43': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thermal-expans': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'plane-stress': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thermal-shock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'length-depth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '496': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'buzz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '712': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1120': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'secant': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'iteration-i': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'self-gener': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '659': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '406': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unretard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2p': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1129': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '269': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'energy-method': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '666': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '454': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'colloc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hyperbolic-trigonometr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'finitediffer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fewer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '202': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2492': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'violent': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'inerti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'airscrew': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'monograph': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flutter-prevent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'airload': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1116': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4237': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1324': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2779': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'china-clai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pimpl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'protuber': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'laminar-flow': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '004': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.p.h.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '002': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sea-level': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '692': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l54e05a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'propulsive-jet': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '498': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '834': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rieding': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'manager-rand': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'div': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'klebanoff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '803': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'dryden': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '695': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l56e07': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '300degreer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 're-establish': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2742': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unfavour': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1111': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.a.aero.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.a.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '419-446j': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '205': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'maillard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a55c08': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '64a010': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quarterchord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'infiniteaspect-ratio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wing-sect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-dimensional-airfoil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inboard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '95-percent-semispan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'span-load': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '464': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'exploit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'law-spe': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wedgetyp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'livingood': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.n.b.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3588': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wedge-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'variabletemperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stream-to-wal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wall-temperatur': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1145': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'core-stabil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'don': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'brush': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boundary-layer-shockwav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1377': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ofr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'talbot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4327': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semivertex-angl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'viscous-interact': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ramet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1383': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vonkarman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '251': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'naysmith': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2423': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bluff-afterbodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd-1503': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'combut': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '407': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g.z.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shukhovitskii': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'soviet': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1348': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yoshikawa': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1074': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weight-to-area': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '894': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '438': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'super': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'offenhartz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wisblatt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flagg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '860': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mw-1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heldenfel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rosecran': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'grifith': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l53e27': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unanticip': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'connor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1559': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lower-limit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '632': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'joyc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2836': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1384': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'th': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'glanc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'waist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2879': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'four-bai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bohon': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'd921': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bai': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'psf': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '1370': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1142': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tirumalesa': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'satyanarayana': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-lay': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stonecyph': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 't.e.': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'barrier': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'grober': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1189': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonreact': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '867': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nimon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'landau': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '850': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'loop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'non-conduct': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'selfpreserv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '893': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'salter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unexplain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '260': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4335': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1180': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schetz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dorodnitzinhowarth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deflagr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boiloff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'booster': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '436': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'super-satellit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'prep': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2173-61': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hirshfeld': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.chem.phys.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '604': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'farnborough': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'crane': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'fuse': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'silica': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'centrelin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mackenzi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ninefold': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reference-enthalpi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1346': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'grant': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd452': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'price': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1174': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'langhaar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'expound': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bleich': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'salmon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'goodier': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'append': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'you.s.a.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'readi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '409': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '176-180': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blunt-trailing-edg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'isoenerget': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '851': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kirchhoff-lov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'late': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'friedman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'windstream': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fage': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'townend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1379': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sce': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '217-227': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'debat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'breguet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flow-dash': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'highenergi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1173': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'angel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '293': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wisniewski': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'advent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1-4': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sharp-tip': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dubos': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rml53g10a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aspect-ratio-2': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aspect-ratio-6': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '603': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heater': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'centigrad': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'realis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quickact': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'valv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cure': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'preheat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quick-act': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p.s.i.g.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fibrou': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hotter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hog': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'greenwood': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2725': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'grape': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3296': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1187': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aren': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spiegler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '869': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p.w.h.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eigenvector': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sub-divis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multiply-connect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '258': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slider': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chou': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'y.t.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'saibei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slider-bear': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'load-carri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '856': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sylvest': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3914': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'magnesium': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'korkegi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'phosphoresc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lacquer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'total-head': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rake': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hasten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '835': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wright-patterson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'temperature-load': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-el': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'greensit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constanc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'condamp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'coefstant': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exponentiallyfici': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spoken': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'essenc': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'laguerr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nice': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'puzzl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'afresh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unfamiliar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '204': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hunton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l.w.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a55c23': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'certainti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'secondary-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'whitehead': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'basic-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '452': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eulerian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quadruplet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l56106': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nacelleexit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1110': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ah': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1322': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonweil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't.r.f.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '22670': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'resili': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'non-visc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'altogeth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1128': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'yuseff': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '0005': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '832': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wegstein': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'comm': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'minneapoli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'entireti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1325': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3083': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'drill': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'low-drag-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5-ft': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'throttl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'handlei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ltd': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'suck': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1117': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'whisker': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tailor': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'blowdown-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cross-le': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'glick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hyp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'proj': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'improp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stanford': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'classifi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bridg': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'surface-heat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '203': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sinott': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3045': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multicellular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '804': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mullen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'afftc-tn-56-20': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dev': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'command': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'you.s.a.f.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'popularli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'widespread': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f-100': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5524-f1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '497': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1313': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schultz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3265': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'primary-shock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'straight-through': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sight': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3726': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'plasticity-reduct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'maximum-shear': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'octahedral-shear': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eminton': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2564': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ensu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p43': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'castolit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'epoxi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'resin': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'hysol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6000-op': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'focu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thereof': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '669': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'haberman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'carderock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'md': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'caplan': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'tamada': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '1119': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '656': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'brai': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'k.n.c.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '983': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'begun': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '3472': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vapor-screen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ink-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '232': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2764': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conical-shock-expans': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1126': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conceptu': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '62-106': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'foundat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1314': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1852': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '000k': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '490': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gundersen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'magnetic-field': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '430': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4ft': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'andrew': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '2820': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'high-supersonic-spe': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2716': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lay-out': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'workingsect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '266': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.n.c.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pierc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'solid-bodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fredholm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'seidel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'edpm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '868': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stanworth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd.s.c.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'n.g.t.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2784': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sixth-degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'seventh-degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1186': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'messit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vertex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'piece-wis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rotationally-symmetr': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'coars': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'velocitydistribut': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'slide': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1340': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'solid-construct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'abbott': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'f.t.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3423': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1172': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gore': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'exp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'radialband': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'strengthen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unfil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '857': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'width-length': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd833': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '78847': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '21153': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'equilibria': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e.v.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1175': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bijlaard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p.p.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yuan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rectangl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tion': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '295': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constantino': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'economo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hypersonic-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'persh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'luiden': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd590': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'g-load': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deepest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hot-ga': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'keat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd349': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wall-cool': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1181': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'c.k.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lynn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'y.m.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonalign': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'superfast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unexpect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'soc.ae.sc.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'richmond': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '605': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semi-angl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'over-expans': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'arsj': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fourcompon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1378': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blunt-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'burk': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jame': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'curti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'degree-half-angl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '408': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'regir': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'grasshof': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1143': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'one-foot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'high-enthalpi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'b.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1428': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'combustionh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ottawa': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ont': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'canada': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'heat-input': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'capacit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1371': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'swirl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lengthen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2849': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'near-spher': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dissociationrecombin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bimolecular-exchang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bimolecular': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'post-shock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'infinite-r': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '859': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aluminium-alloi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gui': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l.d.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1353': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multibai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'external-skin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6-foot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '17-7': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ph': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x-15': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unsupport': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'modified-thickness-ratio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unstress': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'panel-support': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'panel-skin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2864': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1385': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'speed-distribut': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shortli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'financi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '892': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'laid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '268': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '866': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'k.f.a.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '379': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'previouslygiven': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hot-fatigu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1188': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'goebel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't.p.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boyd': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yawed-con': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flat-top': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'singletyp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inner-shock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1382': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2427': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'womerslei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3325': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'viscid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'considered-dashthat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'enthalp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonrigor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'goetz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1487': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shaft-mount': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-mod': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1376': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'p-function': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'master': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.i.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1865': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1144': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'newsom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'loui': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd-1382': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'multiple-propel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stronger': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deeper': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'intensif': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'intensifi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'struck': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'errat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reanalyz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reanalysi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rogh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'height-thick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'air-foil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3804': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dalei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'humphrei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '861': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'faget': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3811': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '895': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'airforc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1349': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fetterman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x127': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-interfer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-static-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'researchtyp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compressed-air': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-static-pressureratio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1460': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'horizontal-tail': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jet-boundari': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'jetboundari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '913': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.s.c.e.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'band-pass': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '779': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3748': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'well-establish': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'imaginari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1204': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bloxson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rhode': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'b.v.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'totalangl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '000effici': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'paint': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x10': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1036': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'internat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'orderof-magnitud': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '580': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thermomechan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'body-forc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'castigliano': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '574': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '821-835': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-compon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'length-react': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'broadbent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'two-fold': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'massbal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'efflux': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '322': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'toba': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'p480-1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shock-gener': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '110': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lucid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'strong-shock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eleg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dismiss': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't-y': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'coordinate-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '914': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'isaac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aero-elast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'negat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'defi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tractabl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'preclud': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'premis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'beg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ban': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mather': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'power-funct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cx': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'repetit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schuh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '0904': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wave-length': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '195e': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hayer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'daughadai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'half-spac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rememb': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1031': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trefftz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'curved-beam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1421': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nonisentrop': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'disturbance-which': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'minhinnick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'surface-tab': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1038': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'edge-load': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '777': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mag': DF = 3, IDF = 5.9922
[construct_weights][L3] Term 'longest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '545': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wing-fin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sell': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'c.c.l.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2805': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gothic': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'free-jet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1007': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'perfect-ga': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'oblique-shock': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1235': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bounari': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'grai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3030': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lmethod': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tail-boom': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tailless': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '589': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'progressive-typ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'instig': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-engin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vodicka': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'app.sc.res.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gorcum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'four-pol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stratiform': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '925': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'til': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '5576': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '784': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'buoyanc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1232': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'curtain': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'undersid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kirchhoff-helmholtz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'steamlin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'intrieri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1299': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'knox': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4363': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'yakura': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flow-visu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'expansion-turn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '542': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lardner': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 't.j.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '770': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'janzen-rayleigh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '186': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'cp271': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inasmuch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sizeabl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conecylind': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'height-to-chord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1266': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '172': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cortright': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'propulsion-system': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vital': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'air-induct': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'goal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nozzle-afterbodi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jetstream': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shockboundary-layer-interact': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'enlighten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1292': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hinson': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1352': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-on': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-plum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nozzle-area': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schjeldrup': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'high-pow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ey': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'endeavour': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'convict': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'assur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '516': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2798': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'longitudinal-st': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transonic-tunnel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1259': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inhomogen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '985': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'k.j.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hamrick': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'stream-fila': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'meridional-plan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '529': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gous': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thermocoupl': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'self-balanc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'potentiomet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'micromanomet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'manomet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nylon': DF = 3, IDF = 5.9922
[construct_weights][L3] Term '971': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cubbison': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd-580': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'arrow-w': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reentrytyp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '511': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm359': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '723': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'alan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spacewis': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1295': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '949': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1333': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nozzle-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd-693': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '175': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chinneck': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2782': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1261': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uram': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ary-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boundwel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'depict': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pseudo-two-dimension': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1053': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pressure-deflect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dead-weight': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '976': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hroma': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'coalesc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'swallow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '000-10': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spite': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l-band': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '000-ft': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'uhf': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'requenc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1098': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'walton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x-263': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6-inchdiamet': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unabl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'arc-heat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '378': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '585-587': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '40-5': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'finite-thick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'copper': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'molybdenum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tungsten': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '371': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'letter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'demarc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'halfangl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'charter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mach-zehnd': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1091': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vtol-stol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'huston': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'winston': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd397': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '008': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'vertregt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'j.brit.inter.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'energy-requir': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '4048': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'accident': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'daiber': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schlieren-photomultipli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'floor': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '385': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1065': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'witt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd1500': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'researchvehicl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'in-flight': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ablatedlength': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '518': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'co-planar': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'platinum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r1019': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pre-asymptot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'anh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'numger': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1268': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '700f': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1062': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rubin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'loitsianskii': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bolshakov': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'velocity-enthalpi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constant-dens': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'merger': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'outermost': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'momentum-area': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'displacement-area': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dilat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '382': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'howard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inyokern': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'china': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'low-aspect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hammond': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.d.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'd-1374': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'untrim': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'subsonic-lift': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'benton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tail-interfer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wingtail': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '978': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inafinit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schlesing': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sashkin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1096': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l58e22': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lucit': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'polystyren': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'inorgan': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'salt': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'ammonium': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'chlorid': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'phenol': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'melamine-fib': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lamin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '144': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'j.am.r.s.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thickness-to-radiu': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '376': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'le': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '349': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lieberstein': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'recurs': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '947': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semi-vertex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fohrman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1648': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '25x10': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'axial-forc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wing-body-tail': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nielsen': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'j.n.': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'anastasio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3959': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'trapezoid': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'unbank': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a51j04': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a52b06': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '118': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '381-397': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '588': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'benser': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e56b03b': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'xiii': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stall-limit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'severli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stage-load': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'clipping': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r794': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'twenty-on': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'eniac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shortest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '315': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm3012': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'transition-fix': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'spuriou': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'physico-math': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'raleigh': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'philo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stack': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'doyl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.d.c.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aquart': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '785': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '312-321': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1001': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd-1202': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '18degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'damping-in-pitch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1233': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sturm-liouvil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1039': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'abrupt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '923': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2950': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a50f06': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'higherord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '749': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3011': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1f': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '8a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flat-head': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '7090': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flow-field': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '782': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'queiijo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3245': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'discrete-horseshoe-vortex': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vertical-tail': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'total-tail-assembl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dodecagon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '312': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '6-1-59l': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '16-seri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'granvil': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'boxal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'brit.j.app.phys.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'furnac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '915': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'naaa': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'al1029': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'forcibl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conidt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '90000': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '220000': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '129': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gerrard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'octav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'doppler': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'monopol': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '586': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'szewczyk': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sensibl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1202': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'square-root': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vertexcent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'vertex-cent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pyramid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'weaker': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1030': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'adjacentequilibrium-posit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '572': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'golian': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '353-381': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'strong-interact': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'c.a.l.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '15-dashin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thin-film': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'test-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '740': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r.p.n.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2190': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1942': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1914': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'one-quart': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '912': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hama': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'recesso': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'christiaen': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'electrically-h': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bare': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '02-in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'thermal-lay': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fifti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'indirectli': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '778': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e.o.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'astrophys': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quantum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bodt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'gyrat': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'canard': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1-32': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd.c.f.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hartree-womerslei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'choleski': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1037': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1205': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hemi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kueth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1454': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '9-in': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dewpoint': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'utilz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dust': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '581': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'handicap': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'i-sect': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hosokawa': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'sub': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'local-linear': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5degre': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mayo': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x-222': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '377': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stewartson-illingworth': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x-coordin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unspecifi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'x-transform': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1097': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'levin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l58e15a': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'haveg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rocketon': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heat-absorpt': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'heat-sink': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stagnationpoint': DF = 2, IDF = 6.3287
[construct_weights][L3] Term 'lehnert': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'schermerhorn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wake-flow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1251': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quarter': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quarter-infinit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ifer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '274-180': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'semi-converg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1063': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'russel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '946': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1605': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jetexit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'shock-displac': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wall-properti': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'compressible-fluid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'incompressible-fluid': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kleeman': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2971': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'ob': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1064': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'six-propel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1509': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'largescal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'double-slot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '2-percent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '40-percent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1256': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '384': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lavend': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cone-cylinder-frustum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'conecylinder-frustum-boost': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'cone-frustum': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1090': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wing-propel': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'curri': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'dunsbi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'lr-284': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'twin-engin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'profound': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '142': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aero.eng.rev.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'nomograph': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'anthoni': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'casaccio': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'quasi-spher': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'herriot': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unif': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '526': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'mabei': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'liabl': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'closest': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1269': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'chinitz': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'local-wav': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'fluid-dynam': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'wimbrow': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kester': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r1109': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'blunttrailing-edg': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'stroboscop': DF = 2, IDF = 6.3287
[construct_weights][L3] Term '519': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '941': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'florida': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'delft': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'holland': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'atten': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bimetal': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tradit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1260': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rosenzweig': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'm.l.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'high-frequ': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'unresolv': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '722': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'a.a.s.you.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'loudspeak': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'usaf-support': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'apoge': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'graze': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 're-orbit': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '174': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bromm': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'l54i16': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '9-inch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'e53h25': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '948': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd1385': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'bulent': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'momentum-thick': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'i.w.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'user': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1256-57': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r-ft1': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'deffer': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'flax': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'adjoint': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1099': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'r-9': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '977': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1930': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '341': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'foelsch': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'undetermin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1293': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hast': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '3224': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'interchang': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'reaction-resist': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'airlin': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'pod-mount': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'aircrafttyp': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jet-induc': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1267': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kogan': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '187': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'kuehn': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'oil-film': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '5000000': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'classificaiton': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'tni': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'incident': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '1055': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '528': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'jai': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'silver': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'zero-pressure-gradi': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'constant-wall-temperatur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'firstord': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '970': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'd-649': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'center-of-pressur': DF = 1, IDF = 6.8395
[construct_weights][L3] Term '984': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'hub-shroud': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'o.k.': DF = 1, IDF = 6.8395
[construct_weights][L3] Term 'rarefied-air': DF = 1, IDF = 6.8395
[main][L1] Scoring complete. Vocabulary size = 9415
[main][L2] Processing query 1: what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft .
[retrieveDocuments][L2] Retrieving documents for query: what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['similar', 'law', 'must', 'obei', 'construct', 'aeroelast', 'model', 'heat', 'high', 'speed', 'aircraft']
[retrieveDocuments][L3] Retrieval complete. 801 documents found.
[main][L2] Query 1: Retrieved 801 documents.
[main][L2] Processing query 2: what are the structural and aeroelastic problems associated with flight of high speed aircraft .
[retrieveDocuments][L2] Retrieving documents for query: what are the structural and aeroelastic problems associated with flight of high speed aircraft .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['structur', 'aeroelast', 'problem', 'associ', 'flight', 'high', 'speed', 'aircraft']
[retrieveDocuments][L3] Retrieval complete. 750 documents found.
[main][L2] Query 2: Retrieved 750 documents.
[main][L2] Processing query 3: what problems of heat conduction in composite slabs have been solved so far .
[retrieveDocuments][L2] Retrieving documents for query: what problems of heat conduction in composite slabs have been solved so far .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['problem', 'heat', 'conduct', 'composit', 'slab', 'solv', 'far']
[retrieveDocuments][L3] Retrieval complete. 654 documents found.
[main][L2] Query 3: Retrieved 654 documents.
[main][L2] Processing query 4: can a criterion be developed to show empirically the validity of flow solutions for chemically reacting gas mixtures based on the simplifying assumption of instantaneous local chemical equilibrium .
[retrieveDocuments][L2] Retrieving documents for query: can a criterion be developed to show empirically the validity of flow solutions for chemically reacting gas mixtures based on the simplifying assumption of instantaneous local chemical equilibrium .
[retrieveDocuments][L3] Query tokenized into 19 tokens: ['can', 'criterion', 'develop', 'show', 'empir', 'valid', 'flow', 'solut', 'chemic', 'react', 'ga', 'mixtur', 'base', 'simplifi', 'assumpt', 'instantan', 'local', 'chemic', 'equilibrium']
[retrieveDocuments][L3] Retrieval complete. 1178 documents found.
[main][L2] Query 4: Retrieved 1178 documents.
[main][L2] Processing query 5: what chemical kinetic system is applicable to hypersonic aerodynamic problems .
[retrieveDocuments][L2] Retrieving documents for query: what chemical kinetic system is applicable to hypersonic aerodynamic problems .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['chemic', 'kinet', 'system', 'applic', 'hyperson', 'aerodynam', 'problem']
[retrieveDocuments][L3] Retrieval complete. 719 documents found.
[main][L2] Query 5: Retrieved 719 documents.
[main][L2] Processing query 6: what theoretical and experimental guides do we have as to turbulent couette flow behaviour .
[retrieveDocuments][L2] Retrieving documents for query: what theoretical and experimental guides do we have as to turbulent couette flow behaviour .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['theoret', 'experiment', 'guid', 'turbul', 'couett', 'flow', 'behaviour']
[retrieveDocuments][L3] Retrieval complete. 966 documents found.
[main][L2] Query 6: Retrieved 966 documents.
[main][L2] Processing query 7: is it possible to relate the available pressure distributions for an ogive forebody at zero angle of attack to the lower surface pressures of an equivalent ogive forebody at angle of attack .
[retrieveDocuments][L2] Retrieving documents for query: is it possible to relate the available pressure distributions for an ogive forebody at zero angle of attack to the lower surface pressures of an equivalent ogive forebody at angle of attack .
[retrieveDocuments][L3] Query tokenized into 18 tokens: ['possibl', 'relat', 'avail', 'pressur', 'distribut', 'ogiv', 'forebodi', 'zero', 'angl', 'attack', 'lower', 'surfac', 'pressur', 'equival', 'ogiv', 'forebodi', 'angl', 'attack']
[retrieveDocuments][L3] Retrieval complete. 1038 documents found.
[main][L2] Query 7: Retrieved 1038 documents.
[main][L2] Processing query 8: what methods -dash exact or approximate -dash are presently available for predicting body pressures at angle of attack.
[retrieveDocuments][L2] Retrieving documents for query: what methods -dash exact or approximate -dash are presently available for predicting body pressures at angle of attack.
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['method', 'dash', 'exact', 'approxim', 'dash', 'present', 'avail', 'predict', 'bodi', 'pressur', 'angl', 'attack']
[retrieveDocuments][L3] Retrieval complete. 1205 documents found.
[main][L2] Query 8: Retrieved 1205 documents.
[main][L2] Processing query 9: papers on internal slip flow heat transfer studies .
[retrieveDocuments][L2] Retrieving documents for query: papers on internal slip flow heat transfer studies .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['paper', 'intern', 'slip', 'flow', 'heat', 'transfer', 'studi']
[retrieveDocuments][L3] Retrieval complete. 1000 documents found.
[main][L2] Query 9: Retrieved 1000 documents.
[main][L2] Processing query 10: are real-gas transport properties for air available over a wide range of enthalpies and densities .
[retrieveDocuments][L2] Retrieving documents for query: are real-gas transport properties for air available over a wide range of enthalpies and densities .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['real-ga', 'transport', 'properti', 'air', 'avail', 'wide', 'rang', 'enthalpi', 'densiti']
[retrieveDocuments][L3] Retrieval complete. 596 documents found.
[main][L2] Query 10: Retrieved 596 documents.
[main][L2] Processing query 11: is it possible to find an analytical,  similar solution of the strong blast wave problem in the newtonian approximation .
[retrieveDocuments][L2] Retrieving documents for query: is it possible to find an analytical,  similar solution of the strong blast wave problem in the newtonian approximation .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['possibl', 'find', 'analyt', 'similar', 'solut', 'strong', 'blast', 'wave', 'problem', 'newtonian', 'approxim']
[retrieveDocuments][L3] Retrieval complete. 974 documents found.
[main][L2] Query 11: Retrieved 974 documents.
[main][L2] Processing query 12: how can the aerodynamic performance of channel flow ground effect machines be calculated .
[retrieveDocuments][L2] Retrieving documents for query: how can the aerodynamic performance of channel flow ground effect machines be calculated .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['can', 'aerodynam', 'perform', 'channel', 'flow', 'ground', 'effect', 'machin', 'calcul']
[retrieveDocuments][L3] Retrieval complete. 1144 documents found.
[main][L2] Query 12: Retrieved 1144 documents.
[main][L2] Processing query 13: what is the basic mechanism of the transonic aileron buzz .
[retrieveDocuments][L2] Retrieving documents for query: what is the basic mechanism of the transonic aileron buzz .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['basic', 'mechan', 'transon', 'aileron', 'buzz']
[retrieveDocuments][L3] Retrieval complete. 149 documents found.
[main][L2] Query 13: Retrieved 149 documents.
[main][L2] Processing query 14: papers on shock-sound wave interaction .
[retrieveDocuments][L2] Retrieving documents for query: papers on shock-sound wave interaction .
[retrieveDocuments][L3] Query tokenized into 4 tokens: ['paper', 'shock-sound', 'wave', 'interact']
[retrieveDocuments][L3] Retrieval complete. 417 documents found.
[main][L2] Query 14: Retrieved 417 documents.
[main][L2] Processing query 15: material properties of photoelastic materials .
[retrieveDocuments][L2] Retrieving documents for query: material properties of photoelastic materials .
[retrieveDocuments][L3] Query tokenized into 4 tokens: ['materi', 'properti', 'photoelast', 'materi']
[retrieveDocuments][L3] Retrieval complete. 157 documents found.
[main][L2] Query 15: Retrieved 157 documents.
[main][L2] Processing query 16: can the transverse potential flow about a body of revolution be calculated efficiently by an electronic computer .
[retrieveDocuments][L2] Retrieving documents for query: can the transverse potential flow about a body of revolution be calculated efficiently by an electronic computer .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['can', 'transvers', 'potenti', 'flow', 'bodi', 'revolut', 'calcul', 'effici', 'electron', 'comput']
[retrieveDocuments][L3] Retrieval complete. 1042 documents found.
[main][L2] Query 16: Retrieved 1042 documents.
[main][L2] Processing query 17: can the three-dimensional problem of a transverse potential flow about a body of revolution be reduced to a two-dimensional problem .
[retrieveDocuments][L2] Retrieving documents for query: can the three-dimensional problem of a transverse potential flow about a body of revolution be reduced to a two-dimensional problem .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['can', 'three-dimension', 'problem', 'transvers', 'potenti', 'flow', 'bodi', 'revolut', 'reduc', 'two-dimension', 'problem']
[retrieveDocuments][L3] Retrieval complete. 1079 documents found.
[main][L2] Query 17: Retrieved 1079 documents.
[main][L2] Processing query 18: are experimental pressure distributions on bodies of revolution at angle of attack available .
[retrieveDocuments][L2] Retrieving documents for query: are experimental pressure distributions on bodies of revolution at angle of attack available .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['experiment', 'pressur', 'distribut', 'bodi', 'revolut', 'angl', 'attack', 'avail']
[retrieveDocuments][L3] Retrieval complete. 968 documents found.
[main][L2] Query 18: Retrieved 968 documents.
[main][L2] Processing query 19: does there exist a good basic treatment of the dynamics of re-entry combining consideration of realistic effects with relative simplicity of results .
[retrieveDocuments][L2] Retrieving documents for query: does there exist a good basic treatment of the dynamics of re-entry combining consideration of realistic effects with relative simplicity of results .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['exist', 'good', 'basic', 'treatment', 'dynam', 're-entri', 'combin', 'consider', 'realist', 'effect', 'rel', 'simplic', 'result']
[retrieveDocuments][L3] Retrieval complete. 1100 documents found.
[main][L2] Query 19: Retrieved 1100 documents.
[main][L2] Processing query 20: has anyone formally determined the influence of joule heating,  produced by the induced current,  in magnetohydrodynamic free convection flows under general conditions .
[retrieveDocuments][L2] Retrieving documents for query: has anyone formally determined the influence of joule heating,  produced by the induced current,  in magnetohydrodynamic free convection flows under general conditions .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['anyon', 'formal', 'determin', 'influenc', 'joul', 'heat', 'produc', 'induc', 'current', 'magnetohydrodynam', 'free', 'convect', 'flow', 'gener', 'condit']
[retrieveDocuments][L3] Retrieval complete. 1157 documents found.
[main][L2] Query 20: Retrieved 1157 documents.
[main][L2] Processing query 21: why does the compressibility transformation fail to correlate the high speed data for helium and air .
[retrieveDocuments][L2] Retrieving documents for query: why does the compressibility transformation fail to correlate the high speed data for helium and air .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['compress', 'transform', 'fail', 'correl', 'high', 'speed', 'data', 'helium', 'air']
[retrieveDocuments][L3] Retrieval complete. 775 documents found.
[main][L2] Query 21: Retrieved 775 documents.
[main][L2] Processing query 22: did anyone else discover that the turbulent skin friction is not over sensitive to the nature of the variation of the viscosity with temperature .
[retrieveDocuments][L2] Retrieving documents for query: did anyone else discover that the turbulent skin friction is not over sensitive to the nature of the variation of the viscosity with temperature .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['anyon', 'els', 'discov', 'turbul', 'skin', 'friction', 'sensit', 'natur', 'variat', 'viscos', 'temperatur']
[retrieveDocuments][L3] Retrieval complete. 549 documents found.
[main][L2] Query 22: Retrieved 549 documents.
[main][L2] Processing query 23: what progress has been made in research on unsteady aerodynamics .
[retrieveDocuments][L2] Retrieving documents for query: what progress has been made in research on unsteady aerodynamics .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['progress', 'made', 'research', 'unsteadi', 'aerodynam']
[retrieveDocuments][L3] Retrieval complete. 521 documents found.
[main][L2] Query 23: Retrieved 521 documents.
[main][L2] Processing query 24: what are the factors which influence the time required to invert large structural matrices .
[retrieveDocuments][L2] Retrieving documents for query: what are the factors which influence the time required to invert large structural matrices .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['factor', 'influenc', 'time', 'requir', 'invert', 'larg', 'structur', 'matric']
[retrieveDocuments][L3] Retrieval complete. 591 documents found.
[main][L2] Query 24: Retrieved 591 documents.
[main][L2] Processing query 25: does a practical flow follow the theoretical concepts for the interaction between adjacent blade rows of a supersonic cascade .
[retrieveDocuments][L2] Retrieving documents for query: does a practical flow follow the theoretical concepts for the interaction between adjacent blade rows of a supersonic cascade .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['practic', 'flow', 'follow', 'theoret', 'concept', 'interact', 'adjac', 'blade', 'row', 'superson', 'cascad']
[retrieveDocuments][L3] Retrieval complete. 1005 documents found.
[main][L2] Query 25: Retrieved 1005 documents.
[main][L2] Processing query 26: what is a single approximate formula for the displacement thickness of a laminar boundary layer in compressible flow on a flat plate .
[retrieveDocuments][L2] Retrieving documents for query: what is a single approximate formula for the displacement thickness of a laminar boundary layer in compressible flow on a flat plate .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['singl', 'approxim', 'formula', 'displac', 'thick', 'laminar', 'boundari', 'layer', 'compress', 'flow', 'flat', 'plate']
[retrieveDocuments][L3] Retrieval complete. 1130 documents found.
[main][L2] Query 26: Retrieved 1130 documents.
[main][L2] Processing query 27: how is the design of ring or part ring wings by linear theory affected by thickness .
[retrieveDocuments][L2] Retrieving documents for query: how is the design of ring or part ring wings by linear theory affected by thickness .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['design', 'ring', 'part', 'ring', 'wing', 'linear', 'theori', 'affect', 'thick']
[retrieveDocuments][L3] Retrieval complete. 834 documents found.
[main][L2] Query 27: Retrieved 834 documents.
[main][L2] Processing query 28: what application has the linear theory design of curved wings .
[retrieveDocuments][L2] Retrieving documents for query: what application has the linear theory design of curved wings .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['applic', 'linear', 'theori', 'design', 'curv', 'wing']
[retrieveDocuments][L3] Retrieval complete. 832 documents found.
[main][L2] Query 28: Retrieved 832 documents.
[main][L2] Processing query 29: what is the effect of cross sectional shape on the flow over simple delta wings with sharp leading edges .
[retrieveDocuments][L2] Retrieving documents for query: what is the effect of cross sectional shape on the flow over simple delta wings with sharp leading edges .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['effect', 'cross', 'section', 'shape', 'flow', 'simpl', 'delta', 'wing', 'sharp', 'lead', 'edg']
[retrieveDocuments][L3] Retrieval complete. 1148 documents found.
[main][L2] Query 29: Retrieved 1148 documents.
[main][L2] Processing query 30: papers on flow visualization on slender conical wings .
[retrieveDocuments][L2] Retrieving documents for query: papers on flow visualization on slender conical wings .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['paper', 'flow', 'visual', 'slender', 'conic', 'wing']
[retrieveDocuments][L3] Retrieval complete. 940 documents found.
[main][L2] Query 30: Retrieved 940 documents.
[main][L2] Processing query 31: what size of end plate can be safely used to simulate two-dimensional flow conditions over a bluff cylindrical body of finite aspect ratio .
[retrieveDocuments][L2] Retrieving documents for query: what size of end plate can be safely used to simulate two-dimensional flow conditions over a bluff cylindrical body of finite aspect ratio .
[retrieveDocuments][L3] Query tokenized into 16 tokens: ['size', 'end', 'plate', 'can', 'safe', 'us', 'simul', 'two-dimension', 'flow', 'condit', 'bluff', 'cylindr', 'bodi', 'finit', 'aspect', 'ratio']
[retrieveDocuments][L3] Retrieval complete. 1250 documents found.
[main][L2] Query 31: Retrieved 1250 documents.
[main][L2] Processing query 32: to find an approximate correction for thickness in slender thin-wing theory .
[retrieveDocuments][L2] Retrieving documents for query: to find an approximate correction for thickness in slender thin-wing theory .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['find', 'approxim', 'correct', 'thick', 'slender', 'thin-w', 'theori']
[retrieveDocuments][L3] Retrieval complete. 816 documents found.
[main][L2] Query 32: Retrieved 816 documents.
[main][L2] Processing query 33: how do interference-free longitudinal stability measurements (made using free-flight models) compare with similar measurements made in a low-blockage wind tunnel .
[retrieveDocuments][L2] Retrieving documents for query: how do interference-free longitudinal stability measurements (made using free-flight models) compare with similar measurements made in a low-blockage wind tunnel .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['interference-fre', 'longitudin', 'stabil', 'measur', 'made', 'us', 'free-flight', 'model', 'compar', 'similar', 'measur', 'made', 'low-blockag', 'wind', 'tunnel']
[retrieveDocuments][L3] Retrieval complete. 1040 documents found.
[main][L2] Query 33: Retrieved 1040 documents.
[main][L2] Processing query 34: have wind tunnel interference effects been investigated on a systematic basis .
[retrieveDocuments][L2] Retrieving documents for query: have wind tunnel interference effects been investigated on a systematic basis .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['wind', 'tunnel', 'interfer', 'effect', 'investig', 'systemat', 'basi']
[retrieveDocuments][L3] Retrieval complete. 781 documents found.
[main][L2] Query 34: Retrieved 781 documents.
[main][L2] Processing query 35: are there any papers dealing with acoustic wave propagation in reacting gases .
[retrieveDocuments][L2] Retrieving documents for query: are there any papers dealing with acoustic wave propagation in reacting gases .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['paper', 'deal', 'acoust', 'wave', 'propag', 'react', 'gase']
[retrieveDocuments][L3] Retrieval complete. 430 documents found.
[main][L2] Query 35: Retrieved 430 documents.
[main][L2] Processing query 36: has anyone investigated relaxation effects on gaseous heat transfer to a suddenly heated wall .
[retrieveDocuments][L2] Retrieving documents for query: has anyone investigated relaxation effects on gaseous heat transfer to a suddenly heated wall .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['anyon', 'investig', 'relax', 'effect', 'gaseou', 'heat', 'transfer', 'suddenli', 'heat', 'wall']
[retrieveDocuments][L3] Retrieval complete. 885 documents found.
[main][L2] Query 36: Retrieved 885 documents.
[main][L2] Processing query 37: are there any theoretical methods for predicting base pressure .
[retrieveDocuments][L2] Retrieving documents for query: are there any theoretical methods for predicting base pressure .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['theoret', 'method', 'predict', 'base', 'pressur']
[retrieveDocuments][L3] Retrieval complete. 1005 documents found.
[main][L2] Query 37: Retrieved 1005 documents.
[main][L2] Processing query 38: does transition in the hypersonic wake depend on body geometry and size
[retrieveDocuments][L2] Retrieving documents for query: does transition in the hypersonic wake depend on body geometry and size
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['transit', 'hyperson', 'wake', 'depend', 'bodi', 'geometri', 'size']
[retrieveDocuments][L3] Retrieval complete. 555 documents found.
[main][L2] Query 38: Retrieved 555 documents.
[main][L2] Processing query 39: how can one detect transition phenomena in boundary layers .
[retrieveDocuments][L2] Retrieving documents for query: how can one detect transition phenomena in boundary layers .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['can', 'on', 'detect', 'transit', 'phenomena', 'boundari', 'layer']
[retrieveDocuments][L3] Retrieval complete. 779 documents found.
[main][L2] Query 39: Retrieved 779 documents.
[main][L2] Processing query 40: how can one detect transition phenomena in hypersonic wakes .
[retrieveDocuments][L2] Retrieving documents for query: how can one detect transition phenomena in hypersonic wakes .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['can', 'on', 'detect', 'transit', 'phenomena', 'hyperson', 'wake']
[retrieveDocuments][L3] Retrieval complete. 663 documents found.
[main][L2] Query 40: Retrieved 663 documents.
[main][L2] Processing query 41: has anyone investigated and developed a simple model for the vortex wake behind a cruciform wing .
[retrieveDocuments][L2] Retrieving documents for query: has anyone investigated and developed a simple model for the vortex wake behind a cruciform wing .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['anyon', 'investig', 'develop', 'simpl', 'model', 'vortex', 'wake', 'behind', 'cruciform', 'wing']
[retrieveDocuments][L3] Retrieval complete. 845 documents found.
[main][L2] Query 41: Retrieved 845 documents.
[main][L2] Processing query 42: what is a criterion that the transonic flow around an airfoil with a round leading edge be validly analyzed by the linearized transonic flow theory .
[retrieveDocuments][L2] Retrieving documents for query: what is a criterion that the transonic flow around an airfoil with a round leading edge be validly analyzed by the linearized transonic flow theory .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['criterion', 'transon', 'flow', 'around', 'airfoil', 'round', 'lead', 'edg', 'validli', 'analyz', 'linear', 'transon', 'flow', 'theori']
[retrieveDocuments][L3] Retrieval complete. 1059 documents found.
[main][L2] Query 42: Retrieved 1059 documents.
[main][L2] Processing query 43: can the transonic flow around an arbitrary smooth thin airfoil be analysed in a simple approximate way .
[retrieveDocuments][L2] Retrieving documents for query: can the transonic flow around an arbitrary smooth thin airfoil be analysed in a simple approximate way .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['can', 'transon', 'flow', 'around', 'arbitrari', 'smooth', 'thin', 'airfoil', 'analys', 'simpl', 'approxim', 'wai']
[retrieveDocuments][L3] Retrieval complete. 1097 documents found.
[main][L2] Query 43: Retrieved 1097 documents.
[main][L2] Processing query 44: what are the details of the rigorous kinetic theory of gases . (chapman-enskog theory) .
[retrieveDocuments][L2] Retrieving documents for query: what are the details of the rigorous kinetic theory of gases . (chapman-enskog theory) .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['detail', 'rigor', 'kinet', 'theori', 'gase', 'chapman-enskog', 'theori']
[retrieveDocuments][L3] Retrieval complete. 562 documents found.
[main][L2] Query 44: Retrieved 562 documents.
[main][L2] Processing query 45: has anyone investigated the effect of surface mass transfer on hypersonic viscous interactions .
[retrieveDocuments][L2] Retrieving documents for query: has anyone investigated the effect of surface mass transfer on hypersonic viscous interactions .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['anyon', 'investig', 'effect', 'surfac', 'mass', 'transfer', 'hyperson', 'viscou', 'interact']
[retrieveDocuments][L3] Retrieval complete. 984 documents found.
[main][L2] Query 45: Retrieved 984 documents.
[main][L2] Processing query 46: what is the combined effect of surface heat and mass transfer on hypersonic flow .
[retrieveDocuments][L2] Retrieving documents for query: what is the combined effect of surface heat and mass transfer on hypersonic flow .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['combin', 'effect', 'surfac', 'heat', 'mass', 'transfer', 'hyperson', 'flow']
[retrieveDocuments][L3] Retrieval complete. 1110 documents found.
[main][L2] Query 46: Retrieved 1110 documents.
[main][L2] Processing query 47: what are the existing solutions for hypersonic viscous interactions over an insulated flat plate .
[retrieveDocuments][L2] Retrieving documents for query: what are the existing solutions for hypersonic viscous interactions over an insulated flat plate .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['exist', 'solut', 'hyperson', 'viscou', 'interact', 'insul', 'flat', 'plate']
[retrieveDocuments][L3] Retrieval complete. 750 documents found.
[main][L2] Query 47: Retrieved 750 documents.
[main][L2] Processing query 48: what controls leading-edge attachment at transonic speeds .
[retrieveDocuments][L2] Retrieving documents for query: what controls leading-edge attachment at transonic speeds .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['control', 'leading-edg', 'attach', 'transon', 'speed']
[retrieveDocuments][L3] Retrieval complete. 344 documents found.
[main][L2] Query 48: Retrieved 344 documents.
[main][L2] Processing query 49: can the three-point boundary-value problem for the blasius equation be integrated numerically,  using suitable transformations,  without iteration on the boundary conditions .
[retrieveDocuments][L2] Retrieving documents for query: can the three-point boundary-value problem for the blasius equation be integrated numerically,  using suitable transformations,  without iteration on the boundary conditions .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['can', 'three-point', 'boundary-valu', 'problem', 'blasiu', 'equat', 'integr', 'numer', 'us', 'suitabl', 'transform', 'without', 'iter', 'boundari', 'condit']
[retrieveDocuments][L3] Retrieval complete. 1171 documents found.
[main][L2] Query 49: Retrieved 1171 documents.
[main][L2] Processing query 50: what are the effects of small amounts of gas rarefaction on the characteristics of the boundary layers on slender bodies of revolution .
[retrieveDocuments][L2] Retrieving documents for query: what are the effects of small amounts of gas rarefaction on the characteristics of the boundary layers on slender bodies of revolution .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['effect', 'small', 'amount', 'ga', 'rarefact', 'characterist', 'boundari', 'layer', 'slender', 'bodi', 'revolut']
[retrieveDocuments][L3] Retrieval complete. 1032 documents found.
[main][L2] Query 50: Retrieved 1032 documents.
[main][L2] Processing query 51: what is the available information pertaining to boundary layers on very slender bodies of revolution in continuum flow (the ?transverse curvature  effect) .
[retrieveDocuments][L2] Retrieving documents for query: what is the available information pertaining to boundary layers on very slender bodies of revolution in continuum flow (the ?transverse curvature  effect) .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['avail', 'inform', 'pertain', 'boundari', 'layer', 'slender', 'bodi', 'revolut', 'continuum', 'flow', 'transvers', 'curvatur', 'effect']
[retrieveDocuments][L3] Retrieval complete. 1130 documents found.
[main][L2] Query 51: Retrieved 1130 documents.
[main][L2] Processing query 52: what is the available information pertaining to the effect of slight rarefaction on boundary layer flows (the ?slip? effect) .
[retrieveDocuments][L2] Retrieving documents for query: what is the available information pertaining to the effect of slight rarefaction on boundary layer flows (the ?slip? effect) .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['avail', 'inform', 'pertain', 'effect', 'slight', 'rarefact', 'boundari', 'layer', 'flow', 'slip', 'effect']
[retrieveDocuments][L3] Retrieval complete. 1073 documents found.
[main][L2] Query 52: Retrieved 1073 documents.
[main][L2] Processing query 53: what investigations have been made of the flow field about a body moving through a rarefied,  partially ionized gas in the presence of a magnetic field .
[retrieveDocuments][L2] Retrieving documents for query: what investigations have been made of the flow field about a body moving through a rarefied,  partially ionized gas in the presence of a magnetic field .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['investig', 'made', 'flow', 'field', 'bodi', 'move', 'rarefi', 'partial', 'ioniz', 'ga', 'presenc', 'magnet', 'field']
[retrieveDocuments][L3] Retrieval complete. 1091 documents found.
[main][L2] Query 53: Retrieved 1091 documents.
[main][L2] Processing query 54: how is the heat transfer downstream of the mass transfer region effected by mass transfer at the nose of a blunted cone .
[retrieveDocuments][L2] Retrieving documents for query: how is the heat transfer downstream of the mass transfer region effected by mass transfer at the nose of a blunted cone .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['heat', 'transfer', 'downstream', 'mass', 'transfer', 'region', 'effect', 'mass', 'transfer', 'nose', 'blunt', 'cone']
[retrieveDocuments][L3] Retrieval complete. 881 documents found.
[main][L2] Query 54: Retrieved 881 documents.
[main][L2] Processing query 55: to what extent can the available information for incompressible boundary layers be applied to problems involving compressible boundary layers .
[retrieveDocuments][L2] Retrieving documents for query: to what extent can the available information for incompressible boundary layers be applied to problems involving compressible boundary layers .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['extent', 'can', 'avail', 'inform', 'incompress', 'boundari', 'layer', 'appli', 'problem', 'involv', 'compress', 'boundari', 'layer']
[retrieveDocuments][L3] Retrieval complete. 993 documents found.
[main][L2] Query 55: Retrieved 993 documents.
[main][L2] Processing query 56: to what extent can readily available steady-state aerodynamic data be utilized to predict lifting-surface flutter characteristics .
[retrieveDocuments][L2] Retrieving documents for query: to what extent can readily available steady-state aerodynamic data be utilized to predict lifting-surface flutter characteristics .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['extent', 'can', 'readili', 'avail', 'steady-st', 'aerodynam', 'data', 'util', 'predict', 'lifting-surfac', 'flutter', 'characterist']
[retrieveDocuments][L3] Retrieval complete. 774 documents found.
[main][L2] Query 56: Retrieved 774 documents.
[main][L2] Processing query 57: what are the significant steady and non-steady flow characteristics which affect the flutter mechanism .
[retrieveDocuments][L2] Retrieving documents for query: what are the significant steady and non-steady flow characteristics which affect the flutter mechanism .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['signific', 'steadi', 'non-steadi', 'flow', 'characterist', 'affect', 'flutter', 'mechan']
[retrieveDocuments][L3] Retrieval complete. 878 documents found.
[main][L2] Query 57: Retrieved 878 documents.
[main][L2] Processing query 58: is it possible to determine rates of forced convective heat transfer from heated cylinders of non-circular cross-section,  (the fluid flow being along the generators) .
[retrieveDocuments][L2] Retrieving documents for query: is it possible to determine rates of forced convective heat transfer from heated cylinders of non-circular cross-section,  (the fluid flow being along the generators) .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['possibl', 'determin', 'rate', 'forc', 'convect', 'heat', 'transfer', 'heat', 'cylind', 'non-circular', 'cross-sect', 'fluid', 'flow', 'along', 'gener']
[retrieveDocuments][L3] Retrieval complete. 1189 documents found.
[main][L2] Query 58: Retrieved 1189 documents.
[main][L2] Processing query 59: how much is known about boundary layer flows along non-circular cylinders .
[retrieveDocuments][L2] Retrieving documents for query: how much is known about boundary layer flows along non-circular cylinders .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['much', 'known', 'boundari', 'layer', 'flow', 'along', 'non-circular', 'cylind']
[retrieveDocuments][L3] Retrieval complete. 1001 documents found.
[main][L2] Query 59: Retrieved 1001 documents.
[main][L2] Processing query 60: is there any simple,  but practical,  method for numerical integration of the mixing problem (i.e. the blasius problem with three-point boundary conditions) .
[retrieveDocuments][L2] Retrieving documents for query: is there any simple,  but practical,  method for numerical integration of the mixing problem (i.e. the blasius problem with three-point boundary conditions) .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['simpl', 'practic', 'method', 'numer', 'integr', 'mix', 'problem', 'i.e.', 'blasiu', 'problem', 'three-point', 'boundari', 'condit']
[retrieveDocuments][L3] Retrieval complete. 1061 documents found.
[main][L2] Query 60: Retrieved 1061 documents.
[main][L2] Processing query 61: does there exist a closed-form expression for the local heat transfer around a yawed cylinder .
[retrieveDocuments][L2] Retrieving documents for query: does there exist a closed-form expression for the local heat transfer around a yawed cylinder .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['exist', 'closed-form', 'express', 'local', 'heat', 'transfer', 'around', 'yaw', 'cylind']
[retrieveDocuments][L3] Retrieval complete. 669 documents found.
[main][L2] Query 61: Retrieved 669 documents.
[main][L2] Processing query 62: how far around a cylinder and under what conditions of flow,  if any, is the velocity just outside of the boundary layer a linear function of the distance around the cylinder .
[retrieveDocuments][L2] Retrieving documents for query: how far around a cylinder and under what conditions of flow,  if any, is the velocity just outside of the boundary layer a linear function of the distance around the cylinder .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['far', 'around', 'cylind', 'condit', 'flow', 'veloc', 'just', 'outsid', 'boundari', 'layer', 'linear', 'function', 'distanc', 'around', 'cylind']
[retrieveDocuments][L3] Retrieval complete. 1131 documents found.
[main][L2] Query 62: Retrieved 1131 documents.
[main][L2] Processing query 63: where can i find pressure data on surfaces of swept cylinders .
[retrieveDocuments][L2] Retrieving documents for query: where can i find pressure data on surfaces of swept cylinders .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['can', 'find', 'pressur', 'data', 'surfac', 'swept', 'cylind']
[retrieveDocuments][L3] Retrieval complete. 991 documents found.
[main][L2] Query 63: Retrieved 991 documents.
[main][L2] Processing query 64: can't the static deflection shapes be used in predicting flutter in place of vibrational shapes . if so,  can we provide a justification by means of an example .
[retrieveDocuments][L2] Retrieving documents for query: can't the static deflection shapes be used in predicting flutter in place of vibrational shapes . if so,  can we provide a justification by means of an example .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['static', 'deflect', 'shape', 'us', 'predict', 'flutter', 'place', 'vibrat', 'shape', 'can', 'provid', 'justif', 'mean', 'exampl']
[retrieveDocuments][L3] Retrieval complete. 1023 documents found.
[main][L2] Query 64: Retrieved 1023 documents.
[main][L2] Processing query 65: does the boundary layer on a flat plate in a shear flow induce a pressure gradient .
[retrieveDocuments][L2] Retrieving documents for query: does the boundary layer on a flat plate in a shear flow induce a pressure gradient .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['boundari', 'layer', 'flat', 'plate', 'shear', 'flow', 'induc', 'pressur', 'gradient']
[retrieveDocuments][L3] Retrieval complete. 1089 documents found.
[main][L2] Query 65: Retrieved 1089 documents.
[main][L2] Processing query 66: can the procedure of matching inner and outer solutions for a viscous flow problem be applied when the main stream is a shear flow .
[retrieveDocuments][L2] Retrieving documents for query: can the procedure of matching inner and outer solutions for a viscous flow problem be applied when the main stream is a shear flow .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['can', 'procedur', 'match', 'inner', 'outer', 'solut', 'viscou', 'flow', 'problem', 'appli', 'main', 'stream', 'shear', 'flow']
[retrieveDocuments][L3] Retrieval complete. 1130 documents found.
[main][L2] Query 66: Retrieved 1130 documents.
[main][L2] Processing query 67: can series expansions be found for the boundary layer on a flat plate in a shear flow .
[retrieveDocuments][L2] Retrieving documents for query: can series expansions be found for the boundary layer on a flat plate in a shear flow .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['can', 'seri', 'expans', 'found', 'boundari', 'layer', 'flat', 'plate', 'shear', 'flow']
[retrieveDocuments][L3] Retrieval complete. 1110 documents found.
[main][L2] Query 67: Retrieved 1110 documents.
[main][L2] Processing query 68: what possible techniques are available for computing the injection distribution corresponding to an isothermal transpiration cooled hemisphere .
[retrieveDocuments][L2] Retrieving documents for query: what possible techniques are available for computing the injection distribution corresponding to an isothermal transpiration cooled hemisphere .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['possibl', 'techniqu', 'avail', 'comput', 'inject', 'distribut', 'correspond', 'isotherm', 'transpir', 'cool', 'hemispher']
[retrieveDocuments][L3] Retrieval complete. 743 documents found.
[main][L2] Query 68: Retrieved 743 documents.
[main][L2] Processing query 69: what is known regarding asymptotic solutions to the exact boundary layer equations .
[retrieveDocuments][L2] Retrieving documents for query: what is known regarding asymptotic solutions to the exact boundary layer equations .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['known', 'regard', 'asymptot', 'solut', 'exact', 'boundari', 'layer', 'equat']
[retrieveDocuments][L3] Retrieval complete. 834 documents found.
[main][L2] Query 69: Retrieved 834 documents.
[main][L2] Processing query 70: previous solutions to the boundary layer similarity equations .
[retrieveDocuments][L2] Retrieving documents for query: previous solutions to the boundary layer similarity equations .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['previou', 'solut', 'boundari', 'layer', 'similar', 'equat']
[retrieveDocuments][L3] Retrieval complete. 847 documents found.
[main][L2] Query 70: Retrieved 847 documents.
[main][L2] Processing query 71: experimental results on hypersonic viscous interaction .
[retrieveDocuments][L2] Retrieving documents for query: experimental results on hypersonic viscous interaction .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['experiment', 'result', 'hyperson', 'viscou', 'interact']
[retrieveDocuments][L3] Retrieval complete. 900 documents found.
[main][L2] Query 71: Retrieved 900 documents.
[main][L2] Processing query 72: what has been done about viscous interactions in relatively low reynolds number flows,  particularly at high mach numbers .
[retrieveDocuments][L2] Retrieving documents for query: what has been done about viscous interactions in relatively low reynolds number flows,  particularly at high mach numbers .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['done', 'viscou', 'interact', 'rel', 'low', 'reynold', 'number', 'flow', 'particularli', 'high', 'mach', 'number']
[retrieveDocuments][L3] Retrieval complete. 1033 documents found.
[main][L2] Query 72: Retrieved 1033 documents.
[main][L2] Processing query 73: what role does the effect of chemical reaction (particularly when out of equilibrium) play in the similitude laws governing hypersonic flows over slender aerodynamic bodies .
[retrieveDocuments][L2] Retrieving documents for query: what role does the effect of chemical reaction (particularly when out of equilibrium) play in the similitude laws governing hypersonic flows over slender aerodynamic bodies .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['role', 'effect', 'chemic', 'reaction', 'particularli', 'equilibrium', 'plai', 'similitud', 'law', 'govern', 'hyperson', 'flow', 'slender', 'aerodynam', 'bodi']
[retrieveDocuments][L3] Retrieval complete. 1108 documents found.
[main][L2] Query 73: Retrieved 1108 documents.
[main][L2] Processing query 74: how significant is the possible pressure of a dissociated free stream with respect to the realization of hypersonic simulation in high enthalpy wind tunnels .
[retrieveDocuments][L2] Retrieving documents for query: how significant is the possible pressure of a dissociated free stream with respect to the realization of hypersonic simulation in high enthalpy wind tunnels .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['signific', 'possibl', 'pressur', 'dissoci', 'free', 'stream', 'respect', 'realiz', 'hyperson', 'simul', 'high', 'enthalpi', 'wind', 'tunnel']
[retrieveDocuments][L3] Retrieval complete. 938 documents found.
[main][L2] Query 74: Retrieved 938 documents.
[main][L2] Processing query 75: do the discrepancies among current analyses of the vorticity effect on stagnation-point heat transfer result primarily from the differences in the viscosity-temperature law assumed .
[retrieveDocuments][L2] Retrieving documents for query: do the discrepancies among current analyses of the vorticity effect on stagnation-point heat transfer result primarily from the differences in the viscosity-temperature law assumed .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['discrep', 'among', 'current', 'analys', 'vortic', 'effect', 'stagnation-point', 'heat', 'transfer', 'result', 'primarili', 'differ', 'viscosity-temperatur', 'law', 'assum']
[retrieveDocuments][L3] Retrieval complete. 1108 documents found.
[main][L2] Query 75: Retrieved 1108 documents.
[main][L2] Processing query 76: how far can one trust the linear viscosity-temperature solution assumed in some of the analyses of hypersonic shock layer at low reynolds number .
[retrieveDocuments][L2] Retrieving documents for query: how far can one trust the linear viscosity-temperature solution assumed in some of the analyses of hypersonic shock layer at low reynolds number .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['far', 'can', 'on', 'trust', 'linear', 'viscosity-temperatur', 'solut', 'assum', 'analys', 'hyperson', 'shock', 'layer', 'low', 'reynold', 'number']
[retrieveDocuments][L3] Retrieval complete. 1173 documents found.
[main][L2] Query 76: Retrieved 1173 documents.
[main][L2] Processing query 77: how close is the comparison of the shock layer theory with existing experiments in the low reynolds number (merged-layer) regime .
[retrieveDocuments][L2] Retrieving documents for query: how close is the comparison of the shock layer theory with existing experiments in the low reynolds number (merged-layer) regime .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['close', 'comparison', 'shock', 'layer', 'theori', 'exist', 'experi', 'low', 'reynold', 'number', 'merged-lay', 'regim']
[retrieveDocuments][L3] Retrieval complete. 1085 documents found.
[main][L2] Query 77: Retrieved 1085 documents.
[main][L2] Processing query 78: has anyone explained the kink in the surge line of a multi-stage axial compressor .
[retrieveDocuments][L2] Retrieving documents for query: has anyone explained the kink in the surge line of a multi-stage axial compressor .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['anyon', 'explain', 'kink', 'surg', 'line', 'multi-stag', 'axial', 'compressor']
[retrieveDocuments][L3] Retrieval complete. 228 documents found.
[main][L2] Query 78: Retrieved 228 documents.
[main][L2] Processing query 79: have any aerodynamic derivatives been measured at hypersonic mach numbers and comparison been made with theoretical work .
[retrieveDocuments][L2] Retrieving documents for query: have any aerodynamic derivatives been measured at hypersonic mach numbers and comparison been made with theoretical work .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['aerodynam', 'deriv', 'measur', 'hyperson', 'mach', 'number', 'comparison', 'made', 'theoret', 'work']
[retrieveDocuments][L3] Retrieval complete. 1084 documents found.
[main][L2] Query 79: Retrieved 1084 documents.
[main][L2] Processing query 80: are methods of measuring aerodynamic derivatives available which could be adopted for use in short running time facilities .
[retrieveDocuments][L2] Retrieving documents for query: are methods of measuring aerodynamic derivatives available which could be adopted for use in short running time facilities .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['method', 'measur', 'aerodynam', 'deriv', 'avail', 'adopt', 'us', 'short', 'run', 'time', 'facil']
[retrieveDocuments][L3] Retrieval complete. 1041 documents found.
[main][L2] Query 80: Retrieved 1041 documents.
[main][L2] Processing query 81: what are wind-tunnel corrections for a two-dimensional aerofoil mounted off-centre in a tunnel .
[retrieveDocuments][L2] Retrieving documents for query: what are wind-tunnel corrections for a two-dimensional aerofoil mounted off-centre in a tunnel .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['wind-tunnel', 'correct', 'two-dimension', 'aerofoil', 'mount', 'off-centr', 'tunnel']
[retrieveDocuments][L3] Retrieval complete. 375 documents found.
[main][L2] Query 81: Retrieved 375 documents.
[main][L2] Processing query 82: how do kuchemann's and multhopp's methods for calculating lift distributions on swept wings in subsonic flow compare with each other and with experiment .
[retrieveDocuments][L2] Retrieving documents for query: how do kuchemann's and multhopp's methods for calculating lift distributions on swept wings in subsonic flow compare with each other and with experiment .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['kuchemann', 's', 'multhopp', 's', 'method', 'calcul', 'lift', 'distribut', 'swept', 'wing', 'subson', 'flow', 'compar', 'experi']
[retrieveDocuments][L3] Retrieval complete. 1210 documents found.
[main][L2] Query 82: Retrieved 1210 documents.
[main][L2] Processing query 83: what is the present state of the theory of quasi-conical flows .
[retrieveDocuments][L2] Retrieving documents for query: what is the present state of the theory of quasi-conical flows .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['present', 'state', 'theori', 'quasi-con', 'flow']
[retrieveDocuments][L3] Retrieval complete. 1104 documents found.
[main][L2] Query 83: Retrieved 1104 documents.
[main][L2] Processing query 84: references on the methods available for accurately estimating aerodynamic heat transfer to conical bodies for both laminar and turbulent flow .
[retrieveDocuments][L2] Retrieving documents for query: references on the methods available for accurately estimating aerodynamic heat transfer to conical bodies for both laminar and turbulent flow .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['refer', 'method', 'avail', 'accur', 'estim', 'aerodynam', 'heat', 'transfer', 'conic', 'bodi', 'laminar', 'turbul', 'flow']
[retrieveDocuments][L3] Retrieval complete. 1155 documents found.
[main][L2] Query 84: Retrieved 1155 documents.
[main][L2] Processing query 85: what parameters can seriously influence natural transition from laminar to turbulent flow on a model in a wind tunnel .
[retrieveDocuments][L2] Retrieving documents for query: what parameters can seriously influence natural transition from laminar to turbulent flow on a model in a wind tunnel .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['paramet', 'can', 'serious', 'influenc', 'natur', 'transit', 'laminar', 'turbul', 'flow', 'model', 'wind', 'tunnel']
[retrieveDocuments][L3] Retrieval complete. 1070 documents found.
[main][L2] Query 85: Retrieved 1070 documents.
[main][L2] Processing query 86: can a satisfactory experimental technique be developed for measuring oscillatory derivatives on slender sting-mounted models in supersonic wind tunnels .
[retrieveDocuments][L2] Retrieving documents for query: can a satisfactory experimental technique be developed for measuring oscillatory derivatives on slender sting-mounted models in supersonic wind tunnels .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['can', 'satisfactori', 'experiment', 'techniqu', 'develop', 'measur', 'oscillatori', 'deriv', 'slender', 'sting-mount', 'model', 'superson', 'wind', 'tunnel']
[retrieveDocuments][L3] Retrieval complete. 1073 documents found.
[main][L2] Query 86: Retrieved 1073 documents.
[main][L2] Processing query 87: what effect has the boundary layer in modifying the basic inviscid flow behind the shock,  neglecting effects of leading edge and corner .
[retrieveDocuments][L2] Retrieving documents for query: what effect has the boundary layer in modifying the basic inviscid flow behind the shock,  neglecting effects of leading edge and corner .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['effect', 'boundari', 'layer', 'modifi', 'basic', 'inviscid', 'flow', 'behind', 'shock', 'neglect', 'effect', 'lead', 'edg', 'corner']
[retrieveDocuments][L3] Retrieval complete. 1134 documents found.
[main][L2] Query 87: Retrieved 1134 documents.
[main][L2] Processing query 88: how does a satellite orbit contract under the action of air drag in an atmosphere in which the scale height varies with altitude .
[retrieveDocuments][L2] Retrieving documents for query: how does a satellite orbit contract under the action of air drag in an atmosphere in which the scale height varies with altitude .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['satellit', 'orbit', 'contract', 'action', 'air', 'drag', 'atmospher', 'scale', 'height', 'vari', 'altitud']
[retrieveDocuments][L3] Retrieval complete. 493 documents found.
[main][L2] Query 88: Retrieved 493 documents.
[main][L2] Processing query 89: how is the flow at transonic speeds about a delta wing different from that on a closely-related tapered sweptback wing .
[retrieveDocuments][L2] Retrieving documents for query: how is the flow at transonic speeds about a delta wing different from that on a closely-related tapered sweptback wing .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['flow', 'transon', 'speed', 'delta', 'wing', 'differ', 'closely-rel', 'taper', 'sweptback', 'wing']
[retrieveDocuments][L3] Retrieval complete. 955 documents found.
[main][L2] Query 89: Retrieved 955 documents.
[main][L2] Processing query 90: recent data on shock-induced boundary-layer separation .
[retrieveDocuments][L2] Retrieving documents for query: recent data on shock-induced boundary-layer separation .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['recent', 'data', 'shock-induc', 'boundary-lay', 'separ']
[retrieveDocuments][L3] Retrieval complete. 489 documents found.
[main][L2] Query 90: Retrieved 489 documents.
[main][L2] Processing query 91: what interference effects are likely at transonic speeds .
[retrieveDocuments][L2] Retrieving documents for query: what interference effects are likely at transonic speeds .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['interfer', 'effect', 'like', 'transon', 'speed']
[retrieveDocuments][L3] Retrieval complete. 680 documents found.
[main][L2] Query 91: Retrieved 680 documents.
[main][L2] Processing query 92: given complete freedom in the design of an airplane,  what procedure would be used in order to minimize sonic boom intensity,  and is there a limit to the degree of minimizing that can be accomplished .
[retrieveDocuments][L2] Retrieving documents for query: given complete freedom in the design of an airplane,  what procedure would be used in order to minimize sonic boom intensity,  and is there a limit to the degree of minimizing that can be accomplished .
[retrieveDocuments][L3] Query tokenized into 17 tokens: ['given', 'complet', 'freedom', 'design', 'airplan', 'procedur', 'us', 'order', 'minim', 'sonic', 'boom', 'intens', 'limit', 'degre', 'minim', 'can', 'accomplish']
[retrieveDocuments][L3] Retrieval complete. 1045 documents found.
[main][L2] Query 92: Retrieved 1045 documents.
[main][L2] Processing query 93: can methane-air combustion product be used as a hypersonic test medium and predict, within experimental accuracies, the results obtained in air .
[retrieveDocuments][L2] Retrieving documents for query: can methane-air combustion product be used as a hypersonic test medium and predict, within experimental accuracies, the results obtained in air .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['can', 'methane-air', 'combust', 'product', 'us', 'hyperson', 'test', 'medium', 'predict', 'within', 'experiment', 'accuraci', 'result', 'obtain', 'air']
[retrieveDocuments][L3] Retrieval complete. 1238 documents found.
[main][L2] Query 93: Retrieved 1238 documents.
[main][L2] Processing query 94: what is the theoretical heat transfer rate at the stagnation point of a blunt body .
[retrieveDocuments][L2] Retrieving documents for query: what is the theoretical heat transfer rate at the stagnation point of a blunt body .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['theoret', 'heat', 'transfer', 'rate', 'stagnat', 'point', 'blunt', 'bodi']
[retrieveDocuments][L3] Retrieval complete. 808 documents found.
[main][L2] Query 94: Retrieved 808 documents.
[main][L2] Processing query 95: what is the theoretical heat transfer distribution around a hemisphere .
[retrieveDocuments][L2] Retrieving documents for query: what is the theoretical heat transfer distribution around a hemisphere .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['theoret', 'heat', 'transfer', 'distribut', 'around', 'hemispher']
[retrieveDocuments][L3] Retrieval complete. 738 documents found.
[main][L2] Query 95: Retrieved 738 documents.
[main][L2] Processing query 96: has anyone investigated the unsteady lift distributions on finite wings in subsonic flow .
[retrieveDocuments][L2] Retrieving documents for query: has anyone investigated the unsteady lift distributions on finite wings in subsonic flow .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['anyon', 'investig', 'unsteadi', 'lift', 'distribut', 'finit', 'wing', 'subson', 'flow']
[retrieveDocuments][L3] Retrieval complete. 1054 documents found.
[main][L2] Query 96: Retrieved 1054 documents.
[main][L2] Processing query 97: what information is available for dynamic response of airplanes to gusts  or blasts in the subsonic regime .
[retrieveDocuments][L2] Retrieving documents for query: what information is available for dynamic response of airplanes to gusts  or blasts in the subsonic regime .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['inform', 'avail', 'dynam', 'respons', 'airplan', 'gust', 'blast', 'subson', 'regim']
[retrieveDocuments][L3] Retrieval complete. 349 documents found.
[main][L2] Query 97: Retrieved 349 documents.
[main][L2] Processing query 98: will forward or apex located controls be effective at low subsonic speeds and how do they compare with conventional trailing-edge flaps .
[retrieveDocuments][L2] Retrieving documents for query: will forward or apex located controls be effective at low subsonic speeds and how do they compare with conventional trailing-edge flaps .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['will', 'forward', 'apex', 'locat', 'control', 'effect', 'low', 'subson', 'speed', 'compar', 'convent', 'trailing-edg', 'flap']
[retrieveDocuments][L3] Retrieval complete. 943 documents found.
[main][L2] Query 98: Retrieved 943 documents.
[main][L2] Processing query 99: given that an uncontrolled vehicle will tumble as it enters an atmosphere, is it possible to predict when and how it will stop tumbling and its subsequent motion .
[retrieveDocuments][L2] Retrieving documents for query: given that an uncontrolled vehicle will tumble as it enters an atmosphere, is it possible to predict when and how it will stop tumbling and its subsequent motion .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['given', 'uncontrol', 'vehicl', 'will', 'tumbl', 'enter', 'atmospher', 'possibl', 'predict', 'will', 'stop', 'tumbl', 'subsequ', 'motion']
[retrieveDocuments][L3] Retrieval complete. 807 documents found.
[main][L2] Query 99: Retrieved 807 documents.
[main][L2] Processing query 100: what are the effects of initial imperfections on the elastic buckling of cylindrical shells under axial compression .
[retrieveDocuments][L2] Retrieving documents for query: what are the effects of initial imperfections on the elastic buckling of cylindrical shells under axial compression .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['effect', 'initi', 'imperfect', 'elast', 'buckl', 'cylindr', 'shell', 'axial', 'compress']
[retrieveDocuments][L3] Retrieval complete. 840 documents found.
[main][L2] Query 100: Retrieved 840 documents.
[main][L2] Processing query 101: why does the incremental theory and the deformation theory of plastic stress-strain relationship differ greatly when applied to stability problems .
[retrieveDocuments][L2] Retrieving documents for query: why does the incremental theory and the deformation theory of plastic stress-strain relationship differ greatly when applied to stability problems .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['increment', 'theori', 'deform', 'theori', 'plastic', 'stress-strain', 'relationship', 'differ', 'greatli', 'appli', 'stabil', 'problem']
[retrieveDocuments][L3] Retrieval complete. 919 documents found.
[main][L2] Query 101: Retrieved 919 documents.
[main][L2] Processing query 102: basic dynamic characteristics of structures continuous over many spans .
[retrieveDocuments][L2] Retrieving documents for query: basic dynamic characteristics of structures continuous over many spans .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['basic', 'dynam', 'characterist', 'structur', 'continu', 'mani', 'span']
[retrieveDocuments][L3] Retrieval complete. 441 documents found.
[main][L2] Query 102: Retrieved 441 documents.
[main][L2] Processing query 103: is the information on the buckling of sandwich sphere available .
[retrieveDocuments][L2] Retrieving documents for query: is the information on the buckling of sandwich sphere available .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['inform', 'buckl', 'sandwich', 'sphere', 'avail']
[retrieveDocuments][L3] Retrieval complete. 266 documents found.
[main][L2] Query 103: Retrieved 266 documents.
[main][L2] Processing query 104: can the load deformation characteristics of a beam be obtained with the material being inelastic and a non uniform temperature being present .
[retrieveDocuments][L2] Retrieving documents for query: can the load deformation characteristics of a beam be obtained with the material being inelastic and a non uniform temperature being present .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['can', 'load', 'deform', 'characterist', 'beam', 'obtain', 'materi', 'inelast', 'non', 'uniform', 'temperatur', 'present']
[retrieveDocuments][L3] Retrieval complete. 1117 documents found.
[main][L2] Query 104: Retrieved 1117 documents.
[main][L2] Processing query 105: what is the effect of an internal liquid column on the breathing vibrations of a cylindrical shell .
[retrieveDocuments][L2] Retrieving documents for query: what is the effect of an internal liquid column on the breathing vibrations of a cylindrical shell .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['effect', 'intern', 'liquid', 'column', 'breath', 'vibrat', 'cylindr', 'will']
[retrieveDocuments][L3] Retrieval complete. 729 documents found.
[main][L2] Query 105: Retrieved 729 documents.
[main][L2] Processing query 106: experimental techniques in shell vibration .
[retrieveDocuments][L2] Retrieving documents for query: experimental techniques in shell vibration .
[retrieveDocuments][L3] Query tokenized into 4 tokens: ['experiment', 'techniqu', 'will', 'vibrat']
[retrieveDocuments][L3] Retrieval complete. 546 documents found.
[main][L2] Query 106: Retrieved 546 documents.
[main][L2] Processing query 107: in summarizing theoretical and experimental work on the behaviour of a typical aircraft structure in a noise environment is it possible to develop a design procedure .
[retrieveDocuments][L2] Retrieving documents for query: in summarizing theoretical and experimental work on the behaviour of a typical aircraft structure in a noise environment is it possible to develop a design procedure .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['summar', 'theoret', 'experiment', 'work', 'behaviour', 'typic', 'aircraft', 'structur', 'nois', 'environ', 'possibl', 'develop', 'design', 'procedur']
[retrieveDocuments][L3] Retrieval complete. 904 documents found.
[main][L2] Query 107: Retrieved 904 documents.
[main][L2] Processing query 108: what data is there on the fatigue of structures under acoustic loading .
[retrieveDocuments][L2] Retrieving documents for query: what data is there on the fatigue of structures under acoustic loading .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['data', 'fatigu', 'structur', 'acoust', 'load']
[retrieveDocuments][L3] Retrieval complete. 447 documents found.
[main][L2] Query 108: Retrieved 447 documents.
[main][L2] Processing query 109: panels subjected to aerodynamic heating .
[retrieveDocuments][L2] Retrieving documents for query: panels subjected to aerodynamic heating .
[retrieveDocuments][L3] Query tokenized into 4 tokens: ['panel', 'subject', 'aerodynam', 'heat']
[retrieveDocuments][L3] Retrieval complete. 510 documents found.
[main][L2] Query 109: Retrieved 510 documents.
[main][L2] Processing query 110: can increasing the edge loading of a plate beyond the critical value for buckling change the buckling mode .
[retrieveDocuments][L2] Retrieving documents for query: can increasing the edge loading of a plate beyond the critical value for buckling change the buckling mode .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['can', 'increas', 'edg', 'load', 'plate', 'beyond', 'critic', 'valu', 'buckl', 'chang', 'buckl', 'mode']
[retrieveDocuments][L3] Retrieval complete. 951 documents found.
[main][L2] Query 110: Retrieved 951 documents.
[main][L2] Processing query 111: have the effects of an elastic edge restraint been considered in previous papers on panel flutter .
[retrieveDocuments][L2] Retrieving documents for query: have the effects of an elastic edge restraint been considered in previous papers on panel flutter .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['effect', 'elast', 'edg', 'restraint', 'consid', 'previou', 'paper', 'panel', 'flutter']
[retrieveDocuments][L3] Retrieval complete. 896 documents found.
[main][L2] Query 111: Retrieved 896 documents.
[main][L2] Processing query 112: has the solution of the clamped plate problem,  in the classical theory of bending,  been reduced to two successive membrane boundary value problems .
[retrieveDocuments][L2] Retrieving documents for query: has the solution of the clamped plate problem,  in the classical theory of bending,  been reduced to two successive membrane boundary value problems .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['solut', 'clamp', 'plate', 'problem', 'classic', 'theori', 'bend', 'reduc', 'two', 'success', 'membran', 'boundari', 'valu', 'problem']
[retrieveDocuments][L3] Retrieval complete. 1168 documents found.
[main][L2] Query 112: Retrieved 1168 documents.
[main][L2] Processing query 113: what data exists on oscillatory aerodynamic forces on control surfaces at transonic mach numbers .
[retrieveDocuments][L2] Retrieving documents for query: what data exists on oscillatory aerodynamic forces on control surfaces at transonic mach numbers .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['data', 'exist', 'oscillatori', 'aerodynam', 'forc', 'control', 'surfac', 'transon', 'mach', 'number']
[retrieveDocuments][L3] Retrieval complete. 977 documents found.
[main][L2] Query 113: Retrieved 977 documents.
[main][L2] Processing query 114: it is not likely that the airforces on a wing of general planform oscillating in transonic flow can be determined by purely analytical methods . is it possible to determine the airforces on a single particular planform, such as the rectangular one by such method .
[retrieveDocuments][L2] Retrieving documents for query: it is not likely that the airforces on a wing of general planform oscillating in transonic flow can be determined by purely analytical methods . is it possible to determine the airforces on a single particular planform, such as the rectangular one by such method .
[retrieveDocuments][L3] Query tokenized into 22 tokens: ['like', 'airforc', 'wing', 'gener', 'planform', 'oscil', 'transon', 'flow', 'can', 'determin', 'pure', 'analyt', 'method', 'possibl', 'determin', 'airforc', 'singl', 'particular', 'planform', 'rectangular', 'on', 'method']
[retrieveDocuments][L3] Retrieval complete. 1245 documents found.
[main][L2] Query 114: Retrieved 1245 documents.
[main][L2] Processing query 115: is the problem of similarity for representative investigations of aeroelastic effects in heated flow as intractable as previous investigations imply .
[retrieveDocuments][L2] Retrieving documents for query: is the problem of similarity for representative investigations of aeroelastic effects in heated flow as intractable as previous investigations imply .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['problem', 'similar', 'repres', 'investig', 'aeroelast', 'effect', 'heat', 'flow', 'intract', 'previou', 'investig', 'impli']
[retrieveDocuments][L3] Retrieval complete. 1211 documents found.
[main][L2] Query 115: Retrieved 1211 documents.
[main][L2] Processing query 116: what is the magnitude and distribution of lift over the cone and the cylindrical portion of a cone-cylinder configuration .
[retrieveDocuments][L2] Retrieving documents for query: what is the magnitude and distribution of lift over the cone and the cylindrical portion of a cone-cylinder configuration .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['magnitud', 'distribut', 'lift', 'cone', 'cylindr', 'portion', 'cone-cylind', 'configur']
[retrieveDocuments][L3] Retrieval complete. 652 documents found.
[main][L2] Query 116: Retrieved 652 documents.
[main][L2] Processing query 117: is there any information on how the addition of a /boat-tail/ affects the normal force on the body of various angles of incidence .
[retrieveDocuments][L2] Retrieving documents for query: is there any information on how the addition of a /boat-tail/ affects the normal force on the body of various angles of incidence .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['inform', 'addit', 'boat-tail', 'affect', 'normal', 'forc', 'bodi', 'variou', 'angl', 'incid']
[retrieveDocuments][L3] Retrieval complete. 736 documents found.
[main][L2] Query 117: Retrieved 736 documents.
[main][L2] Processing query 118: what are the aerodynamic interference effects on the fin lift and body lift of a fin-body combination .
[retrieveDocuments][L2] Retrieving documents for query: what are the aerodynamic interference effects on the fin lift and body lift of a fin-body combination .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['aerodynam', 'interfer', 'effect', 'fin', 'lift', 'bodi', 'lift', 'fin-bodi', 'combin']
[retrieveDocuments][L3] Retrieval complete. 835 documents found.
[main][L2] Query 118: Retrieved 835 documents.
[main][L2] Processing query 119: what is the effect of initial axisymmetric deviations from circularity on the non linear (large-deflection) load-deflection response of cylinders under hydrostatic pressure .
[retrieveDocuments][L2] Retrieving documents for query: what is the effect of initial axisymmetric deviations from circularity on the non linear (large-deflection) load-deflection response of cylinders under hydrostatic pressure .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['effect', 'initi', 'axisymmetr', 'deviat', 'circular', 'non', 'linear', 'large-deflect', 'load-deflect', 'respons', 'cylind', 'hydrostat', 'pressur']
[retrieveDocuments][L3] Retrieval complete. 1027 documents found.
[main][L2] Query 119: Retrieved 1027 documents.
[main][L2] Processing query 120: are previous analyses of circumferential thermal buckling of circular cylindrical shells unnecessarily involved or even inaccurate due to the assumed forms of buckling mode .
[retrieveDocuments][L2] Retrieving documents for query: are previous analyses of circumferential thermal buckling of circular cylindrical shells unnecessarily involved or even inaccurate due to the assumed forms of buckling mode .
[retrieveDocuments][L3] Query tokenized into 17 tokens: ['previou', 'analys', 'circumferenti', 'thermal', 'buckl', 'circular', 'cylindr', 'shell', 'unnecessarili', 'involv', 'even', 'inaccur', 'due', 'assum', 'form', 'buckl', 'mode']
[retrieveDocuments][L3] Retrieval complete. 785 documents found.
[main][L2] Query 120: Retrieved 785 documents.
[main][L2] Processing query 121: what papers are there dealing with circumferential buckling either thermal buckling or due to mechanical loading .
[retrieveDocuments][L2] Retrieving documents for query: what papers are there dealing with circumferential buckling either thermal buckling or due to mechanical loading .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['paper', 'deal', 'circumferenti', 'buckl', 'either', 'thermal', 'buckl', 'due', 'mechan', 'load']
[retrieveDocuments][L3] Retrieval complete. 640 documents found.
[main][L2] Query 121: Retrieved 640 documents.
[main][L2] Processing query 122: what analytical investigations have been made of the stability of conical shells . how do the results compare with experiment .
[retrieveDocuments][L2] Retrieving documents for query: what analytical investigations have been made of the stability of conical shells . how do the results compare with experiment .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['analyt', 'investig', 'made', 'stabil', 'conic', 'shell', 'result', 'compar', 'experi']
[retrieveDocuments][L3] Retrieval complete. 1085 documents found.
[main][L2] Query 122: Retrieved 1085 documents.
[main][L2] Processing query 123: has any work been done on determining the nature of compressible viscous flow in a straight channel .
[retrieveDocuments][L2] Retrieving documents for query: has any work been done on determining the nature of compressible viscous flow in a straight channel .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['work', 'done', 'determin', 'natur', 'compress', 'viscou', 'flow', 'straight', 'channel']
[retrieveDocuments][L3] Retrieval complete. 983 documents found.
[main][L2] Query 123: Retrieved 983 documents.
[main][L2] Processing query 124: in what areas, other than low density wind tunnel flows, is viscous compressible flow in slender channels a problem . what analytical investigations have been made of the stability of conical shells . how do the results compare with experiment .
[retrieveDocuments][L2] Retrieving documents for query: in what areas, other than low density wind tunnel flows, is viscous compressible flow in slender channels a problem . what analytical investigations have been made of the stability of conical shells . how do the results compare with experiment .
[retrieveDocuments][L3] Query tokenized into 21 tokens: ['area', 'low', 'densiti', 'wind', 'tunnel', 'flow', 'viscou', 'compress', 'flow', 'slender', 'channel', 'problem', 'analyt', 'investig', 'made', 'stabil', 'conic', 'shell', 'result', 'compar', 'experi']
[retrieveDocuments][L3] Retrieval complete. 1321 documents found.
[main][L2] Query 124: Retrieved 1321 documents.
[main][L2] Processing query 125: jet interference with supersonic flow  -dash experimental papers .
[retrieveDocuments][L2] Retrieving documents for query: jet interference with supersonic flow  -dash experimental papers .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['jet', 'interfer', 'superson', 'flow', 'dash', 'experiment', 'paper']
[retrieveDocuments][L3] Retrieval complete. 1036 documents found.
[main][L2] Query 125: Retrieved 1036 documents.
[main][L2] Processing query 126: thrust vector control by fluid injection -dash papers .
[retrieveDocuments][L2] Retrieving documents for query: thrust vector control by fluid injection -dash papers .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['thrust', 'vector', 'control', 'fluid', 'inject', 'dash', 'paper']
[retrieveDocuments][L3] Retrieval complete. 462 documents found.
[main][L2] Query 126: Retrieved 462 documents.
[main][L2] Processing query 127: is it possible to obtain a reasonably simple analytical solution to the heat equation for an exponential (in time) heat input .
[retrieveDocuments][L2] Retrieving documents for query: is it possible to obtain a reasonably simple analytical solution to the heat equation for an exponential (in time) heat input .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['possibl', 'obtain', 'reason', 'simpl', 'analyt', 'solut', 'heat', 'equat', 'exponenti', 'time', 'heat', 'input']
[retrieveDocuments][L3] Retrieval complete. 1058 documents found.
[main][L2] Query 127: Retrieved 1058 documents.
[main][L2] Processing query 128: has anyone programmed a pump design method for a high-speed digital computer .
[retrieveDocuments][L2] Retrieving documents for query: has anyone programmed a pump design method for a high-speed digital computer .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['anyon', 'program', 'pump', 'design', 'method', 'high-spe', 'digit', 'comput']
[retrieveDocuments][L3] Retrieval complete. 611 documents found.
[main][L2] Query 128: Retrieved 611 documents.
[main][L2] Processing query 129: has anyone derived simplified pump design equation from the fundamental three-dimensional equations for incompressible nonviscous flow .
[retrieveDocuments][L2] Retrieving documents for query: has anyone derived simplified pump design equation from the fundamental three-dimensional equations for incompressible nonviscous flow .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['anyon', 'deriv', 'simplifi', 'pump', 'design', 'equat', 'fundament', 'three-dimension', 'equat', 'incompress', 'nonvisc', 'flow']
[retrieveDocuments][L3] Retrieval complete. 1030 documents found.
[main][L2] Query 129: Retrieved 1030 documents.
[main][L2] Processing query 130: what are the flutter characteristics of the exposed skin panels of the x-15 vertical stabilizer when subjected to aerodynamic heating .
[retrieveDocuments][L2] Retrieving documents for query: what are the flutter characteristics of the exposed skin panels of the x-15 vertical stabilizer when subjected to aerodynamic heating .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['flutter', 'characterist', 'expos', 'skin', 'panel', 'x-15', 'vertic', 'stabil', 'subject', 'aerodynam', 'heat']
[retrieveDocuments][L3] Retrieval complete. 716 documents found.
[main][L2] Query 130: Retrieved 716 documents.
[main][L2] Processing query 131: what agreement is found between theoretically predicted instability times and experimentally measured collapse times for compressed columns in creep .
[retrieveDocuments][L2] Retrieving documents for query: what agreement is found between theoretically predicted instability times and experimentally measured collapse times for compressed columns in creep .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['agreement', 'found', 'theoret', 'predict', 'instabl', 'time', 'experiment', 'measur', 'collaps', 'time', 'compress', 'column', 'creep']
[retrieveDocuments][L3] Retrieval complete. 960 documents found.
[main][L2] Query 131: Retrieved 960 documents.
[main][L2] Processing query 132: theoretical studies of creep buckling .
[retrieveDocuments][L2] Retrieving documents for query: theoretical studies of creep buckling .
[retrieveDocuments][L3] Query tokenized into 4 tokens: ['theoret', 'studi', 'creep', 'buckl']
[retrieveDocuments][L3] Retrieval complete. 502 documents found.
[main][L2] Query 132: Retrieved 502 documents.
[main][L2] Processing query 133: experimental studies of creep buckling .
[retrieveDocuments][L2] Retrieving documents for query: experimental studies of creep buckling .
[retrieveDocuments][L3] Query tokenized into 4 tokens: ['experiment', 'studi', 'creep', 'buckl']
[retrieveDocuments][L3] Retrieval complete. 582 documents found.
[main][L2] Query 133: Retrieved 582 documents.
[main][L2] Processing query 134: is it possible to correlate the results on the creep buckling of widely different structures within the framework of a single theory .
[retrieveDocuments][L2] Retrieving documents for query: is it possible to correlate the results on the creep buckling of widely different structures within the framework of a single theory .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['possibl', 'correl', 'result', 'creep', 'buckl', 'wide', 'differ', 'structur', 'within', 'framework', 'singl', 'theori']
[retrieveDocuments][L3] Retrieval complete. 1079 documents found.
[main][L2] Query 134: Retrieved 1079 documents.
[main][L2] Processing query 135: what are the experimental results for the creep buckling of columns .
[retrieveDocuments][L2] Retrieving documents for query: what are the experimental results for the creep buckling of columns .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['experiment', 'result', 'creep', 'buckl', 'column']
[retrieveDocuments][L3] Retrieval complete. 845 documents found.
[main][L2] Query 135: Retrieved 845 documents.
[main][L2] Processing query 136: what are the results for the creep buckling of round tubes under external pressure .
[retrieveDocuments][L2] Retrieving documents for query: what are the results for the creep buckling of round tubes under external pressure .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['result', 'creep', 'buckl', 'round', 'tube', 'extern', 'pressur']
[retrieveDocuments][L3] Retrieval complete. 980 documents found.
[main][L2] Query 136: Retrieved 980 documents.
[main][L2] Processing query 137: have any analytical studies been conducted on the time-to-failure mechanism associated with creep collapse for a long circular cylindrical shell which exhibits both primary and secondary creep as well as elastic deformations under various distributed force systems .
[retrieveDocuments][L2] Retrieving documents for query: have any analytical studies been conducted on the time-to-failure mechanism associated with creep collapse for a long circular cylindrical shell which exhibits both primary and secondary creep as well as elastic deformations under various distributed force systems .
[retrieveDocuments][L3] Query tokenized into 23 tokens: ['analyt', 'studi', 'conduct', 'time-to-failur', 'mechan', 'associ', 'creep', 'collaps', 'long', 'circular', 'cylindr', 'will', 'exhibit', 'primari', 'secondari', 'creep', 'well', 'elast', 'deform', 'variou', 'distribut', 'forc', 'system']
[retrieveDocuments][L3] Retrieval complete. 1085 documents found.
[main][L2] Query 137: Retrieved 1085 documents.
[main][L2] Processing query 138: has the effect of initial stresses,  on the frequencies of vibration of circular cylindrical shells,  been investigated .
[retrieveDocuments][L2] Retrieving documents for query: has the effect of initial stresses,  on the frequencies of vibration of circular cylindrical shells,  been investigated .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['effect', 'initi', 'stress', 'frequenc', 'vibrat', 'circular', 'cylindr', 'shell', 'investig']
[retrieveDocuments][L3] Retrieval complete. 952 documents found.
[main][L2] Query 138: Retrieved 952 documents.
[main][L2] Processing query 139: has the effect of the change of initial pressure due to deformation,  on the frequencies of vibration of circular cylindrical shells been investigated .
[retrieveDocuments][L2] Retrieving documents for query: has the effect of the change of initial pressure due to deformation,  on the frequencies of vibration of circular cylindrical shells been investigated .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['effect', 'chang', 'initi', 'pressur', 'due', 'deform', 'frequenc', 'vibrat', 'circular', 'cylindr', 'shell', 'investig']
[retrieveDocuments][L3] Retrieval complete. 1103 documents found.
[main][L2] Query 139: Retrieved 1103 documents.
[main][L2] Processing query 140: what are the discontinuity stresses at junctions in pressurized structures .
[retrieveDocuments][L2] Retrieving documents for query: what are the discontinuity stresses at junctions in pressurized structures .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['discontinu', 'stress', 'junction', 'pressur', 'structur']
[retrieveDocuments][L3] Retrieval complete. 689 documents found.
[main][L2] Query 140: Retrieved 689 documents.
[main][L2] Processing query 141: what analytical solutions are available for stresses in edge-loaded shells of revolution .
[retrieveDocuments][L2] Retrieving documents for query: what analytical solutions are available for stresses in edge-loaded shells of revolution .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['analyt', 'solut', 'avail', 'stress', 'edge-load', 'shell', 'revolut']
[retrieveDocuments][L3] Retrieval complete. 691 documents found.
[main][L2] Query 141: Retrieved 691 documents.
[main][L2] Processing query 142: what dome contours minimize discontinuity stresses when used as closures on cylindrical pressure vessels .
[retrieveDocuments][L2] Retrieving documents for query: what dome contours minimize discontinuity stresses when used as closures on cylindrical pressure vessels .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['dome', 'contour', 'minim', 'discontinu', 'stress', 'us', 'closur', 'cylindr', 'pressur', 'vessel']
[retrieveDocuments][L3] Retrieval complete. 939 documents found.
[main][L2] Query 142: Retrieved 939 documents.
[main][L2] Processing query 143: what general solutions for the stresses in pressurized shells of revolution are available .
[retrieveDocuments][L2] Retrieving documents for query: what general solutions for the stresses in pressurized shells of revolution are available .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['gener', 'solut', 'stress', 'pressur', 'shell', 'revolut', 'avail']
[retrieveDocuments][L3] Retrieval complete. 1039 documents found.
[main][L2] Query 143: Retrieved 1039 documents.
[main][L2] Processing query 144: can studies of pure membrane cylinders having no wall bending stiffness but maintaining their shape by virtue of internal pressure provide any insight into the behaviour of pressurized cylinders with finite wall stiffness .
[retrieveDocuments][L2] Retrieving documents for query: can studies of pure membrane cylinders having no wall bending stiffness but maintaining their shape by virtue of internal pressure provide any insight into the behaviour of pressurized cylinders with finite wall stiffness .
[retrieveDocuments][L3] Query tokenized into 21 tokens: ['can', 'studi', 'pure', 'membran', 'cylind', 'wall', 'bend', 'stiff', 'maintain', 'shape', 'virtu', 'intern', 'pressur', 'provid', 'insight', 'behaviour', 'pressur', 'cylind', 'finit', 'wall', 'stiff']
[retrieveDocuments][L3] Retrieval complete. 1061 documents found.
[main][L2] Query 144: Retrieved 1061 documents.
[main][L2] Processing query 145: what are the best experimental data and classical small deflection theory analyses available for pressurized cylinders in bending .
[retrieveDocuments][L2] Retrieving documents for query: what are the best experimental data and classical small deflection theory analyses available for pressurized cylinders in bending .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['best', 'experiment', 'data', 'classic', 'small', 'deflect', 'theori', 'analys', 'avail', 'pressur', 'cylind', 'bend']
[retrieveDocuments][L3] Retrieval complete. 1072 documents found.
[main][L2] Query 145: Retrieved 1072 documents.
[main][L2] Processing query 146: does a membrane theory exist by which the behaviour of pressurized membrane cylinders in bending can be predicted .
[retrieveDocuments][L2] Retrieving documents for query: does a membrane theory exist by which the behaviour of pressurized membrane cylinders in bending can be predicted .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['membran', 'theori', 'exist', 'behaviour', 'pressur', 'membran', 'cylind', 'bend', 'can', 'predict']
[retrieveDocuments][L3] Retrieval complete. 1021 documents found.
[main][L2] Query 146: Retrieved 1021 documents.
[main][L2] Processing query 147: what are the equations which define the stability of simply supported corrugated core sandwich cylinders .
[retrieveDocuments][L2] Retrieving documents for query: what are the equations which define the stability of simply supported corrugated core sandwich cylinders .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['equat', 'defin', 'stabil', 'simpli', 'support', 'corrug', 'core', 'sandwich', 'cylind']
[retrieveDocuments][L3] Retrieval complete. 654 documents found.
[main][L2] Query 147: Retrieved 654 documents.
[main][L2] Processing query 148: papers on small deflection theory for buckling of sandwich cylinders .
[retrieveDocuments][L2] Retrieving documents for query: papers on small deflection theory for buckling of sandwich cylinders .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['paper', 'small', 'deflect', 'theori', 'buckl', 'sandwich', 'cylind']
[retrieveDocuments][L3] Retrieval complete. 826 documents found.
[main][L2] Query 148: Retrieved 826 documents.
[main][L2] Processing query 149: has anyone developed an analysis which accurately establishes the large deflection behaviour of conical shells .
[retrieveDocuments][L2] Retrieving documents for query: has anyone developed an analysis which accurately establishes the large deflection behaviour of conical shells .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['anyon', 'develop', 'analysi', 'accur', 'establish', 'larg', 'deflect', 'behaviour', 'conic', 'shell']
[retrieveDocuments][L3] Retrieval complete. 761 documents found.
[main][L2] Query 149: Retrieved 761 documents.
[main][L2] Processing query 150: what is the magnitude of second-order wing-body interference at high supersonic mach number .
[retrieveDocuments][L2] Retrieving documents for query: what is the magnitude of second-order wing-body interference at high supersonic mach number .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['magnitud', 'second-ord', 'wing-bodi', 'interfer', 'high', 'superson', 'mach', 'number']
[retrieveDocuments][L3] Retrieval complete. 794 documents found.
[main][L2] Query 150: Retrieved 794 documents.
[main][L2] Processing query 151: what is the best theoretical method for calculating pressure on the surface of a wing alone .
[retrieveDocuments][L2] Retrieving documents for query: what is the best theoretical method for calculating pressure on the surface of a wing alone .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['best', 'theoret', 'method', 'calcul', 'pressur', 'surfac', 'wing', 'alon']
[retrieveDocuments][L3] Retrieval complete. 1091 documents found.
[main][L2] Query 151: Retrieved 1091 documents.
[main][L2] Processing query 152: how can the effect of the boundary-layer on wing pressure be calculated, and what is its magnitude .
[retrieveDocuments][L2] Retrieving documents for query: how can the effect of the boundary-layer on wing pressure be calculated, and what is its magnitude .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['can', 'effect', 'boundary-lay', 'wing', 'pressur', 'calcul', 'magnitud']
[retrieveDocuments][L3] Retrieval complete. 1087 documents found.
[main][L2] Query 152: Retrieved 1087 documents.
[main][L2] Processing query 153: how should the navier-stokes difference equations be solved .
[retrieveDocuments][L2] Retrieving documents for query: how should the navier-stokes difference equations be solved .
[retrieveDocuments][L3] Query tokenized into 4 tokens: ['navier-stok', 'differ', 'equat', 'solv']
[retrieveDocuments][L3] Retrieval complete. 553 documents found.
[main][L2] Query 153: Retrieved 553 documents.
[main][L2] Processing query 154: which  iterative method for solving linear elliptic difference equations is most rapidly convergent .
[retrieveDocuments][L2] Retrieving documents for query: which  iterative method for solving linear elliptic difference equations is most rapidly convergent .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['iter', 'method', 'solv', 'linear', 'ellipt', 'differ', 'equat', 'rapidli', 'converg']
[retrieveDocuments][L3] Retrieval complete. 847 documents found.
[main][L2] Query 154: Retrieved 847 documents.
[main][L2] Processing query 155: technical report on measurement of ablation during flight .
[retrieveDocuments][L2] Retrieving documents for query: technical report on measurement of ablation during flight .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['technic', 'report', 'measur', 'ablat', 'flight']
[retrieveDocuments][L3] Retrieval complete. 439 documents found.
[main][L2] Query 155: Retrieved 439 documents.
[main][L2] Processing query 156: what qualitative and quantitative material is available on ablation materials research .
[retrieveDocuments][L2] Retrieving documents for query: what qualitative and quantitative material is available on ablation materials research .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['qualit', 'quantit', 'materi', 'avail', 'ablat', 'materi', 'research']
[retrieveDocuments][L3] Retrieval complete. 251 documents found.
[main][L2] Query 156: Retrieved 251 documents.
[main][L2] Processing query 157: have flow fields been calculated for blunt-nosed bodies and compared with experiment for a wide range of free stream conditions and body shapes .
[retrieveDocuments][L2] Retrieving documents for query: have flow fields been calculated for blunt-nosed bodies and compared with experiment for a wide range of free stream conditions and body shapes .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['flow', 'field', 'calcul', 'blunt-nos', 'bodi', 'compar', 'experi', 'wide', 'rang', 'free', 'stream', 'condit', 'bodi', 'shape']
[retrieveDocuments][L3] Retrieval complete. 1185 documents found.
[main][L2] Query 157: Retrieved 1185 documents.
[main][L2] Processing query 158: what are the available properties of high-temperature air .
[retrieveDocuments][L2] Retrieving documents for query: what are the available properties of high-temperature air .
[retrieveDocuments][L3] Query tokenized into 4 tokens: ['avail', 'properti', 'high-temperatur', 'air']
[retrieveDocuments][L3] Retrieval complete. 333 documents found.
[main][L2] Query 158: Retrieved 333 documents.
[main][L2] Processing query 159: what is the magnitude of aerodynamic damping in flexible vibration modes of a slender body of revolution characteristic of launch vehicles .
[retrieveDocuments][L2] Retrieving documents for query: what is the magnitude of aerodynamic damping in flexible vibration modes of a slender body of revolution characteristic of launch vehicles .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['magnitud', 'aerodynam', 'damp', 'flexibl', 'vibrat', 'mode', 'slender', 'bodi', 'revolut', 'characterist', 'launch', 'vehicl']
[retrieveDocuments][L3] Retrieval complete. 662 documents found.
[main][L2] Query 159: Retrieved 662 documents.
[main][L2] Processing query 160: compressive circumferential stresses in a torispherical shell reveal the possibility of buckling under internal pressure . has anyone investigated for which ranges of shell parameters these stresses are sufficiently large to cause elastic buckling .
[retrieveDocuments][L2] Retrieving documents for query: compressive circumferential stresses in a torispherical shell reveal the possibility of buckling under internal pressure . has anyone investigated for which ranges of shell parameters these stresses are sufficiently large to cause elastic buckling .
[retrieveDocuments][L3] Query tokenized into 21 tokens: ['compress', 'circumferenti', 'stress', 'torispher', 'will', 'reveal', 'possibl', 'buckl', 'intern', 'pressur', 'anyon', 'investig', 'rang', 'will', 'paramet', 'stress', 'suffici', 'larg', 'caus', 'elast', 'buckl']
[retrieveDocuments][L3] Retrieval complete. 1126 documents found.
[main][L2] Query 160: Retrieved 1126 documents.
[main][L2] Processing query 161: is there an integral method to give a single and sufficiently accurate method of calculating the laminar separate point for various incompressible and compressible boundary layers with zero heat transfer .
[retrieveDocuments][L2] Retrieving documents for query: is there an integral method to give a single and sufficiently accurate method of calculating the laminar separate point for various incompressible and compressible boundary layers with zero heat transfer .
[retrieveDocuments][L3] Query tokenized into 19 tokens: ['integr', 'method', 'give', 'singl', 'suffici', 'accur', 'method', 'calcul', 'laminar', 'separ', 'point', 'variou', 'incompress', 'compress', 'boundari', 'layer', 'zero', 'heat', 'transfer']
[retrieveDocuments][L3] Retrieval complete. 1199 documents found.
[main][L2] Query 161: Retrieved 1199 documents.
[main][L2] Processing query 162: what accurate or exact solutions of the laminar separation point for various incompressible and compressible boundary layers with zero heat transfer are available .
[retrieveDocuments][L2] Retrieving documents for query: what accurate or exact solutions of the laminar separation point for various incompressible and compressible boundary layers with zero heat transfer are available .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['accur', 'exact', 'solut', 'laminar', 'separ', 'point', 'variou', 'incompress', 'compress', 'boundari', 'layer', 'zero', 'heat', 'transfer', 'avail']
[retrieveDocuments][L3] Retrieval complete. 1091 documents found.
[main][L2] Query 162: Retrieved 1091 documents.
[main][L2] Processing query 163: can the hypersonic similarity results be applied to the technique of predicting surface pressures of an ogive forebody at angle of attack .
[retrieveDocuments][L2] Retrieving documents for query: can the hypersonic similarity results be applied to the technique of predicting surface pressures of an ogive forebody at angle of attack .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['can', 'hyperson', 'similar', 'result', 'appli', 'techniqu', 'predict', 'surfac', 'pressur', 'ogiv', 'forebodi', 'angl', 'attack']
[retrieveDocuments][L3] Retrieval complete. 1208 documents found.
[main][L2] Query 163: Retrieved 1208 documents.
[main][L2] Processing query 164: what determines the onset of shock-induced boundary-layer separation .
[retrieveDocuments][L2] Retrieving documents for query: what determines the onset of shock-induced boundary-layer separation .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['determin', 'onset', 'shock-induc', 'boundary-lay', 'separ']
[retrieveDocuments][L3] Retrieval complete. 505 documents found.
[main][L2] Query 164: Retrieved 505 documents.
[main][L2] Processing query 165: are the stable profiles of a compressible boundary layer induced by a moving wave known .
[retrieveDocuments][L2] Retrieving documents for query: are the stable profiles of a compressible boundary layer induced by a moving wave known .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['stabl', 'profil', 'compress', 'boundari', 'layer', 'induc', 'move', 'wave', 'known']
[retrieveDocuments][L3] Retrieval complete. 803 documents found.
[main][L2] Query 165: Retrieved 803 documents.
[main][L2] Processing query 166: are there experimental results on the stability of a compressible boundary layer induced by a moving wave .
[retrieveDocuments][L2] Retrieving documents for query: are there experimental results on the stability of a compressible boundary layer induced by a moving wave .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['experiment', 'result', 'stabil', 'compress', 'boundari', 'layer', 'induc', 'move', 'wave']
[retrieveDocuments][L3] Retrieval complete. 1117 documents found.
[main][L2] Query 166: Retrieved 1117 documents.
[main][L2] Processing query 167: exact solution methods for calculating the ablative mass loss of a material ablating at high temperatures in a hypersonic flight environment .
[retrieveDocuments][L2] Retrieving documents for query: exact solution methods for calculating the ablative mass loss of a material ablating at high temperatures in a hypersonic flight environment .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['exact', 'solut', 'method', 'calcul', 'abl', 'mass', 'loss', 'materi', 'ablat', 'high', 'temperatur', 'hyperson', 'flight', 'environ']
[retrieveDocuments][L3] Retrieval complete. 1071 documents found.
[main][L2] Query 167: Retrieved 1071 documents.
[main][L2] Processing query 168: what approximate solutions are known to the direct problem of transonic flow in the throat of a nozzle,  i.e. finding the flow in a given nozzle .
[retrieveDocuments][L2] Retrieving documents for query: what approximate solutions are known to the direct problem of transonic flow in the throat of a nozzle,  i.e. finding the flow in a given nozzle .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['approxim', 'solut', 'known', 'direct', 'problem', 'transon', 'flow', 'throat', 'nozzl', 'i.e.', 'find', 'flow', 'given', 'nozzl']
[retrieveDocuments][L3] Retrieval complete. 1171 documents found.
[main][L2] Query 168: Retrieved 1171 documents.
[main][L2] Processing query 169: what approximate solutions are known to the indirect problem of transonic flow in the throat of a nozzle,  i.e. finding a nozzle which has a given axial velocity distribution .
[retrieveDocuments][L2] Retrieving documents for query: what approximate solutions are known to the indirect problem of transonic flow in the throat of a nozzle,  i.e. finding a nozzle which has a given axial velocity distribution .
[retrieveDocuments][L3] Query tokenized into 16 tokens: ['approxim', 'solut', 'known', 'indirect', 'problem', 'transon', 'flow', 'throat', 'nozzl', 'i.e.', 'find', 'nozzl', 'given', 'axial', 'veloc', 'distribut']
[retrieveDocuments][L3] Retrieval complete. 1246 documents found.
[main][L2] Query 169: Retrieved 1246 documents.
[main][L2] Processing query 170: why do users of orthodox pitot-static tubes often find that the calibrations appear to be,. - (a) significantly different from those formerly specified,  (b) wildly variable at low reynolds numbers .
[retrieveDocuments][L2] Retrieving documents for query: why do users of orthodox pitot-static tubes often find that the calibrations appear to be,. - (a) significantly different from those formerly specified,  (b) wildly variable at low reynolds numbers .
[retrieveDocuments][L3] Query tokenized into 18 tokens: ['user', 'orthodox', 'pitot-stat', 'tube', 'often', 'find', 'calibr', 'appear', 'significantli', 'differ', 'formerli', 'specifi', 'b', 'wildli', 'variabl', 'low', 'reynold', 'number']
[retrieveDocuments][L3] Retrieval complete. 857 documents found.
[main][L2] Query 170: Retrieved 857 documents.
[main][L2] Processing query 171: has a comparison been made between interference-free drag measurements using free-flight models and similar measurements made in a low-blockage wind tunnel .
[retrieveDocuments][L2] Retrieving documents for query: has a comparison been made between interference-free drag measurements using free-flight models and similar measurements made in a low-blockage wind tunnel .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['comparison', 'made', 'interference-fre', 'drag', 'measur', 'us', 'free-flight', 'model', 'similar', 'measur', 'made', 'low-blockag', 'wind', 'tunnel']
[retrieveDocuments][L3] Retrieval complete. 994 documents found.
[main][L2] Query 171: Retrieved 994 documents.
[main][L2] Processing query 172: solution of the blasius problem with three-point boundary conditions .
[retrieveDocuments][L2] Retrieving documents for query: solution of the blasius problem with three-point boundary conditions .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['solut', 'blasiu', 'problem', 'three-point', 'boundari', 'condit']
[retrieveDocuments][L3] Retrieval complete. 881 documents found.
[main][L2] Query 172: Retrieved 881 documents.
[main][L2] Processing query 173: references on lyapunov's method on the stability of linear differential equations with periodic coefficients .
[retrieveDocuments][L2] Retrieving documents for query: references on lyapunov's method on the stability of linear differential equations with periodic coefficients .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['refer', 'lyapunov', 's', 'method', 'stabil', 'linear', 'differenti', 'equat', 'period', 'coeffici']
[retrieveDocuments][L3] Retrieval complete. 964 documents found.
[main][L2] Query 173: Retrieved 964 documents.
[main][L2] Processing query 174: obtain all papers and reports that contain shock detachment distance data .
[retrieveDocuments][L2] Retrieving documents for query: obtain all papers and reports that contain shock detachment distance data .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['obtain', 'paper', 'report', 'contain', 'shock', 'detach', 'distanc', 'data']
[retrieveDocuments][L3] Retrieval complete. 901 documents found.
[main][L2] Query 174: Retrieved 901 documents.
[main][L2] Processing query 175: work on flow in channels at low reynolds numbers .
[retrieveDocuments][L2] Retrieving documents for query: work on flow in channels at low reynolds numbers .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['work', 'flow', 'channel', 'low', 'reynold', 'number']
[retrieveDocuments][L3] Retrieval complete. 988 documents found.
[main][L2] Query 175: Retrieved 988 documents.
[main][L2] Processing query 176: some approximate analytical heat conduction solutions using methods other than biot's principle .
[retrieveDocuments][L2] Retrieving documents for query: some approximate analytical heat conduction solutions using methods other than biot's principle .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['approxim', 'analyt', 'heat', 'conduct', 'solut', 'us', 'method', 'biot', 's', 'principl']
[retrieveDocuments][L3] Retrieval complete. 1145 documents found.
[main][L2] Query 176: Retrieved 1145 documents.
[main][L2] Processing query 177: what mode of stalling can be expected for each stage of an axial compressor .
[retrieveDocuments][L2] Retrieving documents for query: what mode of stalling can be expected for each stage of an axial compressor .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['mode', 'stall', 'can', 'expect', 'stage', 'axial', 'compressor']
[retrieveDocuments][L3] Retrieval complete. 483 documents found.
[main][L2] Query 177: Retrieved 483 documents.
[main][L2] Processing query 178: has a criterion been established for determining the axial compressor choking line .
[retrieveDocuments][L2] Retrieving documents for query: has a criterion been established for determining the axial compressor choking line .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['criterion', 'establish', 'determin', 'axial', 'compressor', 'choke', 'line']
[retrieveDocuments][L3] Retrieval complete. 493 documents found.
[main][L2] Query 178: Retrieved 493 documents.
[main][L2] Processing query 179: has a theory of quasi-conical flows been developed, in supersonic linearised theory, for which the upwash distribution on the lifting surface, apart from being a homogeneous function in the co-ordinate, is permitted to have a quite general functional form .
[retrieveDocuments][L2] Retrieving documents for query: has a theory of quasi-conical flows been developed, in supersonic linearised theory, for which the upwash distribution on the lifting surface, apart from being a homogeneous function in the co-ordinate, is permitted to have a quite general functional form .
[retrieveDocuments][L3] Query tokenized into 20 tokens: ['theori', 'quasi-con', 'flow', 'develop', 'superson', 'linearis', 'theori', 'upwash', 'distribut', 'lift', 'surfac', 'apart', 'homogen', 'function', 'co-ordin', 'permit', 'quit', 'gener', 'function', 'form']
[retrieveDocuments][L3] Retrieval complete. 1249 documents found.
[main][L2] Query 179: Retrieved 1249 documents.
[main][L2] Processing query 180: how does scale height vary with altitude in an atmosphere .
[retrieveDocuments][L2] Retrieving documents for query: how does scale height vary with altitude in an atmosphere .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['scale', 'height', 'vari', 'altitud', 'atmospher']
[retrieveDocuments][L3] Retrieval complete. 275 documents found.
[main][L2] Query 180: Retrieved 275 documents.
[main][L2] Processing query 181: jet interference with supersonic flows theoretical papers .
[retrieveDocuments][L2] Retrieving documents for query: jet interference with supersonic flows theoretical papers .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['jet', 'interfer', 'superson', 'flow', 'theoret', 'paper']
[retrieveDocuments][L3] Retrieval complete. 1007 documents found.
[main][L2] Query 181: Retrieved 1007 documents.
[main][L2] Processing query 182: effects of leading-edge bluntness on the flutter characteristics of some square-planform double-wedge airfoils at mach numbers less than 15.4.
[retrieveDocuments][L2] Retrieving documents for query: effects of leading-edge bluntness on the flutter characteristics of some square-planform double-wedge airfoils at mach numbers less than 15.4.
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['effect', 'leading-edg', 'blunt', 'flutter', 'characterist', 'square-planform', 'double-wedg', 'airfoil', 'mach', 'number', 'less', '15', '4']
[retrieveDocuments][L3] Retrieval complete. 971 documents found.
[main][L2] Query 182: Retrieved 971 documents.
[main][L2] Processing query 183: what factors have been shown to have a primary influence on sonic boom strength .
[retrieveDocuments][L2] Retrieving documents for query: what factors have been shown to have a primary influence on sonic boom strength .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['factor', 'shown', 'primari', 'influenc', 'sonic', 'boom', 'strength']
[retrieveDocuments][L3] Retrieval complete. 499 documents found.
[main][L2] Query 183: Retrieved 499 documents.
[main][L2] Processing query 184: work on small-oscillation re-entry motions .
[retrieveDocuments][L2] Retrieving documents for query: work on small-oscillation re-entry motions .
[retrieveDocuments][L3] Query tokenized into 4 tokens: ['work', 'small-oscil', 're-entri', 'motion']
[retrieveDocuments][L3] Retrieval complete. 238 documents found.
[main][L2] Query 184: Retrieved 238 documents.
[main][L2] Processing query 185: experimental studies on panel flutter .
[retrieveDocuments][L2] Retrieving documents for query: experimental studies on panel flutter .
[retrieveDocuments][L3] Query tokenized into 4 tokens: ['experiment', 'studi', 'panel', 'flutter']
[retrieveDocuments][L3] Retrieval complete. 541 documents found.
[main][L2] Query 185: Retrieved 541 documents.
[main][L2] Processing query 186: how can wing-body,  flow field interference effects be approximated rationally .
[retrieveDocuments][L2] Retrieving documents for query: how can wing-body,  flow field interference effects be approximated rationally .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['can', 'wing-bodi', 'flow', 'field', 'interfer', 'effect', 'approxim', 'ration']
[retrieveDocuments][L3] Retrieval complete. 1134 documents found.
[main][L2] Query 186: Retrieved 1134 documents.
[main][L2] Processing query 187: has anyone analytically or experimentally investigated the effects of internal pressure on the buckling of circular-cylindrical shells under bending .
[retrieveDocuments][L2] Retrieving documents for query: has anyone analytically or experimentally investigated the effects of internal pressure on the buckling of circular-cylindrical shells under bending .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['anyon', 'analyt', 'experiment', 'investig', 'effect', 'intern', 'pressur', 'buckl', 'circular-cylindr', 'shell', 'bend']
[retrieveDocuments][L3] Retrieval complete. 1098 documents found.
[main][L2] Query 187: Retrieved 1098 documents.
[main][L2] Processing query 188: what theoretical and experimental work has been done on the excitation and response of typical structures in a noise environment .
[retrieveDocuments][L2] Retrieving documents for query: what theoretical and experimental work has been done on the excitation and response of typical structures in a noise environment .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['theoret', 'experiment', 'work', 'done', 'excit', 'respons', 'typic', 'structur', 'nois', 'environ']
[retrieveDocuments][L3] Retrieval complete. 627 documents found.
[main][L2] Query 188: Retrieved 627 documents.
[main][L2] Processing query 189: is there a design method for calculating thermal fatigue endurances of components of various types and sizes in a variety of circumstances .
[retrieveDocuments][L2] Retrieving documents for query: is there a design method for calculating thermal fatigue endurances of components of various types and sizes in a variety of circumstances .
[retrieveDocuments][L3] Query tokenized into 12 tokens: ['design', 'method', 'calcul', 'thermal', 'fatigu', 'endur', 'compon', 'variou', 'type', 'size', 'varieti', 'circumst']
[retrieveDocuments][L3] Retrieval complete. 911 documents found.
[main][L2] Query 189: Retrieved 911 documents.
[main][L2] Processing query 190: will an analysis of panel flutter based on arbitrarily assumed modes of deformation prove satisfactory,  and if so,  what is the minimum number of modes that need be considered .
[retrieveDocuments][L2] Retrieving documents for query: will an analysis of panel flutter based on arbitrarily assumed modes of deformation prove satisfactory,  and if so,  what is the minimum number of modes that need be considered .
[retrieveDocuments][L3] Query tokenized into 16 tokens: ['will', 'analysi', 'panel', 'flutter', 'base', 'arbitrarili', 'assum', 'mode', 'deform', 'prove', 'satisfactori', 'minimum', 'number', 'mode', 'need', 'consid']
[retrieveDocuments][L3] Retrieval complete. 1063 documents found.
[main][L2] Query 190: Retrieved 1063 documents.
[main][L2] Processing query 191: what is the criterion for true panel flutter,  as opposed to small amplitude vibration arising from acoustic disturbances .
[retrieveDocuments][L2] Retrieving documents for query: what is the criterion for true panel flutter,  as opposed to small amplitude vibration arising from acoustic disturbances .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['criterion', 'true', 'panel', 'flutter', 'oppos', 'small', 'amplitud', 'vibrat', 'aris', 'acoust', 'disturb']
[retrieveDocuments][L3] Retrieval complete. 420 documents found.
[main][L2] Query 191: Retrieved 420 documents.
[main][L2] Processing query 192: papers dealing with uniformly loaded sectors .
[retrieveDocuments][L2] Retrieving documents for query: papers dealing with uniformly loaded sectors .
[retrieveDocuments][L3] Query tokenized into 5 tokens: ['paper', 'deal', 'uniformli', 'load', 'sector']
[retrieveDocuments][L3] Retrieval complete. 390 documents found.
[main][L2] Query 192: Retrieved 390 documents.
[main][L2] Processing query 193: general methods of solving clamped plate problems .
[retrieveDocuments][L2] Retrieving documents for query: general methods of solving clamped plate problems .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['gener', 'method', 'solv', 'clamp', 'plate', 'problem']
[retrieveDocuments][L3] Retrieval complete. 891 documents found.
[main][L2] Query 193: Retrieved 891 documents.
[main][L2] Processing query 194: how can the analytical solution of the buckling strength of a uniform circular cylinder loaded in axial compression be refined so as to lower the buckling load .
[retrieveDocuments][L2] Retrieving documents for query: how can the analytical solution of the buckling strength of a uniform circular cylinder loaded in axial compression be refined so as to lower the buckling load .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['can', 'analyt', 'solut', 'buckl', 'strength', 'uniform', 'circular', 'cylind', 'load', 'axial', 'compress', 'refin', 'lower', 'buckl', 'load']
[retrieveDocuments][L3] Retrieval complete. 984 documents found.
[main][L2] Query 194: Retrieved 984 documents.
[main][L2] Processing query 195: in the problem of the buckling strength of uniform circular cylinders loaded in axial compression,  does the linear solution help with improving the non-linear one .
[retrieveDocuments][L2] Retrieving documents for query: in the problem of the buckling strength of uniform circular cylinders loaded in axial compression,  does the linear solution help with improving the non-linear one .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['problem', 'buckl', 'strength', 'uniform', 'circular', 'cylind', 'load', 'axial', 'compress', 'linear', 'solut', 'help', 'improv', 'non-linear', 'on']
[retrieveDocuments][L3] Retrieval complete. 1026 documents found.
[main][L2] Query 195: Retrieved 1026 documents.
[main][L2] Processing query 196: the problem of similarity for representative investigation of aeroelastic effects in a flow with the absence of heating effects .
[retrieveDocuments][L2] Retrieving documents for query: the problem of similarity for representative investigation of aeroelastic effects in a flow with the absence of heating effects .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['problem', 'similar', 'repres', 'investig', 'aeroelast', 'effect', 'flow', 'absenc', 'heat', 'effect']
[retrieveDocuments][L3] Retrieval complete. 1205 documents found.
[main][L2] Query 196: Retrieved 1205 documents.
[main][L2] Processing query 197: how is fatigue damage estimated using the normal long-hand method .
[retrieveDocuments][L2] Retrieving documents for query: how is fatigue damage estimated using the normal long-hand method .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['fatigu', 'damag', 'estim', 'us', 'normal', 'long-hand', 'method']
[retrieveDocuments][L3] Retrieval complete. 824 documents found.
[main][L2] Query 197: Retrieved 824 documents.
[main][L2] Processing query 198: is there any information available on the difference in the effects of various edge conditions on the buckling of cylindrical shells .
[retrieveDocuments][L2] Retrieving documents for query: is there any information available on the difference in the effects of various edge conditions on the buckling of cylindrical shells .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['inform', 'avail', 'differ', 'effect', 'variou', 'edg', 'condit', 'buckl', 'cylindr', 'shell']
[retrieveDocuments][L3] Retrieval complete. 1022 documents found.
[main][L2] Query 198: Retrieved 1022 documents.
[main][L2] Processing query 199: have non-linear large deflection analyses been conducted for shell shapes other than conical .
[retrieveDocuments][L2] Retrieving documents for query: have non-linear large deflection analyses been conducted for shell shapes other than conical .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['non-linear', 'larg', 'deflect', 'analys', 'conduct', 'will', 'shape', 'conic']
[retrieveDocuments][L3] Retrieval complete. 645 documents found.
[main][L2] Query 199: Retrieved 645 documents.
[main][L2] Processing query 200: are asymptotic methods sufficiently accurate in the determination of pre-buckling stresses in torispherical shells,  or must we resort to numerical methods .
[retrieveDocuments][L2] Retrieving documents for query: are asymptotic methods sufficiently accurate in the determination of pre-buckling stresses in torispherical shells,  or must we resort to numerical methods .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['asymptot', 'method', 'suffici', 'accur', 'determin', 'pre-buckl', 'stress', 'torispher', 'shell', 'must', 'resort', 'numer', 'method']
[retrieveDocuments][L3] Retrieval complete. 872 documents found.
[main][L2] Query 200: Retrieved 872 documents.
[main][L2] Processing query 201: what are the nonequilibrium chemical constituents in the viscous shock layer ahead of a blunt re-entry vehicle .
[retrieveDocuments][L2] Retrieving documents for query: what are the nonequilibrium chemical constituents in the viscous shock layer ahead of a blunt re-entry vehicle .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['nonequilibrium', 'chemic', 'constitu', 'viscou', 'shock', 'layer', 'ahead', 'blunt', 're-entri', 'vehicl']
[retrieveDocuments][L3] Retrieval complete. 654 documents found.
[main][L2] Query 201: Retrieved 654 documents.
[main][L2] Processing query 202: how accurate are existing analytical theories in estimating pressure distributions on cones at incidence,  at hypersonic speeds .
[retrieveDocuments][L2] Retrieving documents for query: how accurate are existing analytical theories in estimating pressure distributions on cones at incidence,  at hypersonic speeds .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['accur', 'exist', 'analyt', 'theori', 'estim', 'pressur', 'distribut', 'cone', 'incid', 'hyperson', 'speed']
[retrieveDocuments][L3] Retrieval complete. 1108 documents found.
[main][L2] Query 202: Retrieved 1108 documents.
[main][L2] Processing query 203: are simple empirical methods of any use for estimating pressure distribution in cones .
[retrieveDocuments][L2] Retrieving documents for query: are simple empirical methods of any use for estimating pressure distribution in cones .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['simpl', 'empir', 'method', 'us', 'estim', 'pressur', 'distribut', 'cone']
[retrieveDocuments][L3] Retrieval complete. 1096 documents found.
[main][L2] Query 203: Retrieved 1096 documents.
[main][L2] Processing query 204: do viscous effects seriously modify pressure distributions .
[retrieveDocuments][L2] Retrieving documents for query: do viscous effects seriously modify pressure distributions .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['viscou', 'effect', 'serious', 'modifi', 'pressur', 'distribut']
[retrieveDocuments][L3] Retrieval complete. 975 documents found.
[main][L2] Query 204: Retrieved 975 documents.
[main][L2] Processing query 205: has anyone investigated theoretically whether surface flexibility can stabilize a laminar boundary layer .
[retrieveDocuments][L2] Retrieving documents for query: has anyone investigated theoretically whether surface flexibility can stabilize a laminar boundary layer .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['anyon', 'investig', 'theoret', 'whether', 'surfac', 'flexibl', 'can', 'stabil', 'laminar', 'boundari', 'layer']
[retrieveDocuments][L3] Retrieval complete. 1059 documents found.
[main][L2] Query 205: Retrieved 1059 documents.
[main][L2] Processing query 206: how do subsonic and transonic flutter data measured in the new langley transonic dynamics tunnel compare with similar data obtained in other facilities .
[retrieveDocuments][L2] Retrieving documents for query: how do subsonic and transonic flutter data measured in the new langley transonic dynamics tunnel compare with similar data obtained in other facilities .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['subson', 'transon', 'flutter', 'data', 'measur', 'new', 'langlei', 'transon', 'dynam', 'tunnel', 'compar', 'similar', 'data', 'obtain', 'facil']
[retrieveDocuments][L3] Retrieval complete. 995 documents found.
[main][L2] Query 206: Retrieved 995 documents.
[main][L2] Processing query 207: how do large changes in new mass ratio quantitatively affect wing-flutter boundaries .
[retrieveDocuments][L2] Retrieving documents for query: how do large changes in new mass ratio quantitatively affect wing-flutter boundaries .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['larg', 'chang', 'new', 'mass', 'ratio', 'quantit', 'affect', 'wing-flutt', 'boundari']
[retrieveDocuments][L3] Retrieval complete. 844 documents found.
[main][L2] Query 207: Retrieved 844 documents.
[main][L2] Processing query 208: what is the effect of the shape of the drag polar of a lifting spacecraft on the amount of reduction in maximum deceleration obtainable by continuously varying the aerodynamic coefficients during re-entry .
[retrieveDocuments][L2] Retrieving documents for query: what is the effect of the shape of the drag polar of a lifting spacecraft on the amount of reduction in maximum deceleration obtainable by continuously varying the aerodynamic coefficients during re-entry .
[retrieveDocuments][L3] Query tokenized into 16 tokens: ['effect', 'shape', 'drag', 'polar', 'lift', 'spacecraft', 'amount', 'reduct', 'maximum', 'deceler', 'obtain', 'continu', 'vari', 'aerodynam', 'coeffici', 're-entri']
[retrieveDocuments][L3] Retrieval complete. 1064 documents found.
[main][L2] Query 208: Retrieved 1064 documents.
[main][L2] Processing query 209: what are the physical significance and characteristics of separated laminar and turbulent boundary layer flows .
[retrieveDocuments][L2] Retrieving documents for query: what are the physical significance and characteristics of separated laminar and turbulent boundary layer flows .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['physic', 'signific', 'characterist', 'separ', 'laminar', 'turbul', 'boundari', 'layer', 'flow']
[retrieveDocuments][L3] Retrieval complete. 993 documents found.
[main][L2] Query 209: Retrieved 993 documents.
[main][L2] Processing query 210: has anyone analytically investigated the stabilizing influence of soft elastic cores on the buckling strength of cylindrical shells subjected to non-uniform external pressure .
[retrieveDocuments][L2] Retrieving documents for query: has anyone analytically investigated the stabilizing influence of soft elastic cores on the buckling strength of cylindrical shells subjected to non-uniform external pressure .
[retrieveDocuments][L3] Query tokenized into 16 tokens: ['anyon', 'analyt', 'investig', 'stabil', 'influenc', 'soft', 'elast', 'core', 'buckl', 'strength', 'cylindr', 'shell', 'subject', 'non-uniform', 'extern', 'pressur']
[retrieveDocuments][L3] Retrieval complete. 987 documents found.
[main][L2] Query 210: Retrieved 987 documents.
[main][L2] Processing query 211: what papers are available on the buckling of empty cylindrical shells under non-uniform pressure .
[retrieveDocuments][L2] Retrieving documents for query: what papers are available on the buckling of empty cylindrical shells under non-uniform pressure .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['paper', 'avail', 'buckl', 'empti', 'cylindr', 'shell', 'non-uniform', 'pressur']
[retrieveDocuments][L3] Retrieval complete. 798 documents found.
[main][L2] Query 211: Retrieved 798 documents.
[main][L2] Processing query 212: what effect do thermal stresses have on the compressive buckling strength of ring-stiffened cylinders .
[retrieveDocuments][L2] Retrieving documents for query: what effect do thermal stresses have on the compressive buckling strength of ring-stiffened cylinders .
[retrieveDocuments][L3] Query tokenized into 8 tokens: ['effect', 'thermal', 'stress', 'compress', 'buckl', 'strength', 'ring-stiffen', 'cylind']
[retrieveDocuments][L3] Retrieval complete. 851 documents found.
[main][L2] Query 212: Retrieved 851 documents.
[main][L2] Processing query 213: what is the effect on cylinder buckling of a circumferential stress system that varies in the axial direction .
[retrieveDocuments][L2] Retrieving documents for query: what is the effect on cylinder buckling of a circumferential stress system that varies in the axial direction .
[retrieveDocuments][L3] Query tokenized into 9 tokens: ['effect', 'cylind', 'buckl', 'circumferenti', 'stress', 'system', 'vari', 'axial', 'direct']
[retrieveDocuments][L3] Retrieval complete. 911 documents found.
[main][L2] Query 213: Retrieved 911 documents.
[main][L2] Processing query 214: can non-linear shallow shell analysis be reduced to an engineering technique by use of the matrix .
[retrieveDocuments][L2] Retrieving documents for query: can non-linear shallow shell analysis be reduced to an engineering technique by use of the matrix .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['can', 'non-linear', 'shallow', 'will', 'analysi', 'reduc', 'engin', 'techniqu', 'us', 'matrix']
[retrieveDocuments][L3] Retrieval complete. 941 documents found.
[main][L2] Query 214: Retrieved 941 documents.
[main][L2] Processing query 215: is it possible to predict the shape of a shroud which will allow simulation of the nose region flow field for a sphere in hypersonic flow .
[retrieveDocuments][L2] Retrieving documents for query: is it possible to predict the shape of a shroud which will allow simulation of the nose region flow field for a sphere in hypersonic flow .
[retrieveDocuments][L3] Query tokenized into 14 tokens: ['possibl', 'predict', 'shape', 'shroud', 'will', 'allow', 'simul', 'nose', 'region', 'flow', 'field', 'sphere', 'hyperson', 'flow']
[retrieveDocuments][L3] Retrieval complete. 1071 documents found.
[main][L2] Query 215: Retrieved 1071 documents.
[main][L2] Processing query 216: what investigations have been made of the wave system created by a static pressure distribution over a liquid surface .
[retrieveDocuments][L2] Retrieving documents for query: what investigations have been made of the wave system created by a static pressure distribution over a liquid surface .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['investig', 'made', 'wave', 'system', 'creat', 'static', 'pressur', 'distribut', 'liquid', 'surfac']
[retrieveDocuments][L3] Retrieval complete. 1086 documents found.
[main][L2] Query 216: Retrieved 1086 documents.
[main][L2] Processing query 217: has anyone investigated the effect of shock generated vorticity on heat transfer to a blunt body .
[retrieveDocuments][L2] Retrieving documents for query: has anyone investigated the effect of shock generated vorticity on heat transfer to a blunt body .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['anyon', 'investig', 'effect', 'shock', 'gener', 'vortic', 'heat', 'transfer', 'blunt', 'bodi']
[retrieveDocuments][L3] Retrieval complete. 1064 documents found.
[main][L2] Query 217: Retrieved 1064 documents.
[main][L2] Processing query 218: what is the heat transfer to a blunt body in the absence of vorticity .
[retrieveDocuments][L2] Retrieving documents for query: what is the heat transfer to a blunt body in the absence of vorticity .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['heat', 'transfer', 'blunt', 'bodi', 'absenc', 'vortic']
[retrieveDocuments][L3] Retrieval complete. 559 documents found.
[main][L2] Query 218: Retrieved 559 documents.
[main][L2] Processing query 219: what are the general effects on flow fields when the reynolds number is small .
[retrieveDocuments][L2] Retrieving documents for query: what are the general effects on flow fields when the reynolds number is small .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['gener', 'effect', 'flow', 'field', 'reynold', 'number', 'small']
[retrieveDocuments][L3] Retrieval complete. 1174 documents found.
[main][L2] Query 219: Retrieved 1174 documents.
[main][L2] Processing query 220: find a calculation procedure applicable to all incompressible laminar boundary layer flow problems having good accuracy and reasonable computation time .
[retrieveDocuments][L2] Retrieving documents for query: find a calculation procedure applicable to all incompressible laminar boundary layer flow problems having good accuracy and reasonable computation time .
[retrieveDocuments][L3] Query tokenized into 15 tokens: ['find', 'calcul', 'procedur', 'applic', 'incompress', 'laminar', 'boundari', 'layer', 'flow', 'problem', 'good', 'accuraci', 'reason', 'comput', 'time']
[retrieveDocuments][L3] Retrieval complete. 1199 documents found.
[main][L2] Query 220: Retrieved 1199 documents.
[main][L2] Processing query 221: papers applicable to this problem (calculation procedures for laminar incompressible flow with arbitrary pressure gradient) .
[retrieveDocuments][L2] Retrieving documents for query: papers applicable to this problem (calculation procedures for laminar incompressible flow with arbitrary pressure gradient) .
[retrieveDocuments][L3] Query tokenized into 11 tokens: ['paper', 'applic', 'problem', 'calcul', 'procedur', 'laminar', 'incompress', 'flow', 'arbitrari', 'pressur', 'gradient']
[retrieveDocuments][L3] Retrieval complete. 1217 documents found.
[main][L2] Query 221: Retrieved 1217 documents.
[main][L2] Processing query 222: has anyone investigated the shear buckling of stiffened plates .
[retrieveDocuments][L2] Retrieving documents for query: has anyone investigated the shear buckling of stiffened plates .
[retrieveDocuments][L3] Query tokenized into 6 tokens: ['anyon', 'investig', 'shear', 'buckl', 'stiffen', 'plate']
[retrieveDocuments][L3] Retrieval complete. 634 documents found.
[main][L2] Query 222: Retrieved 634 documents.
[main][L2] Processing query 223: papers on shear buckling of unstiffened rectangular plates under shear .
[retrieveDocuments][L2] Retrieving documents for query: papers on shear buckling of unstiffened rectangular plates under shear .
[retrieveDocuments][L3] Query tokenized into 7 tokens: ['paper', 'shear', 'buckl', 'unstiffen', 'rectangular', 'plate', 'shear']
[retrieveDocuments][L3] Retrieval complete. 529 documents found.
[main][L2] Query 223: Retrieved 529 documents.
[main][L2] Processing query 224: in practice, how close to reality are the assumptions that the flow in a hypersonic shock tube using nitrogen is non-viscous and in thermodynamic equilibrium .
[retrieveDocuments][L2] Retrieving documents for query: in practice, how close to reality are the assumptions that the flow in a hypersonic shock tube using nitrogen is non-viscous and in thermodynamic equilibrium .
[retrieveDocuments][L3] Query tokenized into 13 tokens: ['practic', 'close', 'realiti', 'assumpt', 'flow', 'hyperson', 'shock', 'tube', 'us', 'nitrogen', 'non-visc', 'thermodynam', 'equilibrium']
[retrieveDocuments][L3] Retrieval complete. 1086 documents found.
[main][L2] Query 224: Retrieved 1086 documents.
[main][L2] Processing query 225: what design factors can be used to control lift-drag ratios at mach numbers above 5 .
[retrieveDocuments][L2] Retrieving documents for query: what design factors can be used to control lift-drag ratios at mach numbers above 5 .
[retrieveDocuments][L3] Query tokenized into 10 tokens: ['design', 'factor', 'can', 'us', 'control', 'lift-drag', 'ratio', 'mach', 'number', '5']
[retrieveDocuments][L3] Retrieval complete. 1087 documents found.
[main][L2] Query 225: Retrieved 1087 documents.
[main][L1] Output written to cranfield.bm25.bm25.0.output
